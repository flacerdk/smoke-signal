<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:creativeCommons="http://backend.userland.com/creativeCommonsRssModule" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">

<channel>
	<title>Daniel Lemire's blog</title>
	
	<link>http://lemire.me/blog</link>
	<description>Daniel Lemire is a computer science professor at the University of Quebec. His research is focused on software performance and indexing. He is a techno-optimist.</description>
	<lastBuildDate>Mon, 17 Oct 2016 15:26:11 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>

<image>
	<url>http://lemire.me/blog/wp-content/uploads/2015/10/profile2011_152-150x150.jpg</url>
	<title>Daniel Lemire's blog</title>
	<link>http://lemire.me/blog</link>
	<width>32</width>
	<height>32</height>
</image> 
	<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/daniel-lemire/atom" /><feedburner:info uri="daniel-lemire/atom" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><geo:lat>45</geo:lat><geo:long>-73</geo:long><creativeCommons:license>http://creativecommons.org/licenses/by-nc-sa/2.0/</creativeCommons:license><feedburner:emailServiceId>daniel-lemire/atom</feedburner:emailServiceId><feedburner:feedburnerHostname>https://feedburner.google.com</feedburner:feedburnerHostname><feedburner:feedFlare href="https://add.my.yahoo.com/rss?url=http%3A%2F%2Ffeeds.feedburner.com%2Fdaniel-lemire%2Fatom" src="http://us.i1.yimg.com/us.yimg.com/i/us/my/addtomyyahoo4.gif">Subscribe with My Yahoo!</feedburner:feedFlare><feedburner:feedFlare href="http://www.bloglines.com/sub/http://feeds.feedburner.com/daniel-lemire/atom" src="http://www.bloglines.com/images/sub_modern11.gif">Subscribe with Bloglines</feedburner:feedFlare><feedburner:feedFlare href="http://www.netvibes.com/subscribe.php?url=http%3A%2F%2Ffeeds.feedburner.com%2Fdaniel-lemire%2Fatom" src="//www.netvibes.com/img/add2netvibes.gif">Subscribe with Netvibes</feedburner:feedFlare><feedburner:feedFlare href="http://www.live.com/?add=http%3A%2F%2Ffeeds.feedburner.com%2Fdaniel-lemire%2Fatom" src="http://tkfiles.storage.msn.com/x1piYkpqHC_35nIp1gLE68-wvzLZO8iXl_JMledmJQXP-XTBOLfmQv4zhj4MhcWEJh_GtoBIiAl1Mjh-ndp9k47If7hTaFno0mxW9_i3p_5qQw">Subscribe with Live.com</feedburner:feedFlare><feedburner:feedFlare href="http://www.webwag.com/wwgthis.php?url=http%3A%2F%2Ffeeds.feedburner.com%2Fdaniel-lemire%2Fatom" src="http://www.webwag.com/images/wwgthis.gif">Subscribe with Webwag</feedburner:feedFlare><item>
		<title>Update to my VR bet with Greg Linden</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/oigXn4Wr9dI/</link>
		<comments>http://lemire.me/blog/2016/10/17/update-to-my-vr-bet-with-greg-linden/#comments</comments>
		<pubDate>Mon, 17 Oct 2016 15:26:11 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11876</guid>
		<description>I have an ongoing bet with Greg Linden stating that we are going to sell 10 million virtual-reality (VR) units per year by 2019. I have been paying close attention to VR technology and its impact. What have we learned in the last few months? The technology is fantastic. VR headsets work. The software is &amp;#8230; &lt;a href="http://lemire.me/blog/2016/10/17/update-to-my-vr-bet-with-greg-linden/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Update to my VR bet with Greg Linden&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>I have an <a href="http://glinden.blogspot.ca/2016/03/virtual-reality-hitting-mainstream-next.html">ongoing bet</a> with Greg Linden stating that we are going to sell 10 million virtual-reality (VR) units per year by 2019.</p>
<p>I have been paying close attention to VR technology and its impact.</p>
<p>What have we learned in the last few months?</p>
<ul>
<li>The technology is fantastic. VR headsets work.</li>
<li>The software is currently the weak point. Besides games and &#8220;experiences&#8221;, we simply have no compelling application. And while there are some good games,  none of them is good enough to motivate millions into purchasing a VR headset. </li>
<li>A nice surprise: Sony has managed to make VR work with the relatively underpowered PlayStation 4. The initial reports are very positive. I think that&#8217;s important: it shows that relatively weak and inexpensive processors are sufficient for VR.</li>
</ul>
<p>I key indicator is how many VR headsets Sony manages to sell over Christmas.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=oigXn4Wr9dI:6L_nIqyg4JE:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=oigXn4Wr9dI:6L_nIqyg4JE:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/oigXn4Wr9dI" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/10/17/update-to-my-vr-bet-with-greg-linden/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/10/17/update-to-my-vr-bet-with-greg-linden/</feedburner:origLink></item>
		<item>
		<title>Intel will add deep-learning instructions to its processors</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/oh91OFnS37I/</link>
		<comments>http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/#comments</comments>
		<pubDate>Fri, 14 Oct 2016 15:34:35 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11872</guid>
		<description>Some of the latest Intel processors support the AVX-512 family of vector instructions. These instructions operate on blocks of 512 bits (or 64 bytes). The benefit of such wide instructions is that even without increasing the processor clock speed, systems can still process a lot more data. Most code today operators over 64-bit words (8 &amp;#8230; &lt;a href="http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Intel will add deep-learning instructions to its processors&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>Some of the latest Intel processors support the AVX-512 family of vector instructions. These instructions operate on blocks of 512 bits (or 64 bytes). The benefit of such wide instructions is that even without increasing the processor clock speed, systems can still process a lot more data. Most code today operators over 64-bit words (8 bytes). In theory, keeping everything else constant, you could go 8 times faster by using AVX-512 instructions instead. </p>
<p>Of course, not all code can make use of vector instructions&#8230; but that&#8217;s not relevant. What matters is whether your &#8220;hot code&#8221; (where the processor spends much of its time) can benefit from them. In many systems, the hot code is made of tight loops that need to run billions of times. Just the kind of code that can benefit from vectorization!</p>
<p>The hottest trend in software right now is &#8220;deep learning&#8221;. It can be used to classify pictures, recognize speech or play the game of Go.  Some say that the quickest &#8220;get rich quick&#8221; scheme right now is to launch a deep-learning venture, and get bought by one of the big players (Facebook, Google, Apple, Microsoft, Amazon). It is made easier by the fact that companies like Google have open sourced their code such as <a href="https://github.com/tensorflow/">Tensorflow</a>.</p>
<p>Sadly for Intel, it has been mostly left out of the game. Nvidia graphics processors are the standard off-the-shelf approach to running deep-learning code. That&#8217;s not to say that Intel lacks good technology. But for the kind of brute-force algebra that&#8217;s required by deep learning, Nvidia graphics processors are simply a better fit.</p>
<p>However, Intel is apparently preparing a counter-attack, of sort. In September of this year, they have discreetly revealed that their future processors will support <a href="https://software.intel.com/sites/default/files/managed/69/78/319433-025.pdf">dedicated deep-learning instructions</a>. Intel&#8217;s AVX-512 family of instructions is decomposed in sub-families. There will be two new sub-families for deep-learning: AVX512_4VNNIW and AVX512_4FMAPS.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=oh91OFnS37I:4KMAgWZBb7s:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=oh91OFnS37I:4KMAgWZBb7s:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/oh91OFnS37I" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/</feedburner:origLink></item>
		<item>
		<title>A case study in the performance cost of abstraction (C++’s std::shuffle)</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/IW_fbfy001s/</link>
		<comments>http://lemire.me/blog/2016/10/10/a-case-study-in-the-performance-cost-of-abstraction-cs-stdshuffle/#comments</comments>
		<pubDate>Mon, 10 Oct 2016 16:47:44 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11838</guid>
		<description>Statisticians and machine-learning experts sometimes need to shuffle data quickly and repeatedly. There is one standard and simple algorithm to shuffle an array, the so-called Fisher-Yates shuffle: for (i=size; i&gt;1; i--) { nextpos = random_numbers_in_range(0,i); swap(storage[i-1], storage[nextpos]); } Not very difficult, is it? The C++ programming language, like many others, have a standard function to &amp;#8230; &lt;a href="http://lemire.me/blog/2016/10/10/a-case-study-in-the-performance-cost-of-abstraction-cs-stdshuffle/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;A case study in the performance cost of abstraction (C++&amp;#8217;s std::shuffle)&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>Statisticians and machine-learning experts sometimes need to shuffle data quickly and repeatedly. There is one standard and simple algorithm to shuffle an array, the so-called  Fisher-Yates shuffle:</p>
<pre style='color:#000000;background:#ffffff;'>for (i=size<span style='color:#0000ff; '>;</span> i>1<span style='color:#0000ff; '>;</span> i--) <span style='color:#0000ff; '>{</span>
  nextpos <span style='color:#0000ff; '>=</span> random_numbers_in_range<span style='color:#0000ff; '>(</span><span style='color:#800000; '>0</span><span style='color:#0000ff; '>,</span>i<span style='color:#0000ff; '>)</span><span style='color:#0000ff; '>;</span><span style='color:#008000; '></span>
  swap<span style='color:#0000ff; '>(</span>storage<span style='color:#0000ff; '>[</span>i<span style='color:#0000ff; '>-</span><span style='color:#800000; '>1</span><span style='color:#0000ff; '>]</span><span style='color:#0000ff; '>,</span> storage<span style='color:#0000ff; '>[</span>nextpos<span style='color:#0000ff; '>]</span><span style='color:#0000ff; '>)</span><span style='color:#0000ff; '>;</span>
<span style='color:#0000ff; '>}</span>
</pre>
<p>Not very difficult, is it?  The C++ programming language, like many others, have a standard function to solve this problem (<a href="http://en.cppreference.com/w/cpp/algorithm/random_shuffle">std::shuffle</a>).</p>
<p>How does it fare against a very basic Fisher-Yates shuffle without any optimization whatsoever? To make sure that the comparison is fair, let us  work with the same data (an array of strings) and use the same random number generator (I chose <a href="http://www.pcg-random.org/">PCG</a>). To avoid caching issues, let us use a small array that fits in cache.</p>
<p>Here is my unoptimized C++ code: </p>
<pre style='color:#000000;background:#ffffff;'>template &lt;<span style='color:#0000ff; font-weight:bold; '>class</span> T>
void  shuffle(T *storage, uint32_t size) <span style='color:#0000ff; '>{</span>
    <span style='color:#0000ff; font-weight:bold; '>for</span> <span style='color:#0000ff; '>(</span>uint32_t i<span style='color:#0000ff; '>=</span>size<span style='color:#0000ff; '>;</span> i<span style='color:#0000ff; '>></span><span style='color:#800000; '>1</span><span style='color:#0000ff; '>;</span> i<span style='color:#0000ff; '>-</span><span style='color:#0000ff; '>-</span><span style='color:#0000ff; '>)</span> <span style='color:#0000ff; '>{</span>
        uint32_t nextpos <span style='color:#0000ff; '>=</span> pcg32_random_bounded<span style='color:#0000ff; '>(</span>i<span style='color:#0000ff; '>)</span><span style='color:#0000ff; '>;</span>
        std<span style='color:#0000ff; '>:</span><span style='color:#0000ff; '>:</span>swap<span style='color:#0000ff; '>(</span>storage<span style='color:#0000ff; '>[</span>i<span style='color:#0000ff; '>-</span><span style='color:#800000; '>1</span><span style='color:#0000ff; '>]</span><span style='color:#0000ff; '>,</span>storage<span style='color:#0000ff; '>[</span>nextpos<span style='color:#0000ff; '>]</span><span style='color:#0000ff; '>)</span><span style='color:#0000ff; '>;</span>
    <span style='color:#0000ff; '>}</span>
<span style='color:#0000ff; '>}</span>
</pre>
<p>I claim that this is the &#8220;textbook&#8221; implementation&#8230; meaning that it is the closest thing you can get to taking a textbook and coding up the pseudo-code in C++. (Yes I know that people copy code from Wikipedia or StackOverflow, not textbooks, but humor me.)</p>
<p>The <tt>pcg32_random_bounded</tt> function I call is implemented in a standard (but suboptimal way) to get a random number in a range with two divisions. <a href="http://lemire.me/blog/2016/06/30/fast-random-shuffling/">You can do it with a single multiplication instead</a> but let us ignore optimizations.</p>
<p>Here are my results, expressed in CPU clock cycles per value&#8230; (Skylake processor, clang++ 4.0)</p>
<table style="width:80%">
<tr>
<th>technique</th>
<th>clock cycles per value</th>
</tr>
<tr>
<td><tt>std::shuffle</tt></td>
<td>73</td>
</tr>
<tr>
<td><tt>textbook code</tt></td>
<td>29</td>
</tr>
</table>
<p>So the textbook code is twice as fast as the standard C++ function.</p>
<p>Why is that? </p>
<p>It is often the case that <a href="http://lemire.me/blog/2016/02/01/default-random-number-generators-are-slow/">default random number generators are slow</a> due to concurrency issues. But we provide our own random number generators, so it should not be an issue.</p>
<p>A cursory analysis reveals that the most likely reason for the slowdown is that the standard C++ library tends to use 64-bit arithmetic throughout (on 64-bit systems). I implicitly assume, in my textbook implementation, that you are not going to randomly shuffle arrays containing more than 4 billion elements. I don&#8217;t think I am being unreasonable: an array of 4 billion <tt>std::string</tt> values would use at least 128 GB of RAM. If you need to shuffle that much data, you probably want to parallelize the problem. But, from the point of view of the engineers working on the standard library, they have to work with the requirements set forth by the specification. So 64-bit arithmetic it is! And that&#8217;s how I can beat them without any effort.</p>
<p>The absolute numbers might also surprise you. The Fisher-Yates shuffle is very efficient. We do a single pass over the data. We do not really need to look at the data, just move it. We use a fast random number generator (PCG). How can we end up with 30 or 70 cycles per array element?</p>
<p>Part of the problem is our use of <tt>std::string</tt>. On my system, a single (empty) <tt>std::string</tt> uses 32 bytes whereas a pointer (<tt>char *</tt>) uses only 8 bytes. If we fall back on C strings (<tt>char *</tt>), we can accelerate the processing simply because there is less data to move. Without going overboard with optimizations, I can bring the computational cost to about 7 cycles per element by avoiding divisions and using C strings instead of <tt>std::string</tt> objects. That&#8217;s an order of magnitude faster than the standard shuffle function.</p>
<p>So while <tt>std::shuffle</tt> is very general, being able to sort just about any array using just about any random-number generator&#8230; this generality has a cost in terms of performance.</p>
<p><a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2016/10/10">My code is available</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=IW_fbfy001s:MmgLa4n2FQ0:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=IW_fbfy001s:MmgLa4n2FQ0:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/IW_fbfy001s" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/10/10/a-case-study-in-the-performance-cost-of-abstraction-cs-stdshuffle/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/10/10/a-case-study-in-the-performance-cost-of-abstraction-cs-stdshuffle/</feedburner:origLink></item>
		<item>
		<title>Variable-length strings can be expensive</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/-VDiUR5UX8M/</link>
		<comments>http://lemire.me/blog/2016/10/05/variable-length-strings-can-be-expensive/#comments</comments>
		<pubDate>Wed, 05 Oct 2016 15:59:07 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11819</guid>
		<description>Much of our software deals with variable-length strings. For example, my name &amp;#8220;Daniel&amp;#8221; uses six characters whereas my neighbor&amp;#8217;s name (&amp;#8220;Philippe&amp;#8221;) uses 8 characters. &amp;#8220;Old&amp;#8221; software often shied away from variable-length strings. The Pascal programming language supported fixed-length strings. Most relational databases support fixed-length strings. Flat files with fixed-length lines and fixed-length records were common &amp;#8230; &lt;a href="http://lemire.me/blog/2016/10/05/variable-length-strings-can-be-expensive/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Variable-length strings can be expensive&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>Much of our software deals with variable-length strings. For example, my name &#8220;Daniel&#8221; uses six characters whereas my neighbor&#8217;s name (&#8220;Philippe&#8221;) uses 8 characters. </p>
<p>&#8220;Old&#8221; software often shied away from variable-length strings. The Pascal programming language supported fixed-length strings. Most relational databases support fixed-length strings. Flat files with fixed-length lines and fixed-length records were common in the old days.</p>
<p>It seems that people today never worry about variable-length strings. All the recent programming languages I know ignore the concept of fixed-length strings.</p>
<p>Yet fixed-length strings have clear engineering benefits. For example, if I were to store &#8220;tweets&#8221; (from Twitter) in memory, using a flat 140 characters per tweet, I would be able to support fast direct access without having to go through pointers. Memory allocation would be trivial.</p>
<p>However, maybe processors have gotten so smart, and string operations so optimized, that there is no clear performance benefits to fixed-length strings in today&#8217;s software.</p>
<p>I don&#8217;t think so.</p>
<p>Even today, it is often possible to accelerate software by replacing variable-length strings by fixed-length ones, when you know ahead of time that all your strings are going to be reasonably short.</p>
<p>To examine this idea, I have created lists of strings made simply of the numbers (as strings) from 0 to 1024. Such strings have length ranging between 1 and 4 characters. In C, we use null-terminated characters, so the actual length is between 2 and 5. In C++, they have standard strings (<tt>std::string</tt>), with significant overhead: on my system, a single <tt>std::string</tt> uses 32 bytes, not counting the string content itself.</p>
<p>Instead of using variable-length strings, I can &#8220;pad&#8221; my strings so that they have a fixed length (8 characters), adding null characters as needed.</p>
<p>How long does it take, in CPU cycles per string, to sort a short array of strings (1024 strings)?</p>
<table style="width:80%">
<tr>
<th>string type</th>
<th>CPU cycles per element</th>
</tr>
<tr>
<td>C++ <tt>std::string</tt></td>
<td>520</td>
</tr>
<tr>
<td>standard C string</td>
<td>300</td>
</tr>
<tr>
<td>padded C string</td>
<td>140</td>
</tr>
</table>
<p>So for this experiment, replacing variable-length strings by fixed-length strings more than double the performance! And my code isn&#8217;t even optimized. For example, to keep the comparison &#8220;fair&#8221;, I sorted pointers to strings&#8230; but my padded C strings fit in a machine word and do not require a pointer. So, in fact, fixed-length strings could be nearly three times faster with enough work.</p>
<p>To summarize: Variable-length strings are a convenient abstraction. You may hear that string operations are very cheap, unlikely to be a bottleneck and so forth&#8230; That might be so&#8230; </p>
<p>But I submit to you that the omnipresence of variable-length strings as a universal default can make us blind to very sweet optimization opportunities.</p>
<p><a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/blob/master/2016/10/05/pointersort.cpp">My source code is available</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=-VDiUR5UX8M:3lTmKW8JPm0:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=-VDiUR5UX8M:3lTmKW8JPm0:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/-VDiUR5UX8M" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/10/05/variable-length-strings-can-be-expensive/feed/</wfw:commentRss>
		<slash:comments>20</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/10/05/variable-length-strings-can-be-expensive/</feedburner:origLink></item>
		<item>
		<title>Can Swift code call C code without overhead?</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/hWBDFQivaSU/</link>
		<comments>http://lemire.me/blog/2016/09/29/can-swift-code-call-c-code-without-overhead/#comments</comments>
		<pubDate>Thu, 29 Sep 2016 15:56:40 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11810</guid>
		<description>Swift is the latest hot new language from Apple. It is becoming the standard programming language on Apple systems. I complained in a previous post that Swift 3.0 has only about half of Java&amp;#8217;s speed in tests that I care about. That&amp;#8217;s not great for high-performance programming. But we do have a language that produces &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/29/can-swift-code-call-c-code-without-overhead/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Can Swift code call C code without overhead?&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>Swift is the latest hot new language from Apple. It is becoming the standard programming language on Apple systems. </p>
<p>I complained in a previous post that <a href="http://lemire.me/blog/2016/09/22/swift-versus-java-the-bitset-performance-test/">Swift 3.0 has only about half of Java&#8217;s speed</a> in tests that I care about. That&#8217;s not great for high-performance programming.</p>
<p>But we do have a language that produces very fast code: the C language. </p>
<p>Many languages like Objective-C, C++, Python and Go allow you to call C code with relative ease. C++ and Objective-C can call C code with no overhead. Go  makes it very easy, but the performance overhead is huge. So it is almost never a good idea to call C from Go for performance. Python also suffers from a significant overhead when calling C code, but since native Python is not so fast, it is often a practical idea to rewrite performance-sensitive code in C and call it from Python. Java makes it hard to call C code, so it is usually not even considered.</p>
<p>What about Swift? We know, as per Apple&#8217;s requirements, that Swift must interact constantly with legacy Objective-C code. So we know that it must be good. How good is it?</p>
<p>To put it to the test, I decided to call from Swift a simple Fibonacci recurrence function :</p>
<pre style='color:#000000;background:#ffffff;'>void fibo(int * x, int * y) <span style='color:#0000ff; '>{</span>
  <span style='color:#0000ff; font-weight:bold; '>int</span> c <span style='color:#0000ff; '>=</span> <span style='color:#0000ff; '>*</span> y<span style='color:#0000ff; '>;</span>
  <span style='color:#0000ff; '>*</span>y <span style='color:#0000ff; '>=</span> <span style='color:#0000ff; '>*</span>x <span style='color:#0000ff; '>+</span> <span style='color:#0000ff; '>*</span>y<span style='color:#0000ff; '>;</span>
  <span style='color:#0000ff; '>*</span>x <span style='color:#0000ff; '>=</span> c<span style='color:#0000ff; '>;</span>
<span style='color:#0000ff; '>}</span>
</pre>
<p>(Note: this function can overflow and that is undefined behavior in C.)</p>
<p>How does it fare against pure Swift code?</p>
<pre style='color:#000000;background:#ffffff;'>let c = j<span style='color:#0000ff; '>;</span>
j = i &amp;+ j<span style='color:#0000ff; '>;</span>
i = c<span style='color:#0000ff; '>;</span>
</pre>
<p>To be clear, this is a really extreme case. You should never rewrite such a tiny piece of code in C for performance. I am intentionally pushing the limits.</p>
<p>I wrote a test that calls these functions 3.2 billion times. The pure Swift takes 9.6 seconds on a Haswell processor&#8230; or about 3 nanosecond per call. The C function takes a bit over 13 seconds or about 4 nanoseconds per iteration. Ok. But what if I rewrote the whole thing into one C function, called only once? Then it runs in 11 seconds (it is slower than pure Swift code). </p>
<p>The numbers I have suggest that calling C from Swift is effectively free.</p>
<p>In these tests, I do not pass to Swift any optimization flag. The way you build a swift program is by typing &#8220;<tt>swift build</tt>&#8221; which is nice and elegant. To optimize the binary, you can type &#8220;<tt>swift build --configuration release</tt>&#8220;. Nice! But benchmark code is part of your tests. Sadly, swift seems to insist on only testing &#8220;debug&#8221; code for some reason. Typing  &#8220;<tt>swift test --configuration release</tt>&#8221; fails since the test option does not have a configuration flag. (Calling <tt>swift test -Xswiftc -O</tt> gives me linking errors.)</p>
<p>I rewrote the code using a pure C program, without any Swift. Sure enough, the program runs in about 11 seconds without any optimization flag. This confirms my theory that Swift is testing the code with all optimizations turned off. What if I turn on all C optimizations?  Then I go down to 1.7 seconds (or about half a nanosecond per iteration).</p>
<p>So while calling C from Swift is very cheap, insuring that Swift properly optimizes the code might be trickier. </p>
<p>It seems odd that, by default, Swift runs benchmarks in debug mode. It is not helping programmers who care about performance.</p>
<p>Anyhow, a good way around this problem is to simply build binaries in release mode and measure how long it takes them to run. It is crude,  but it gets the job done in this case:</p>
<pre style='color:#000000;background:#ffffff;'>$ swift build <span style='color:#0000ff; '>-</span><span style='color:#0000ff; '>-</span>configuration release
$ <span style='color:#0000ff; font-weight:bold; '>time</span> .<span style='color:#0000ff; '>/</span>.build<span style='color:#0000ff; '>/</span>release<span style='color:#0000ff; '>/</span>LittleSwiftTest
<span style='color:#800000; '>3221225470</span>

real       0m2.030s
<span style='color:#0000ff; font-weight:bold; '>user</span>       0m2.028s
sys        0m0.000s
$ <span style='color:#0000ff; font-weight:bold; '>time</span> .<span style='color:#0000ff; '>/</span>.build<span style='color:#0000ff; '>/</span>release<span style='color:#0000ff; '>/</span>LittleCOverheadTest
<span style='color:#800000; '>3221225470</span>

real       0m1.778s
<span style='color:#0000ff; font-weight:bold; '>user</span>       0m1.776s
sys        0m0.000s

$ clang <span style='color:#0000ff; '>-</span>Ofast <span style='color:#0000ff; '>-</span>o purec  code<span style='color:#0000ff; '>/</span>purec.c
$ <span style='color:#0000ff; font-weight:bold; '>time</span> .<span style='color:#0000ff; '>/</span>purec
<span style='color:#800000; '>3221225470</span>

real       0m1.747s
<span style='color:#0000ff; font-weight:bold; '>user</span>       0m1.744s
sys        0m0.000s
</pre>
<p>So there is no difference between a straight C program, and a Swift program that calls billions of times a C function. They are both just as fast. </p>
<p>The pure Swift program is slightly slower in this case, however. It suggests that using C for performance-sensitive code could be beneficial in a Swift project.</p>
<p>So I have solid evidence that calling C functions from Swift is very cheap. That is very good news. It means that if for whatever reason, Swift is not fast enough for your needs, you stand a very good chance of being able to rely on C instead.</p>
<p><a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2016/09/29">My Swift source code is available (works under Linux and Mac).</a></p>
<p><strong>Credit</strong>: Thanks to Stephen Canon for helping me realize that I could lower the call overhead by calling directly the C function instead of wrapping it first in a tiny Swift function.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=hWBDFQivaSU:2bzeHbWUjig:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=hWBDFQivaSU:2bzeHbWUjig:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/hWBDFQivaSU" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/29/can-swift-code-call-c-code-without-overhead/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/29/can-swift-code-call-c-code-without-overhead/</feedburner:origLink></item>
		<item>
		<title>Sorting already sorted arrays is much faster?</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/s8nBZiI_6zA/</link>
		<comments>http://lemire.me/blog/2016/09/28/sorting-already-sorted-arrays-is-much-faster/#comments</comments>
		<pubDate>Wed, 28 Sep 2016 15:16:45 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11799</guid>
		<description>If you are reading a random textbook on computer science, it is probably going to tell you all about how good sorting algorithms take linearithmic time. To arrive at this result, they count the number of operations. That&amp;#8217;s a good model to teach computer science, but working programmers need more sophisticated models of software performance. &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/28/sorting-already-sorted-arrays-is-much-faster/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Sorting already sorted arrays is much faster?&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>If you are reading a random textbook on computer science, it is probably going to tell you all about how good sorting algorithms take linearithmic time. To arrive at this result, they count the number of operations. That&#8217;s a good model to teach computer science, but working programmers need more sophisticated models of software performance.</p>
<p>On modern superscalar processors, we expect in-memory sorting to limited by how far ahead the processor can  predict where the data will go. Though moving the data in memory is not free, it is a small cost if it can be done predictably.</p>
<p>We know that sorting &#8220;already sorted data&#8221; can be done in an easy-to-predict manner (just do nothing). So it should be fast. But how much faster is it that sorting randomly shuffled data?</p>
<p>I decided to <a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2016/09/28">run an experiment</a>. </p>
<p>I use arrays containing one million distinct 32-bit integers, and I report the time in CPU cycles per value on a Haswell processor. I wrote my code in C++.</p>
<table style="width:80%">
<tr>
<th>function</th>
<th>sorted data</th>
<th>shuffled data</th>
<th>sorted in reverse</th>
</tr>
<tr>
<td><tt>std::sort</tt></td>
<td>38</td>
<td>200</td>
<td>30</td>
</tr>
</table>
<p>For comparison, it takes roughly <em>n</em> log(<em>n</em>) comparisons to sort an array of size <em>n</em> in the worst case with a good algorithm. In my experiment, log(<em>n</em>) is about 20.</p>
<p>The numbers bear out our analysis. Sorting an already-sorted array takes a fraction of the time needed to sort a shuffled array. One could object that the reason sorting already-sorted arrays is fast is because we do not have to move the data so much. So I also included initial arrays that were sorted in reverse. Interestingly, <tt>std::sort</tt> is even faster with reversed arrays! This is clear evidence for our thesis.</p>
<p>(<a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2016/09/28">The C++ source code is available</a>. My software includes <a href="https://en.wikipedia.org/wiki/Timsort">timsort</a> results if you are interested.)</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=s8nBZiI_6zA:qezYec444EE:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=s8nBZiI_6zA:qezYec444EE:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/s8nBZiI_6zA" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/28/sorting-already-sorted-arrays-is-much-faster/feed/</wfw:commentRss>
		<slash:comments>10</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/28/sorting-already-sorted-arrays-is-much-faster/</feedburner:origLink></item>
		<item>
		<title>Swift versus Java : the bitset performance test</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/lsWumckIY6g/</link>
		<comments>http://lemire.me/blog/2016/09/22/swift-versus-java-the-bitset-performance-test/#comments</comments>
		<pubDate>Thu, 22 Sep 2016 04:21:20 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11779</guid>
		<description>I claimed online that the performance of Apple&amp;#8217;s Swift was not yet on par with Java. People asked me to back my claim with numbers. I decided to construct one test based on bitsets. A bitset is a fast data structure to implement sets of integers. Java comes with its own bitset class called java.util.BitSet. &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/22/swift-versus-java-the-bitset-performance-test/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;Swift versus Java : the bitset performance test&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>I claimed online that the performance of Apple&#8217;s Swift was not yet on par with Java. People asked me to back my claim with numbers.</p>
<p>I decided to construct one test based on bitsets. A bitset is a fast data structure to implement sets of integers. </p>
<p>Java comes with its own bitset class called <tt>java.util.BitSet</tt>. <a href="https://github.com/lemire/microbenchmarks/blob/master/src/main/java/me/lemire/microbenchmarks/bitset/Bitset.java">I wrote three tests for it</a>: the time it takes to add a million integers in sequence to a bitset, the time it takes to count how many integers are present in the bitset, and the time it takes to iterate through the integers.</p>
<p>Here are the results in Java : </p>
<pre style='color:#000000;background:#ffffff;'><span style='color:#005fd2; '>git</span> <span style='color:#0000e6; '>clone</span> <span style='color:#0000e6; '>https://github.com/lemire/microbenchmarks.git</span>
<span style='color:#005fd2; '>cd</span> <span style='color:#0000e6; '>microbenchmarks</span>
<span style='color:#005fd2; '>mvn</span> <span style='color:#0000e6; '>clean</span> <span style='color:#0000e6; '>install</span>
<span style='color:#005fd2; '>java</span> <span style='color:#074726; '>-cp</span> <span style='color:#0000e6; '>target/microbenchmarks-0.0.1-jar-with-dependencies.jar</span> <span style='color:#0000e6; '>me.lemire.microbenchmarks.bitset.Bitset</span>

<span style='color:#005fd2; '>Benchmark</span> <span style='color:#0000e6; '>Mode</span> <span style='color:#0000e6; '>Samples</span> <span style='color:#0000e6; '>Score</span> <span style='color:#0000e6; '>Error</span> <span style='color:#0000e6; '>Units</span>
<span style='color:#005fd2; '>m.l.m.b.Bitset.construct</span> <span style='color:#0000e6; '>avgt</span> <span style='color:#800000; '>5</span> <span style='color:#800000; '>0.008</span> <span style='color:#0000e6; '>±</span> <span style='color:#800000; '>0.002</span> <span style='color:#0000e6; '>s/op</span>
<span style='color:#005fd2; '>m.l.m.b.Bitset.count</span> <span style='color:#0000e6; '>avgt</span> <span style='color:#800000; '>5</span> <span style='color:#800000; '>0.001</span> <span style='color:#0000e6; '>±</span> <span style='color:#800000; '>0.000</span> <span style='color:#0000e6; '>s/op</span>
<span style='color:#005fd2; '>m.l.m.b.Bitset.iterate</span> <span style='color:#0000e6; '>avgt</span> <span style='color:#800000; '>5</span> <span style='color:#800000; '>0.005</span> <span style='color:#0000e6; '>±</span> <span style='color:#800000; '>0.001</span> <span style='color:#0000e6; '>s/op</span>
</pre>
<p>So all tests take at most a few milliseconds. Good.</p>
<p>Next <a href="https://github.com/lemire/SwiftBitset/blob/master/Tests/BitsetTests/BitsetTests.swift">I did my best to reproduce the same tests in Swift</a>:</p>
<pre style='color:#000000;background:#ffffff;'><span style='color:#005fd2; '>git</span> <span style='color:#0000e6; '>clone</span> <span style='color:#0000e6; '>https://github.com/lemire/SwiftBitset.git</span>
<span style='color:#005fd2; '>cd</span> <span style='color:#0000e6; '>SwiftBitset</span>

<span style='color:#005fd2; '>swift</span> <span style='color:#0000e6; '>test</span> <span style='color:#074726; '>-Xswiftc</span> <span style='color:#074726; '>-Ounchecked</span> <span style='color:#074726; '>-s</span> <span style='color:#0000e6; '>BitsetTests.BitsetTests/testAddPerformance</span>
<span style='color:#005fd2; '>Test</span> <span style='color:#0000e6; '>Case</span> <span style='color:#0000e6; '>'</span><span style='color:#0000e6; '>-[BitsetTests.BitsetTests testAddPerformance]</span><span style='color:#0000e6; '>'</span> <span style='color:#0000e6; '>measured</span> <span style='color:#0000e6; '>[Time</span><span style='color:#0000ff; font-weight:bold; '>,</span> <span style='color:#0000e6; '>seconds]</span> <span style='color:#0000e6; '>average:</span> <span style='color:#800000; '>0.019</span>

<span style='color:#005fd2; '>swift</span> <span style='color:#0000e6; '>test</span>  <span style='color:#074726; '>-Xswiftc</span> <span style='color:#074726; '>-Ounchecked</span> <span style='color:#074726; '>-s</span> <span style='color:#0000e6; '>BitsetTests.BitsetTests/testCountPerformance</span>
<span style='color:#005fd2; '>Test</span> <span style='color:#0000e6; '>Case</span> <span style='color:#0000e6; '>'</span><span style='color:#0000e6; '>-[BitsetTests.BitsetTests testCountPerformance]</span><span style='color:#0000e6; '>'</span> <span style='color:#0000e6; '>measured</span> <span style='color:#0000e6; '>[Time</span><span style='color:#0000ff; font-weight:bold; '>,</span> <span style='color:#0000e6; '>seconds]</span> <span style='color:#0000e6; '>average:</span> <span style='color:#800000; '>0.004</span>

<span style='color:#005fd2; '>swift</span> <span style='color:#0000e6; '>test</span>  <span style='color:#074726; '>-Xswiftc</span> <span style='color:#074726; '>-Ounchecked</span>  <span style='color:#074726; '>-s</span> <span style='color:#0000e6; '>BitsetTests.BitsetTests/testIteratorPerformance</span>
<span style='color:#005fd2; '>Test</span> <span style='color:#0000e6; '>Case</span> <span style='color:#0000e6; '>'</span><span style='color:#0000e6; '>-[BitsetTests.BitsetTests testIteratorPerformance]</span><span style='color:#0000e6; '>'</span> <span style='color:#0000e6; '>measured</span> <span style='color:#0000e6; '>[Time</span><span style='color:#0000ff; font-weight:bold; '>,</span> <span style='color:#0000e6; '>seconds]</span> <span style='color:#0000e6; '>average:</span> <span style='color:#800000; '>0.010</span>
</pre>
<p>These tests are rough. I got these numbers of my laptop, without even trying to keep various noise factors in check. Notice however that I disable bound checking in Swift, but not in Java, thus giving something of an unfair advantage to Swift.</p>
<p>But as is evident, <a href="https://github.com/lemire/SwiftBitset">SwiftBitset</a> can be 2 times slower than Java&#8217;s BitSet. Not a small difference.</p>
<p><a href="https://github.com/lemire/SwiftBitset">SwiftBitset</a> is brand new whereas Java&#8217;s BitSet underwent thorough review over the years. It is likely that <a href="https://github.com/lemire/SwiftBitset">SwiftBitset</a> leaves at least some performance on the table. So we could probably close some of the performance gap with better code.</p>
<p>Nevertheless, it does not make me hopeful regarding Swift&#8217;s performance compared to Java, at least for the type of work I care about. But Swift hopefully uses less memory which might be important on mobile devices.</p>
<p>Some people seem to claim that Swift gives iOS an advantage over android and its use of Java. I&#8217;d like to see the numbers.</p>
<p><strong>Update</strong>. I decided to add a <a href="https://github.com/lemire/bitset/blob/master/bitset_test.go">Go benchmark</a> and a <a href="https://github.com/lemire/cbitset/blob/master/benchmarks/lemirebenchmark.c">C benchmark</a>. Here are my results:</p>
<table style="width:80%">
<tr>
<th>language</th>
<th>create</th>
<th>count</th>
<th>iterate</th>
</tr>
<tr>
<td>Java&#8217;s BitSet</td>
<td>8 ms</td>
<td>1 ms</td>
<td>5 ms</td>
</tr>
<tr>
<td><a href="https://github.com/lemire/cbitset">C&#8217;s cbitset</a></td>
<td>11 ms</td>
<td>0 ms</td>
<td>4 ms</td>
</tr>
<tr>
<td><a href="https://github.com/lemire/SwiftBitset">SwiftBitset</a></td>
<td>19 ms</td>
<td>4 ms</td>
<td>10 ms</td>
</tr>
<tr>
<td><a href="https://github.com/willf/bitset">Go&#8217;s bitset</a></td>
<td>18 ms</td>
<td>3 ms</td>
<td>13 ms</td>
</tr>
</table>
<p>It might seem surprising to some, but Java can be a fast language. (The C implementation suffers from my Mac&#8217;s poor &#8220;malloc&#8221; performance. Results would be vastly different under Linux.)</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=lsWumckIY6g:-X5Ocox4dFI:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=lsWumckIY6g:-X5Ocox4dFI:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/lsWumckIY6g" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/22/swift-versus-java-the-bitset-performance-test/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/22/swift-versus-java-the-bitset-performance-test/</feedburner:origLink></item>
		<item>
		<title>My thoughts on Swift</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/uZ4Fvv9NSlw/</link>
		<comments>http://lemire.me/blog/2016/09/21/my-thoughts-on-swift/#respond</comments>
		<pubDate>Wed, 21 Sep 2016 04:30:00 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11770</guid>
		<description>Swift is a new programming language produced by Apple for its iOS devices (primarily the iPhone). It first appeared two years ago and it has been gaining popularity quickly. Before Swift, Apple programmers were &amp;#8220;stuck&amp;#8221; with Objective-C. Objective-C is old and hardly ever used outside the Apple ecosystem. Swift, at least the core language, is &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/21/my-thoughts-on-swift/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;My thoughts on Swift&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Swift_(programming_language)">Swift</a> is a new programming language produced by Apple for its iOS devices (primarily the iPhone).  It first appeared two years ago and it has been gaining popularity quickly.</p>
<p>Before Swift, Apple programmers were &#8220;stuck&#8221; with <a href="https://en.wikipedia.org/wiki/Objective-C">Objective-C</a>. Objective-C is old and hardly ever used outside the Apple ecosystem.</p>
<p>Swift, at least the core language, is <a href="https://swift.org/download/#snapshots">fully available on Linux</a>. There are rumours that it should soon become available for Windows. </p>
<p>If you want to build mobile apps, then learning Swift is probably wise. But setting this context aside, how does Swift stands on its own?</p>
<p>To find out, I wrote and published <a href="https://github.com/lemire/SwiftBitset">a small Bitset library in Swift 3.0</a>.</p>
<ul>
<li>Like most recent languages (e.g., Rust, Go), Swift 3.0 comes with standard and universal tools to test, build and manage dependencies. In contrast, languages like C, C++ or Java depend on additional tools that are not integrated in the language per se. There is no reason, in 2016, to not include unit testing, benchmarking and dependency management as part of a programming language itself. Swift shines in this respect. </li>
<li>Swift feels a lot like Java. It should be easy for Java programmers to learn the language. Swift passes classes per reference and everything else per value though there is an <tt>inout</tt> parameter annotation to override the default. The ability to turn what would have been a pass-by-reference class in Java and make it a pass-by-value struct opens up optimization opportunities in Swift.</li>
<li> In Java, all strings are immutable. In Swift, strings can be either immutable or mutable. I suspect that this may give Swift a performance advantage in some cases.</li>
<li> Swift supports automatic type inference which is meant to give the syntax a &#8220;high-level&#8221; look compared to Java. I am not entirely convinced that it is actual progress in practice.</li>
<li>Swift uses automatic reference counting instead of a more Java-like garbage collection. Presumably, this means fewer long pauses which might be advantageous when latency is a problem (as is the case in some user-facing applications). Hopefully, it should also translate into lower memory usage in most cases. For the programmer, it appears to be more or less transparent.</li>
<li>Swift has operator overloading like C++. It might even be more powerful than C++ in the sense that you can create your own operators on the fly. </li>
<li>By default, Swift &#8220;crashes&#8221; when an operation overflows (like casting, multiplication, addition&#8230;). The intention is noble but I am not sure crashing applications in production is a good default especially if it comes with a performance penalty. Swift also &#8220;crashes&#8221; when you try to allocate too much memory, with apparently no way for the application to recover sanely. Again, I am not sure why it is a good default though maybe it is.</li>
<li>It looks like it is easy to link against C libraries. I built <a href="https://github.com/lemire/SwiftCallingCHeader">a simple example</a>. Unlike Go, I suspect that the performance will be good. </li>
<li>Available benchmarks so far indicate that Swift is slower than languages like Go and Java, which are themselves slower than C. So Swift might not be a wise choice for high-performance programming. </li>
</ul>
<p>My verdict? Swift  compares favourably with Java. I&#8217;d be happy to program in Swift if I needed to build an iOS app. </p>
<p>Would I use Swift for other tasks? There is a lot of talk about using Swift to build web applications. I am not sure. </p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=uZ4Fvv9NSlw:vxswZxNrUDA:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=uZ4Fvv9NSlw:vxswZxNrUDA:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/uZ4Fvv9NSlw" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/21/my-thoughts-on-swift/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/21/my-thoughts-on-swift/</feedburner:origLink></item>
		<item>
		<title>The rise of dark circuits</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/ohNKAJbKHkY/</link>
		<comments>http://lemire.me/blog/2016/09/19/the-rise-of-dark-circuits/#comments</comments>
		<pubDate>Mon, 19 Sep 2016 17:17:37 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11411</guid>
		<description>The latest iPhone 7 from Apple has more computing peak power than most laptops. Apple pulled this off using a technology called ARM big.LITTLE where half of the processor is only used when high performance is needed, otherwise it remains idle. That&amp;#8217;s hardly the sole example of a processor with parts that remain idle most &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/19/the-rise-of-dark-circuits/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;The rise of dark circuits&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>The latest iPhone 7 from Apple has more computing peak power than <a href="http://daringfireball.net/linked/2016/09/14/geekbench-android-a10">most laptops</a>. Apple pulled this off using a technology called <a href="https://en.wikipedia.org/wiki/ARM_big.LITTLE">ARM big.LITTLE</a> where half of the processor is only used when high performance is needed, otherwise it remains idle.</p>
<p>That&#8217;s hardly the sole example of a processor with parts that remain idle most of the time. For example, all recent desktop Intel processors come with an &#8220;Intel processor graphics&#8221; that can process video, replace a graphics card and so forth. It uses roughly <a href="https://software.intel.com/sites/default/files/managed/c5/9a/The-Compute-Architecture-of-Intel-Processor-Graphics-Gen9-v1d0.pdf">half the silicon of your core processor</a> but in many PCs where there is either no display or where there is a graphics card, most of this silicon is unused most of the time.</p>
<p>If you stop to think about it, it is somewhat remarkable. Silicon processors have gotten so cheap that we can afford to leave much of the silicon unused. </p>
<p>In contrast, much of the progress in computing has to do with miniaturization. Smaller transistors use less power, are cheaper to mass-produce and can enable processors running at a higher frequency. Yet transistors in the CPU of your computer are already only dozens of atoms in diameters. Intel has thousands of smart engineers, but none of them can make a silicon-based transistor with less than one atom. So we are about to hit a wall&#8230; a physical wall. Some would argue that this wall is already upon us. We can create wider processors, processors with fancier instructions, processors with more cores&#8230; specialized processors&#8230; but we have a really hard time squeezing out more conventional performance out of single cores.</p>
<p>You can expect companies like Intel to provide us with more efficient processors the conventional manner (by miniaturizing silicon transistors) up till 2020, and maybe at the extreme limit up till 2025&#8230; but then it is game over. We may buy a few extra years by going beyond silicon&#8230; but nobody is talking yet about subatomic computing.</p>
<p>I should caution you against excessive pessimism. Currently, for $15, you can buy a Raspberry Pi 3 computer which is probably closer than you imagine to the power of your laptop. In five years, the successor of the Raspberry Pi might still sell for $15 but be just as fast as the iPhone 7&#8230; and be faster than most laptops sold today. This means that a $30 light bulb might have the computational power of a small server in today&#8217;s terms. So we are not about to run out of computing power&#8230; not yet&#8230;</p>
<p>Still&#8230; where is the next frontier?</p>
<p>We can build 3D processors, to squeeze more transistors into a smaller area&#8230;  But this only helps you so much if each transistor still uses the same power. We can&#8217;t pump more and more power into processors.</p>
<p>You might argue that we can cool chips better or use more powerful batteries&#8230; but none of this helps us if we have to grow the energy usage exponentially. Granted, we might be able to heat our homes with computers, at least those of us living in cold regions&#8230; but who wants an iPhone that burns through your skin?</p>
<p>How does our brain work despite these limitations? Our neurons are large and we have many of them&#8230; much more than we have transistors in any computer. The total computing power of our brain far exceeds the computing power of most powerful silicon processor ever made&#8230;  How do we not burst into flame? The secret is that our neurons are not all firing at the same time billions of times per second. </p>
<p>You might have heard that we only use 10% of our brain. Then you have been told that this is a myth. There is even <a href="https://en.wikipedia.org/wiki/Ten_percent_of_the_brain_myth">a Wikipedia page about this &#8220;myth&#8221;</a>. But it is not a myth. At any one time, you are probably using less than 1% of your brain:</p>
<blockquote><p>The cost of a single spike is high, and this severely limits, possibly to fewer than 1%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. (Lennie, 2003)</p></blockquote>
<p>So, the truth is that you are not even using 10% of your brain&#8230; more like 1%&#8230; Your brain is in constant power-saving mode.</p>
<p>This, I should add, can make us optimistic about intelligence enhancement technologies. It seems entirely possible to force the brain into a higher level of activity, with the trade-off that it might use more energy and generate more heat. For our ancestors, energy was scarce and the weather could be torrid. We can afford to control our temperature, and we overeat.</p>
<p>But, even so, there is no way you could get half of your neurons firing simultaneously. Our biology could not sustain it. We would go into shock.</p>
<p>It stands to reason that our computers must follow the same pattern. We can build ever larger chips, with densely packed transistors&#8230; but most of these circuits must remain inactive most of the time&#8230; that&#8217;s what they call &#8220;<a href="https://en.wikipedia.org/wiki/Dark_silicon">dark silicon</a>&#8220;. &#8220;Dark silicon&#8221; assumes that our technology has to be &#8220;silicon-based&#8221;, clearly something that may change in the near future, so let us use the term &#8220;dark circuits&#8221; instead.</p>
<p>Pause to consider: it means that in the near future, you will buy a computer made of circuits that remain mostly inactive most of the time. In fact, we might imagine a law of the sort&#8230;</p>
<blockquote><p>The percentage of dark circuits  will double every two years in commodity computers.</p></blockquote>
<p>That sounds a bit crazy. This means that one day, we might use only 1% of the circuits in your processors at any one time&mdash;not unlike our brain. Though it sounds crazy, we will see our first effect of this &#8220;law&#8221; with the rise of <a href="https://en.wikipedia.org/wiki/Non-volatile_memory">non-volatile memory</a>. Your current computer relies on volatile memory made of transistors that must be constantly &#8220;charged&#8221; to remain active. As the transistors stop shrinking, this means that the energy usage of RAM per byte will plateau. Hence, the energy usage due to memory will start growing exponentially, assuming that the amount of memory in systems grows exponentially. Exponentially growing energy usage is not good. So we will switch, in part or in full, to non-volatile memory, and that&#8217;s an example of &#8220;dark circuits&#8221;. It is often called &#8220;dark memory&#8221;.</p>
<p>You may assume that memory systems in a computer do not use much energy, but by several accounts, they often account for half of the energy usage because moving data is expensive. If we are to have computers with gigantic memory capacities, we cannot keep moving most of the data most of the time.</p>
<p>In this hypothetical future, what might programming look like? You have lots and lots of fast memory. You have lots and lots of efficient circuits capable of various computations. But we must increasingly &#8220;budget&#8221; our memory transfers and accesses. Moving data takes energy and creates heat.  Moreover, though you might have gigantic computational power, you cannot afford to keep it on for long, because you will either run out of energy or overheat your systems.</p>
<p>Programming might start to sound a lot like biology.</p>
<p><strong>Credit</strong>: This blog post benefited from an email exchange with Nathan Kurz.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=ohNKAJbKHkY:J--ARlVIFPM:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=ohNKAJbKHkY:J--ARlVIFPM:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/ohNKAJbKHkY" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/19/the-rise-of-dark-circuits/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/19/the-rise-of-dark-circuits/</feedburner:origLink></item>
		<item>
		<title>The memory usage of STL containers can be surprising</title>
		<link>http://feedproxy.google.com/~r/daniel-lemire/atom/~3/tcWe5x3vWFU/</link>
		<comments>http://lemire.me/blog/2016/09/15/the-memory-usage-of-stl-containers-can-be-surprising/#comments</comments>
		<pubDate>Thu, 15 Sep 2016 15:07:13 +0000</pubDate>
		<dc:creator><![CDATA[Daniel Lemire]]></dc:creator>
		
		<guid isPermaLink="false">http://lemire.me/blog/?p=11752</guid>
		<description>C++ remains one of the most popular languages today. One of the benefits of C++ is the built-in STL containers offering the standard data structures like vector, list, map, set. They are clean, well tested and well documented. If all you do is program in C++ all day, you might take STL for granted, but &amp;#8230; &lt;a href="http://lemire.me/blog/2016/09/15/the-memory-usage-of-stl-containers-can-be-surprising/" class="more-link"&gt;Continue reading &lt;span class="screen-reader-text"&gt;The memory usage of STL containers can be surprising&lt;/span&gt;&lt;/a&gt;</description>
				<content:encoded><![CDATA[<p>C++ remains one of the most popular languages today. One of the benefits of C++ is the built-in STL containers offering the standard data structures like vector, list, map, set. They are clean, well tested and well documented. </p>
<p>If all you do is program in C++ all day, you might take STL for granted, but more recent languages like Go or JavaScript do not have anything close to STL built-in. The fact that every C++ compiler comes with a decent set of STL containers is just very convenient.</p>
<p>STL containers have a few downsides. Getting the <a href="http://lemire.me/blog/2012/06/20/do-not-waste-time-with-stl-vectors/">very best performance out of them</a> can be tricky in part because they introduce so much abstraction.</p>
<p>Another potential problem with them is their memory usage. This is a &#8220;silent&#8221; problem that will only affect you some of the time, but when it does, it may come as a complete surprise.</p>
<p>I was giving a talk once to a group of developers, and we were joking about the memory usage of modern data structures. I was telling the audience that using close to 32 bits per 32-bit value stored in a container was pretty good sometimes. The organizer joked that one should not be surprised to use 32 bytes per 32-bit integer in Java. Actually, I don&#8217;t think the organizer was joking&#8230; he was being serious&#8230; but the audience thought he was joking.</p>
<p><a href="http://lemire.me/blog/2015/10/15/on-the-memory-usage-of-maps-in-java/">I wrote a blog post showing that each &#8220;Integer&#8221; in Java stored in an array uses 20 bytes</a> and that each entry in an Integer-Integer map could use 80 bytes. </p>
<p>Java is sometimes ridiculous in its memory usage. C++ is better, thankfully. But it is still not nearly as economical as you might expect.</p>
<p>I wrote a <a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/blob/master/2016/09/15/stlsizeof.cpp">small test</a>. Results will vary depending on your compiler, standard library, the size of the container, and so forth&#8230; You should run your own tests&#8230; Still, here are some numbers I got on my Mac:</p>
<table style="width:80%">
<tr>
<td  colspan="2">Storage cost in bytes per 32-bit entry</td>
</tr>
<tr>
<td>STL container</td>
<td>Storage</td>
</tr>
<tr>
<td><tt>std::vector</tt></td>
<td>4</td>
</tr>
<tr>
<td><tt>std::deque</tt></td>
<td>8</td>
</tr>
<tr>
<td><tt>std::list</tt></td>
<td>24</td>
</tr>
<tr>
<td><tt>std::set</tt></td>
<td>32</td>
</tr>
<tr>
<td><tt>std::unordered_set</tt></td>
<td>36</td>
</tr>
</table>
<p>(My Linux box gives slightly different numbers but the conclusion is the same.)</p>
<p>So there is no surprise regarding <tt>std::vector</tt>. It uses 4 bytes to store each 4 byte elements. It is very efficient.</p>
<p>However, both <tt>std::set</tt> and <tt>std::unordered_set</tt> use nearly an order of magnitude more memory than would be strictly necessary.</p>
<p>The problem with the level of abstraction offered by C++ is that you can be completly unaware of how much memory you are using.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/daniel-lemire/atom?a=tcWe5x3vWFU:fBw8NSHGHPc:D7DqB2pKExk"><img src="http://feeds.feedburner.com/~ff/daniel-lemire/atom?i=tcWe5x3vWFU:fBw8NSHGHPc:D7DqB2pKExk" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/daniel-lemire/atom/~4/tcWe5x3vWFU" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://lemire.me/blog/2016/09/15/the-memory-usage-of-stl-containers-can-be-surprising/feed/</wfw:commentRss>
		<slash:comments>15</slash:comments>
		<feedburner:origLink>http://lemire.me/blog/2016/09/15/the-memory-usage-of-stl-containers-can-be-surprising/</feedburner:origLink></item>
	</channel>
</rss><!-- Dynamic page generated in 1.615 seconds. --><!-- Cached page generated by WP-Super-Cache on 2016-10-18 13:33:08 --><!-- Compression = gzip -->
