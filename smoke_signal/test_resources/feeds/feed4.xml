<?xml version="1.0" encoding="iso-8859-1" ?>
<rss version="2.0">
<channel><title>Cryptology ePrint Archive</title>
<link>http://eprint.iacr.org/</link>
<description>Recently modified papers in the IACR Cryptology ePrint Archive</description>
<language>en-us</language>
<lastBuildDate>Wed, 19 Oct 2016 03:41:03 -0600</lastBuildDate>
<webMaster>webmaster@iacr.org</webMaster>
<managingEditor>eprint-editor@iacr.org</managingEditor>
<generator>None of your business</generator>
<ttl>60</ttl>
<item>
<link>http://eprint.iacr.org/2016/995</link>
<title><![CDATA[Measuring small subgroup attacks against Diffie-Hellman]]>, by Luke Valenta and David Adrian and Antonio Sanso and Shaanan Cohney and Joshua Fried and Marcella Hastings and J. Alex Halderman and Nadia Heninger</title>
<description><![CDATA[Several recent standards, including NIST SP 800- 56A and RFC 5114, advocate the use of "DSA" parameters for Diffie-Hellman key exchange. While it is possible to use such parameters securely, additional validation checks are necessary to prevent well-known and potentially devastating attacks. In this paper, we observe that many Diffie-Hellman implementations do not properly validate key exchange inputs. Combined with other protocol properties and implementation choices, this can radically decrease security. We measure the prevalence of these parameter choices in the wild for HTTPS, POP3S, SMTP with STARTTLS, SSH, IKEv1, and IKEv2, finding millions of hosts using DSA and other non-"safe" primes for Diffie-Hellman key exchange, many of them in combination with potentially vulnerable behaviors. We examine over 20 open-source cryptographic libraries and applications and observe that until January 2016, not a single one validated subgroup orders by default. We found feasible full or partial key recovery vulnerabilities in OpenSSL, the Exim mail server, the Unbound DNS client, and Amazon's load balancer, as well as susceptibility to weaker attacks in many other applications.
]]></description>
<guid>http://eprint.iacr.org/2016/995</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/994</link>
<title><![CDATA[Improving Authenticated Dynamic Dictionaries, with Applications to Cryptocurrencies]]>, by Leonid Reyzin and Dmitry Meshkov and Alexander Chepurnoy and Sasha Ivanov</title>
<description><![CDATA[We improve the design and implementation of two-party and three-party authenticated dynamic dictionaries and apply these dictionaries to cryptocurrency ledgers.

A public ledger (blockchain) in a cryptocurrency needs to be easily verifiable. However, maintaining a data structure of all account balances, in order to verify whether a transaction is valid, can be quite burdensome: a verifier who does not have the large amount of RAM required for the data structure will perform slowly because of the need to continually access secondary storage. We use experiments to demonstrate that authenticated dynamic dictionaries can considerably reduce verifier load. On the other hand, per-transaction proofs generated by authenticated dictionaries increase the size of the blockchain, which motivates us to find a solution with most compact proofs.

Our improvements to the design of authenticated dictionaries reduce proof size and speed up verification by 1.4-2.5 times, making them better suited for the cryptocurrency application. We further show that proofs for multiple transactions in a single block can compressed together, reducing their total length by approximately an additional factor of 2.
]]></description>
<guid>http://eprint.iacr.org/2016/994</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/993</link>
<title><![CDATA[Comparing Sboxes of Ciphers from the Perspective of Side-Channel Attacks]]>, by Liran Lerman and Olivier Markowitch and Nikita Veshchikov</title>
<description><![CDATA[Side-channel attacks exploit physical characteristics of implementations of cryptographic algorithms in order to extract sensitive information such as the secret key. These physical attacks are among the most powerful attacks against real-world cryptosystems. This paper analyses the non-linear part (called Sboxes) of ciphers, which is often targeted by implementation attacks. We analyse Sboxes of several candidates that were sub- mitted to the competition on authenticated encryption (CAESAR) as well as several other ciphers. We compare theoretical metrics with results from simulations and with real experiments. In this paper, we demonstrate that, in some contexts, the theoretical metrics provide no information on the resiliency of the Sboxes against side-channel attacks.
]]></description>
<guid>http://eprint.iacr.org/2016/993</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/992</link>
<title><![CDATA[Estimating the cost of generic quantum pre-image attacks on SHA-2 and SHA-3]]>, by Matthew Amy and Olivia Di Matteo and Vlad Gheorghiu and Michele Mosca and Alex Parent and John Schanck</title>
<description><![CDATA[We investigate the cost of Grover's quantum search algorithm when used in the
context of pre-image attacks on the SHA-2 and SHA-3 families of
hash functions.  Our cost model assumes that the attack is run on a surface
code based fault-tolerant quantum computer. Our estimates rely on a time-area
metric that costs the number of logical qubits times the depth of the circuit
in units of surface code cycles.  As a surface code cycle involves a
significant classical processing stage, our cost estimates allow for crude, but
direct, comparisons of classical and quantum algorithms.

We exhibit a circuit for a pre-image attack on SHA-256 that is
approximately $2^{153.8}$ surface code cycles deep and requires approximately
$2^{12.6}$ logical qubits. This yields an overall cost of $2^{166.4}$
logical-qubit-cycles.  Likewise we exhibit a SHA3-256 circuit that is
approximately $2^{146.5}$ surface code cycles deep and requires approximately
$2^{20}$ logical qubits for a total cost of, again, $2^{166.5}$
logical-qubit-cycles. Both attacks require on the order of $2^{128}$ queries in
a quantum black-box model, hence our results suggest that executing these
attacks may be as much as $275$ billion times more expensive than one would
expect from the simple query analysis.
]]></description>
<guid>http://eprint.iacr.org/2016/992</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/991</link>
<title><![CDATA[Bootstrapping the Blockchain --- Directly]]>, by Juan A. Garay and Aggelos Kiayias and Nikos Leonardos and Giorgos Panagiotakos</title>
<description><![CDATA[The Bitcoin backbone protocol [Eurocrypt 2015] extracts 
basic properties of Bitcoin's underlying {\em blockchain} data structure, such as ``common prefix'' and ``chain quality,'' and shows how fundamental applications including consensus and a robust public transaction ledger can be built on top of them. The underlying assumptions are ``proofs of work'' (POWs), adversarial hashing power strictly less than $1/2$ {\em and} no adversarial pre-computation---or, alternatively, the existence of an unpredictable ``genesis'' block.

In this paper we show how to remove the latter assumption, presenting a ``bootstrapped'' Bitcoin-like blockchain protocol relying on POWs that builds genesis blocks ``from scratch'' in the presence of adversarial pre-computation. The only known previous result  in the same setting (unauthenticated parties, no trusted setup) [Crypto 2015]  is indirect in the sense of creating a PKI first 
and then employing conventional PKI-based authenticated communication. 

With our construction we establish that consensus can be solved directly by a blockchain protocol {\em without} trusted setup assuming an honest majority (in terms of computational power). 
%
We also formalize {\em miner unlinkability}, a privacy property for blockchain protocols, 
and demonstrate that our protocol retains the same level of miner unlinkability as Bitcoin itself.
]]></description>
<guid>http://eprint.iacr.org/2016/991</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/990</link>
<title><![CDATA[Revisiting the Wrong-Key-Randomization Hypothesis]]>, by Tomer Ashur and Tim Beyne and Vincent Rijmen</title>
<description><![CDATA[Linear cryptanalysis can be considered to be one of the strongest techniques in the cryptanalyst's arsenal. In most cases, Matsui's Algorithm 2 is used for the key recovery part of the attack. The success rate analysis of this algorithm is based on an assumption regarding the bias of a linear approximation for a wrong key, known as the wrong-key-randomization hypothesis.
This hypothesis was refined by Bogdanov and Tischhauser to take into account the stochastic nature of the bias for a wrong key.
We provide further refinements to the analysis of Matsui's algorithm 2 by considering the more natural setting of sampling without replacement.
This paper derives the distribution for the observed bias for wrong keys when sampling is done without replacement and shows that less data is required when duplicate pairs are discarded.
It also develops formulas for the success probability and the required data complexity when this approach is taken. The formulas predict that the success probability may reach a peak, then decrease as more pairs are considered. We provide a new explanation for this behavior and derive the conditions for encountering it. We empirically verify our results and compare them to previous work.
]]></description>
<guid>http://eprint.iacr.org/2016/990</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/989</link>
<title><![CDATA[Scrypt is Maximally Memory-Hard]]>, by Jo\"el Alwen and Binyi Chen and Krzysztof Pietrzak and Leonid Reyzin and Stefano Tessaro</title>
<description><![CDATA[Memory-hard functions (MHFs) are hash algorithms whose evaluation cost is dominated by memory cost. As memory, unlike computation, costs about the same across different platforms, MHFs cannot be evaluated at significantly lower cost on dedicated hardware like ASICs. MHFs have found widespread applications including password hashing, key derivation, and proofs-of-work.

This paper focuses on scrypt, a simple candidate MHF designed by Percival, and described in RFC 7914. It has been used within a number of cryptocurrencies (e.g., Litecoin and Dogecoin) and has been an inspiration for Argon2d, one of the winners of the recent password-hashing competition. Despite its popularity, no rigorous lower bounds on its memory complexity are known.

We prove that scrypt is optimally memory hard, i.e., its cumulative memory complexity (cmc) in the parallel random oracle model is $\Omega(n^2 w)$, where $w$ and $n$ are the output length and number of invocations of the underlying hash function, respectively. High cmc is a strong security target for MHFs introduced by Alwen and Serbinenko (STOC '15) which implies high memory cost even for adversaries who can amortise the cost over many evaluations and evaluate the underlying hash functions many times in parallel. Our proof is the first showing optimal memory hardness for any MHF.

Our result improves both quantitatively and qualitatively upon the recent work by Alwen et al. (EUROCRYPT '16) who proved a weaker lower bound of $\Omega(n^2 w /\log^2 n)$ for a restricted class of adversaries.
]]></description>
<guid>http://eprint.iacr.org/2016/989</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/988</link>
<title><![CDATA[On Probabilistic Checking in Perfect Zero Knowledge]]>, by Eli Ben-Sasson and Alessandro Chiesa and Michael A. Forbes and Ariel Gabizon and Michael Riabzev and Nicholas Spooner</title>
<description><![CDATA[We present the first constructions of *single*-prover proof systems that achieve *perfect* zero knowledge (PZK) for languages beyond NP, under no intractability assumptions:

1. The complexity class #P has PZK proofs in the model of Interactive PCPs (IPCPs) [KR08], where the verifier first receives from the prover a PCP and then engages with the prover in an Interactive Proof (IP).

2. The complexity class NEXP has PZK proofs in the model of Interactive Oracle Proofs (IOPs) [BCS16,RRR16], where the verifier, in every round of interaction, receives a PCP from the prover.

Unlike PZK multi-prover proof systems [BGKW88], PZK single-prover proof systems are elusive: PZK IPs are limited to AM &#4278; coAM [F87,AH91], while known PCPs and IPCPs achieve only *statistical* simulation [KPT97,GIMS10]. Recent work [BCGV16] has achieved PZK for IOPs but only for languages in NP, while our results go beyond it.

Our constructions rely on *succinct* simulators that enable us to "simulate beyond NP", achieving exponential savings in efficiency over [BCGV16]. These simulators crucially rely on solving a problem that lies at the intersection of coding theory, linear algebra, and computational complexity, which we call the *succinct constraint detection* problem, and consists of detecting dual constraints with polynomial support size for codes of exponential block length. Our two results rely on solutions to this problem for fundamental classes of linear codes:

* An algorithm to detect constraints for Reed--Muller codes of exponential length.

* An algorithm to detect constraints for PCPs of Proximity of Reed--Solomon codes [BS08] of exponential degree.

The first algorithm exploits the Raz--Shpilka [RS05] deterministic polynomial identity testing algorithm, and shows, to our knowledge, a first connection of algebraic complexity theory with zero knowledge. Along the way, we give a perfect zero knowledge analogue of the celebrated sumcheck protocol [LFKN92], by leveraging both succinct constraint detection and low-degree testing. The second algorithm exploits the recursive structure of the PCPs of Proximity to show that small-support constraints are "locally" spanned by a small number of small-support constraints.
]]></description>
<guid>http://eprint.iacr.org/2016/988</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/987</link>
<title><![CDATA[A Key to Success -- Success Exponents for Side-Channel Distinguishers]]>, by Sylvain Guilley and Annelie Heuser and Olivier Rioul</title>
<description><![CDATA[The success rate is the classical metric for evaluating the
performance of side-channel attacks. It is generally computed empirically from measurements for a particular device or using simulations. Closed-form expressions of success rate are desirable because they provide an explicit functional dependence on relevant parameters such as number of measurements and signal-to-noise ratio which help to understand the effectiveness of a given attack and how one can mitigate its threat by countermeasures. However, such closed-form expressions involve high-dimensional complex statistical functions that are hard to estimate.

In this paper, we define the success exponent (SE) of an arbitrary side-channel distinguisher as the first-order exponent of the success rate as the number of measurements increases. Under fairly general assumptions such as soundness, we give a general simple formula for any arbitrary distinguisher and derive closed-form expressions of it for DoM, CPA, MIA and the optimal distinguisher when the model is known (template attack). For DoM and CPA our results are in line with the literature.

Experiments confirm that the theoretical closed-form expression of the
SE coincides with the empirically computed one, even for reasonably
small numbers of measurements. Finally, we highlight that our study
raises many new perspectives for comparing and evaluating side-channel
attacks, countermeasures and implementations.
]]></description>
<guid>http://eprint.iacr.org/2016/987</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/986</link>
<title><![CDATA[Fast Arithmetic Modulo $2^xp^y\pm 1$]]>, by Joppe W. Bos and Simon Friedberger</title>
<description><![CDATA[We give a systematic overview of techniques to compute efficient arithmetic modulo $2^xp^y\pm 1$. This is useful for computations in the supersingular isogeny Diffie-Hellman (SIDH) key-exchange protocol
which is one of the more recent contenders in the post-quantum public-key arena. One of the main computational bottlenecks in this key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape.

Recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. Our overview shows that in the SIDH setting, where arithmetic over a quadratic extension field is required, the approaches based on Montgomery multiplication are to be preferred. Furthermore, the outcome of our search reveals that there exist moduli which result in even faster implementations.
]]></description>
<guid>http://eprint.iacr.org/2016/986</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/985</link>
<title><![CDATA[Hash First, Argue Later: Adaptive Verifiable Computations on Outsourced Data]]>, by Dario Fiore and Cédric Fournet and Esha Ghosh and Markulf Kohlweiss and Olga Ohrimenko and Bryan Parno</title>
<description><![CDATA[Proof systems for verifiable computation (VC) have the potential to make cloud outsourcing more trustworthy. Recent schemes enable a verifier with limited resources to delegate large computations and verify their outcome based on succinct arguments: verification complexity is linear in the size of the inputs and outputs (not the size of the computation). However, cloud computing also often involves large amounts of data, which may exceed the local storage and I/O capabilities of the verifier, and thus limit the use of VC.

In this paper, we investigate multi-relation hash & prove schemes for verifiable computations that operate on succinct data hashes. Hence, the verifier delegates both storage and computation to an untrusted worker. She uploads data and keeps hashes; exchanges hashes with other parties; verifies arguments that consume and produce hashes; and selectively downloads the actual data she needs to access.

Existing instantiations that fit our definition either target restricted classes of computations  or employ relatively inefficient techniques. Instead, we propose efficient constructions that lift classes of existing arguments schemes for fixed relations to multi-relation hash & prove schemes. Our schemes (1) rely on hash algorithms that run linearly in the size of the input; (2) enable constant-time verification of arguments on hashed inputs; (3) incur minimal overhead for the prover. Their main benefit is to amortize the linear cost for the verifier across all relations with shared I/O. Concretely, compared to solutions that can be obtained from prior work, our new hash & prove constructions yield a 1,400x speed-up for provers. We also explain how to further reduce the linear verification costs by partially outsourcing the hash computation itself, obtaining a 480x speed-up when applied to existing VC schemes, even on single-relation executions.
]]></description>
<guid>http://eprint.iacr.org/2016/985</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/984</link>
<title><![CDATA[Design Strategies for ARX with Provable Bounds: SPARX and LAX (Full Version)]]>, by Daniel Dinu and Léo Perrin and Aleksei Udovenko and Vesselin Velichkov and Johann Großschädl and Alex Biryukov</title>
<description><![CDATA[We present, for the first time, a general strategy for designing ARX symmetric-key primitives with provable resistance against single-trail differential and linear cryptanalysis. The latter has been a long standing open problem in the area of ARX design. The wide trail design strategy (WTS), that is at the basis of many S-box based ciphers, including the AES, is not suitable for ARX designs due to the lack of S-boxes in the latter. In this paper we address the mentioned limitation by proposing the long trail design strategy (LTS) -- a dual of the WTS that is applicable (but not limited) to ARX constructions. In contrast to the WTS, that prescribes the use of small and efficient S-boxes at the expense of heavy linear layers with strong mixing properties, the LTS advocates the use of large (ARX-based) S-Boxes together with sparse linear layers. With the help of the so-called Long Trail argument, a designer can bound the maximum differential and linear probabilities for any number of rounds of a cipher built according to the LTS.
    
To illustrate the effectiveness of the new strategy, we propose SPARX -- a family of ARX-based block ciphers designed according to the LTS. SPARX has 32-bit ARX-based S-boxes and has provable bounds against differential and linear cryptanalysis. In addition, SPARX is very efficient on a number of embedded platforms. Its optimized software implementation ranks in the top 6 of the most software-efficient ciphers along with SIMON, SPECK, Chaskey, LEA and RECTANGLE.

As a second contribution we propose another strategy for designing ARX ciphers with provable properties, that is completely independent of the LTS. It is motivated by a challenge proposed earlier by Wall{\'{e}}n and uses the differential properties of modular addition to minimize the maximum differential probability across multiple rounds of a cipher. A new primitive, called LAX, is designed following those principles. LAX partly solves the Wall{\'{e}}n challenge.
]]></description>
<guid>http://eprint.iacr.org/2016/984</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/983</link>
<title><![CDATA[Exact Security Analysis of Hash-then-Mask Type Probabilistic MAC Constructions]]>, by Avijit Dutta and Ashwin Jha and Mridul Nandi</title>
<description><![CDATA[Probabilistic  MAC (message authentication code) is an alternative choice for a stateful MAC where maintaining internal state may be difficult or unsafe. Usually tag of a probabilistic MAC consists of an $m$-bit random coin (also called {\em salt}) and an $n$-bit core-tag depending on the salt. In terms of the security, probabilistic MAC falls under birthday collision of salts which is absent in stateful MAC. XMACR is an example of probabilistic MAC which remains secure up to $o(2^{m/2})$ tag generation queries. To achieve security beyond birthday in $n$, one can naturally use a large salt. For example, $\mathrm{MACRX}_3$ sets $m = 3n$ and provides security up to $o(2^{n})$ tag-generation queries. Large salt may restrict its applicability as it increases the cost of random string generation as well as the size of the overall tag. RWMAC (randomized version of WMAC) provides similar security with $m = n$ but it uses a PRF (pseudorandom function) over $2n$-bit inputs which is naturally more costlier than those over $n$-bit inputs. Achieving beyond birthday security using $n$-bit PRF and $n$-bit salt is a practical and challenging problem. Minematsu in FSE 2010 proposed Enhanced Hash-then-Mask (\tx{EHtM}) using $n$-bit salt and showed its security up to $o(2^{2n/3})$ tag-generation queries. In this paper we revisit this construction and we provide exact security analysis of \tx{EHtM}. In particular, we show that it has higher security, namely up to $o(2^{3n/4})$ queries, than what claimed by the designer. Moreover, we demonstrate a single attempt forgery attack which makes about $2^{3n/4}$ tag generation queries. XMACR and \tx{EHtM} follow the hash-then-mask paradigm due to Carter-Wegman. We revisit six possible constructions following hash-then-mask paradigm and we provide exact security analysis for all of these constructions, some of which however were known before.
]]></description>
<guid>http://eprint.iacr.org/2016/983</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/982</link>
<title><![CDATA[Securing Systems with Scarce Entropy: LWE-Based Lossless Computational Fuzzy Extractor for the IoT]]>, by Christopher Huth and Daniela Becker and Jorge Guajardo and Paul Duplys and Tim G\"uneysu</title>
<description><![CDATA[With the advent of the Internet of Things, lightweight devices
necessitate secure and cost-efficient key storage. Since traditional
secure storage is expensive, the valuable entropy could originate from
noisy sources, for which fuzzy extractors allow strong key derivation.
While providing information-theoretic security, fuzzy extractors require
large amount of input entropy to account for entropy loss in the key
extraction process. It has been shown by Fuller et al. [20] that the entropy
loss can be reduced if the requirement is relaxed to computational
security based on the hardness of the Learning with Errors problem.
Using this computational fuzzy extractor, we show how to construct a
device-server authentication system providing outsider chosen perturbation
security and pre-application robustness. We present the first implementation
of a lossless computational fuzzy extractor where the entropy
of the source equals the entropy of the key on a constrained device.
The implementation needs only 1.45KB of SRAM and 9.8KB of Flash
memory on an 8-bit microcontroller. We compare our implementation to
existing work in terms of security, while achieving no entropy loss.
]]></description>
<guid>http://eprint.iacr.org/2016/982</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/981</link>
<title><![CDATA[Efficient No-dictionary Verifiable SSE]]>, by Wakaha Ogata and Kaoru Kurosawa</title>
<description><![CDATA[In the model of "no-dictionary" verifiable searchable symmetric encryption (SSE) scheme, a client does not need to keep  the set of keywords ${\cal W}$ in the search phase, where ${\cal W}$ is called a dictionary. Still a malicious server cannot cheat the client by saying that ``your search word $w$ does not exist in the dictionary ${\cal W}$" when it exists. In the previous such schemes, it takes $O(\log m)$ time for the server  to prove that $w \not\in {\cal W}$, where $m=|{\cal W}|$ is the number of keywords.
  In this paper, we show a generic method  to transform any SSE scheme (that is only secure against passive adversaries) to a no-dictionary verifiable SSE scheme. In the transformed scheme, it takes only $O(1)$ time for the server  to prove that $w \not\in {\cal W}$.
]]></description>
<guid>http://eprint.iacr.org/2016/981</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/980</link>
<title><![CDATA[TruSpy: Cache Side-Channel Information Leakage from the Secure World on ARM Devices]]>, by Ning Zhang and Kun Sun and Deborah Shands and Wenjing Lou and Y. Thomas Hou</title>
<description><![CDATA[As smart, embedded devices are increasingly integrated into our daily life, the security of these devices has become a major concern. The ARM processor family, which powers more than 60% of embedded devices, introduced TrustZone technology to offer security protection via an isolated execution environment called secure world. Caches in TrustZone-enabled processors are extended with a non-secure (NS) bit to indicate whether a cache line is used by the secure world or the normal world. This cache design improves system performance by eliminating the need to perform cache flush during world switches; however, it also enables cache contention between the two worlds.

In this work, we present TruSpy, the first study of timingbased cache side-channel information leakage of TrustZone. Our proposed attack exploits the cache contention between normal world and secure world to recover secret information from secure world. Two attacks are proposed in TruSpy, namely, the normal world OS attack and the normal world Android app attack. In the OS-based attack, the attacker is able to access virtual-to-physical address translation and high precision timers. In the Android app-based attack, these tools are unavailable to the attacker, so we devise a novel method that uses the expected channel statistics to allocate memory for cache probing. We also show how an attacker might use the less accurate performance event interface as a timer.

Using the T-table based AES implementation in OpenSSL 1.0.1f as an example, we demonstrate that it is possible for a normal world attacker to steal a fine-grained secret from the secure world using a timing-based cache side-channel. We can recover the full AES encryption key via either the OSbased attack or the Android app-based attack. Since our zero permission TruSpy attack is based on the cache design in TrustZone enabled ARM processors, it poses a significant threat to a wide array of devices. To mitigate the newly discovered threat, we also propose both application-based and system-oriented countermeasures.
]]></description>
<guid>http://eprint.iacr.org/2016/980</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/979</link>
<title><![CDATA[The Reason Why Some Divide-and-Conquer Algorithms Cannot Be Efficiently Implemented]]>, by Zhengjun Cao, Lihua Liu</title>
<description><![CDATA[In the literature there are some divide-and-conquer algorithms, such as
Karatsuba's algorithm  and Strassen's algorithm, which play a key role in analyzing the performance of some  cryptographic protocols and attacks. But we find these algorithms are  rarely practically implemented although their theoretical complexities are attractive. In this paper, we  point out that the reason of this phenomenon is that decomposing the  original problem into subproblems does not take constant time.  The type of problem decomposition results in data expand exponentially. In such case the time for organizing data (including assigning address, writing and reading) which is conventionally ignored, accumulates significantly.
]]></description>
<guid>http://eprint.iacr.org/2016/979</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/978</link>
<title><![CDATA[Testing the Trustworthiness of IC Testing: An Oracle-less Attack on IC Camouflaging]]>, by Muhammad Yasin and Ozgur Sinanoglu and Jeyavijayan Rajendran </title>
<description><![CDATA[Test of integrated circuits (ICs) is essential to ensure their quality; the test is meant to prevent defective and out-of-spec ICs  from  entering  into  the  supply  chain.  The  test  is  conducted by  comparing  the  observed  IC  output  with  the  expected  test responses for a set of test patterns; the test patterns are generated using   automatic   test   pattern   generation   algorithms.   Existing test-pattern  generation  algorithms  aim  to  achieve  higher  fault coverage at lower test costs. In an attempt to reduce the size of test data, these algorithms reveal the maximum information about the internal circuit structure. This is realized through sensitizing the internal nets to the outputs as much as possible, unintentionally leaking  the  secrets  embedded  in  the  circuit  as  well.

In this paper, we present HackTest,  an  attack  that  extracts secret information generated in the test data, even if the test data does not explicitly contain the secret.  HackTest  can  break  the  existing  intellectual  property  (IP) protection techniques, such as  camouflaging, within two minutes for   our  benchmarks   using   only   the  camouflaged   layout   and the  test  data.  HackTest  applies  to  all  existing  camouflaged  gate-selection  techniques  and  is  successful  even  in  the  presence  of state-of-the-art test infrastructure, i.e. test data compression  circuits.  Our attack necessitates that the IC test data generation algorithms be reinforced with security. We also discuss potential countermeasures to prevent HackTest.
]]></description>
<guid>http://eprint.iacr.org/2016/978</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/977</link>
<title><![CDATA[Side channels in deduplication: trade-offs between leakage and efficiency]]>, by Frederik Armknecht and Colin Boyd and Gareth T. Davies and Kristian Gjøsteen and Mohsen Toorani</title>
<description><![CDATA[Deduplication removes redundant copies of files or data blocks stored on the cloud. Client-side deduplication, where the client only uploads the file upon the request of the server, provides major storage and bandwidth savings, but introduces a number of security concerns. Harnik et al. (2010) showed how cross-user client-side deduplication inherently gives the adversary access to a (noisy) side-channel that may divulge whether or not a particular file is stored on the server, leading to leakage of user information. We provide formal definitions for deduplication strategies and their security in terms of adversarial advantage. Using these definitions, we prove a bound characterizing the best trade-off between security and efficiency in natural scenarios.
]]></description>
<guid>http://eprint.iacr.org/2016/977</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/976</link>
<title><![CDATA[On Adaptively Secure Multiparty Computation with a Short CRS]]>, by Ran Cohen and Chris Peikert</title>
<description><![CDATA[In the setting of multiparty computation, a set of mutually distrusting parties wish to securely compute a joint function of their private inputs. A protocol is adaptively secure if honest parties might get corrupted \emph{after} the protocol has started. Recently (TCC 2015) three constant-round adaptively secure
protocols were presented [CGP15, DKR15, GP15]. All three constructions assume that the parties have access to a \emph{common reference string} (CRS) whose size depends on the function to compute, even when facing semi-honest adversaries. It is unknown whether constant-round adaptively secure protocols exist, without assuming access to such a CRS.

In this work, we study adaptively secure protocols which only rely on a short CRS that is independent on the function to compute.
First, we raise a subtle issue relating to the usage of \emph{non-interactive non-committing encryption} within security proofs in the UC framework, and explain how to overcome it. We demonstrate the problem in the security proof of the adaptively secure oblivious-transfer protocol from [CLOS02] and provide a complete proof of this protocol.

Next, we consider the two-party setting where one of the parties has a polynomial-size input domain, yet the other has no constraints on its input. We show that assuming the existence of adaptively secure oblivious transfer, every deterministic functionality can be computed with adaptive security in a constant number of rounds.

Finally, we present a new primitive called \emph{non-committing indistinguishability obfuscation}, and show that this primitive is \emph{complete} for constructing adaptively secure protocols with round complexity independent of the function.
]]></description>
<guid>http://eprint.iacr.org/2016/976</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/975</link>
<title><![CDATA[(Universal) Unconditional Verifiability in E-Voting without Trusted Parties]]>, by Gina Gallegos-Garcia and Vincenzo Iovino and Alfredo Rial and Peter B. Roenne and Peter Y. A. Ryan</title>
<description><![CDATA[In e-voting protocol design, cryptographers must balance usability and strong security guarantees, such as privacy and verifiability. In traditional e-voting protocols, privacy is often provided by a trusted authority that learns the votes and computes the tally. Some protocols replace the trusted authority by a set of authorities, and privacy is guaranteed if less than a threshold number of authorities are corrupt. For verifiability, stronger security guarantees are demanded. Typically, corrupt authorities that try to fake the result of the tally must always be detected.

To provide verifiability, many e-voting protocols use Non-Interactive Zero-Knowledge proofs (NIZKs). Thanks to their non-interactive nature, NIZKs allow anybody, including third parties that do not participate in the protocol, to verify the correctness of the tally. Therefore, NIZKs can be used to obtain universal verifiability. Additionally, NIZKs also improve usability because they allow voters to cast a vote using a non-interactive protocol.

The disadvantage of NIZKs is that their security is based on setup assumptions such as the common reference string (CRS) or the random oracle (RO) model. The former requires a trusted party for the generation of a common reference string. The latter, though a popular methodology for designing secure protocols, has been shown to be unsound.

In this paper, we address the design of an e-voting protocol that provides verifiability without any trust assumptions, where verifiability here is meant without eligibility verification. We show that Non-Interactive Witness-Indistinguishable proofs (NIWI) can be used for this purpose. The e-voting scheme is private under the Decision Linear assumption, while verifiability holds unconditionally. To our knowledge, this is the first private e-voting scheme with perfect universal verifiability, i.e. one in which the probability of a fake tally not being detected is 0, and with non-interactive protocols that does not rely on trust assumptions.
]]></description>
<guid>http://eprint.iacr.org/2016/975</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/974</link>
<title><![CDATA[Server-Aided Revocable Identity-Based Encryption from Lattices]]>, by Khoa Nguyen and Huaxiong Wang and Juanyang Zhang</title>
<description><![CDATA[Server-aided revocable identity-based encryption (SR-IBE), recently proposed by Qin et al. at ESORICS 2015, offers significant advantages over previous user revocation mechanisms in the scope of IBE. In this new system model, almost all the workloads on users are delegated to an untrusted server, and users can compute decryption keys at any time period without having to communicate with either the key generation center or the server.

In this paper, inspired by Qin et al.'s work, we design the first SR-IBE scheme from lattice assumptions. Our scheme is more efficient than existing constructions of lattice-based revocable IBE. We prove that the scheme is selectively secure in the standard model, based on the hardness of the Learning with Errors problem. At the heart of our design is a "double encryption" mechanism that enables smooth interactions between the message sender and the server, as well as between the server and the recipient, while ensuring the confidentiality of messages.
]]></description>
<guid>http://eprint.iacr.org/2016/974</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/973</link>
<title><![CDATA[Invariant Subspace Attack Against Midori64 and The Resistance Criteria for S-box Designs]]>, by Jian Guo and J\'er\'emy Jean andIvica Nikoli\'c and Kexin Qiao andYu Sasaki and Siang Meng Sim</title>
<description><![CDATA[We present an invariant subspace attack on the block cipher Midori64,
proposed at Asiacrypt 2015. Our analysis shows that Midori64 has a class of 2^{32} weak keys. Under any such key, the cipher can be distinguished with only a single chosen query, and the key can be recovered in 2^{16} time with two chosen queries. As both the distinguisher and the key recovery have very low complexities, we confirm our analysis by implementing the attacks.
Some tweaks of round constants make Midori64 more resistant to the attacks, but some lead to even larger weak-key classes. To eliminate the dependency on the round constants, we investigate alternative S-boxes for Midori64 that provide certain level of security against the found invariant subspace attacks, regardless of the choice of the round constants. Our search for S-boxes is enhanced with a dedicated tool which evaluates the depth of any given 4-bit S-box that satisfies certain design criteria. The tool may be of independent interest to future S-box designs.
]]></description>
<guid>http://eprint.iacr.org/2016/973</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/972</link>
<title><![CDATA[Revealing Encryption for Partial Ordering]]>, by Helene Haagh and Yue Ji and Chenxing Li and Claudio Orlandi and and Yifan Song</title>
<description><![CDATA[We generalize the cryptographic notion of Order Revealing Encryption (ORE) to arbitrary functions and we present a construction that allows to determine the (partial) ordering of two vectors i.e., given E(x) and E(y) it is possible to learn whether x is less than or equal to y, y is less than or equal to x or whether x and y are incomparable. This is the first non-trivial example of a Revealing Encryption (RE) scheme with output larger than one bit, and which does not rely on cryptographic obfuscation or multilinear maps.
]]></description>
<guid>http://eprint.iacr.org/2016/972</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/971</link>
<title><![CDATA[Authenticated communication from Quantum Readout of PUFs]]>, by B. Skoric and P.W.H. Pinkse and A.P. Mosk</title>
<description><![CDATA[Quantum Readout of Physical Unclonable Functions (PUFs) is a recently introduced method for remote authentication of objects.
We present an extension of the protocol to enable the authentication of data: a verifier can check if received classical data was sent by the PUF holder. We call this modification QR-d or, in the case of the optical-PUF implementation, QSA-d.
We discuss how QSA-d can be operated in a parallel way. We also present a protocol for authenticating quantum states.
]]></description>
<guid>http://eprint.iacr.org/2016/971</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/970</link>
<title><![CDATA[Statistical Analysis for Access-Driven Cache Attacks Against AES]]>, by Liwei Zhang; A. Adam Ding; Yunsi Fei; Zhen Hang Jiang</title>
<description><![CDATA[In recent years, side-channel timing attacks utilizing architectural behavior have been applied to cloud settings, presenting a realistic and serious cyber threat. Access-driven cache attacks allow the adversary to observe side-channel leakage (cache access pattern) of a critical cryptographic implementation to infer the secret key. However, what the attackers observe may deviate from the real cache footprint of the victim process, affecting the effectiveness of cache-based timing attacks using the observed leakage. 
Various countermeasures, including secure cache and architectures design, should also be evaluated accurately for their side-channel resilience. 
To address this need, this paper proposes a mathematical model for access-driven cache attacks, and derives explicit success rate formulas for those attacks. It is the first theoretical model that explicitly considers the misclassification errors for cache access and cache non-access by the victim cryptographic process.
We implement several access-driven cache attacks and use our models to evaluate them. We demonstrate that the proposed statistical model predicts the success rate of cache-based timing attacks accurately.  We also apply the model onto various cache defense architectures for evaluation.
]]></description>
<guid>http://eprint.iacr.org/2016/970</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/969</link>
<title><![CDATA[Garbling Gadgets for Boolean and Arithmetic Circuits]]>, by Marshall Ball and Tal Malkin and Mike Rosulek</title>
<description><![CDATA[We present simple, practical, and powerful new techniques for garbled circuits. These techniques result in significant concrete and asymptotic improvements over the state of the art, for several natural kinds of computations. 

For arithmetic circuits over the integers, our construction results in garbled circuits with {\em free} addition, weighted threshold gates with cost independent of fan-in, and exponentiation by a fixed
exponent with cost independent of the exponent. For boolean circuits,
our construction gives an {\em exponential} improvement over the state of the art for threshold gates (including AND/OR gates) of high fan-in.

Our construction can be efficiently instantiated with practical symmetric-key primitives (e.g., AES), and is proven secure under similar assumptions to that of the Free-XOR garbling scheme (Kolesnikov \& Schneider, ICALP 2008). We give an extensive comparison between our scheme and state-of-the-art garbling schemes applied to boolean circuits.
]]></description>
<guid>http://eprint.iacr.org/2016/969</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/968</link>
<title><![CDATA[System Clock and Power Supply Cross-Checking for Glitch Detection]]>, by Pei Luo and Chao Luo and Yunsi Fei</title>
<description><![CDATA[Cryptographic systems are vulnerable to different kinds of fault injection attacks. System clock glitch is one of the most widely used fault injection methods used in different attacks. In this paper, we propose a method to detect glitches in system clock to fight against clock glitch based fault attacks. We implement the proposed scheme in Virtex-5 FPGA and inject clock glitches into FPGA, results show that the proposed scheme can be easily implemented in both ASICs and FPGAs with very small overhead. Detection results show that the proposed scheme can detect very high frequency clock glitches with very high detection rate.
]]></description>
<guid>http://eprint.iacr.org/2016/968</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/967</link>
<title><![CDATA[Faulty Clock Detection for Crypto Circuits Against Differential Fault Analysis Attack]]>, by Pei Luo and Yunsi Fei</title>
<description><![CDATA[Clock glitch based Differential Fault Analysis (DFA) attack is a serious threat to cryptographic devices. Previous error detection schemes for cryptographic devices target improving the circuit reliability and cannot resist such DFA attacks. In this paper, we propose a novel faulty clock detection method which can be easily implemented either in FPGAs or integrated circuits to detect the glitches in system clock. Results show that the proposed method can detect glitches efficiently while needs very few system resource. It is also highly reconfigurable to tolerant clock inherent jitters, and will not involve complex design work for different processing technologies.
]]></description>
<guid>http://eprint.iacr.org/2016/967</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/966</link>
<title><![CDATA[High-speed VLSI implementation of Digit-serial Gaussian normal basis Multiplication over GF(2m)]]>, by Bahram Rashidi, Sayed Masoud Sayedi, Reza Rezaeian Farashahi</title>
<description><![CDATA[In this paper, by employing the logical effort technique an efficient and high-speed VLSI implementation of the digit-serial Gaussian normal basis multiplier is presented. It is constructed by using AND, XOR and XOR tree components. To have a low-cost implementation with low number of transistors, the block of AND gates are implemented by using NAND gates based on the property of the XOR gates in the XOR tree. To optimally decrease the delay and increase the drive ability of the circuit the logical effort method as an efficient method for sizing the transistors is employed. By using this method and also a 4-input XOR gate structure, the circuit is designed for minimum delay. The digit-serial Gaussian normal basis multiplier is implemented over two binary finite fields GF(2163) and GF(2233) in 0.18&#956;m CMOS technology for three different digit sizes. The results show that the proposed structures, compared to previous structures, have been improved in terms of delay and area parameters.
]]></description>
<guid>http://eprint.iacr.org/2016/966</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/965</link>
<title><![CDATA[A Cryptographic Proof of Regularity Lemmas]]>, by Maciej Skorski</title>
<description><![CDATA[In this work we present a unified proof for the Strong and Weak Regularity Lemma, based on the cryptographic technique called \emph{low-complexity approximations}. In short, both problems
reduce to a task of finding constructively an approximation for a certain target function under a class of distinguishers (test functions), where distinguishers are combinations of simple rectangle-indicators. 
In our case these approximations can be computed in a naive way, which results in a very simple proof achieving optimal constants. At an abstract level, our proof can be seen a refinement and simplification of the analytic proof given by Lovasz and Szegedy.

Interestingly, with our proof we obtain quantitative improvements on the partition size by a factor equal to the graph density (in the tower heigh for strong regularity and in the exponent for weak regularity). In particular, we achieve best possible bounds for constant densities.
]]></description>
<guid>http://eprint.iacr.org/2016/965</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/964</link>
<title><![CDATA[Practical low data-complexity subspace-trail cryptanalysis of round-reduced PRINCE]]>, by Lorenzo Grassi and Christian Rechberger</title>
<description><![CDATA[Subspace trail cryptanalysis is a very recent new cryptanalysis
technique, and includes differential, truncated differential, 
impossible differential, and integral attacks as special cases. 

In this paper, we consider PRINCE, a widely analyzed block cipher
proposed in 2012. After the identification of a 2.5 rounds subspace trail of PRINCE, we present several (truncated differential) attacks up to 6 rounds of PRINCE. This includes a very practical attack with the lowest data complexity of only 8 plaintexts for 4 rounds, which co-won the final round of the PRINCE challenge in the 4-round chosen-plaintext category. 
The attacks have been verified using a C implementation.
  
Of independent interest, we consider a variant of PRINCE in which ShiftRows and MixLayer operations are exchanged in position. In particular, our result shows that the position of ShiftRows and MixLayer operations influences the security of PRINCE.
The same analysis applies to follow-up designs inspired by PRINCE.
]]></description>
<guid>http://eprint.iacr.org/2016/964</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/963</link>
<title><![CDATA[Efficient compression of SIDH public keys]]>, by Craig Costello and David Jao and Patrick Longa and Michael Naehrig and Joost Renes and David Urbanik</title>
<description><![CDATA[Supersingular isogeny Diffie-Hellman (SIDH) is an attractive candidate for post-quantum key exchange, in large part due to its relatively small public key sizes. A recent paper by Azarderakhsh, Jao, Kalach, Koziel and Leonardi showed that the public keys defined in Jao and De Feo's original SIDH scheme can be further compressed by around a factor of two, but reported that the performance penalty in utilizing this compression blew the overall SIDH runtime out by more than an order of magnitude. Given that the runtime of SIDH key exchange is currently its main drawback in relation to its lattice- and code-based post-quantum alternatives, an order of magnitude performance penalty for a factor of two improvement in bandwidth presents a trade-off that is unlikely to favor public-key compression in many scenarios.

In this paper, we propose a range of new algorithms and techniques that accelerate SIDH public key-compression by more than an order of magnitude, making it roughly as fast as a round of standalone SIDH key exchange, while further reducing the size of the compressed public keys by approximately 13%. These improvements enable the practical use of compression, achieving public keys of only 330 bytes for the concrete parameters used to target 128 bits of quantum security and further strengthens SIDH as a promising post-quantum primitive.
]]></description>
<guid>http://eprint.iacr.org/2016/963</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/962</link>
<title><![CDATA[On Removing Graded Encodings from Functional Encryption]]>, by Nir Bitansky and Huijia Lin and Omer Paneth</title>
<description><![CDATA[Functional encryption (FE) has emerged as an outstanding concept. By now, we know that beyond the immediate application to computation over encrypted data, variants with {\em succinct ciphertexts} are so powerful that they yield the full might of indistinguishability obfuscation (IO). Understanding how, and under which assumptions, such succinct schemes be constructed has become a grand challenge of current research in cryptography. Whereas the first schemes were based themselves on IO, recent progress has produced constructions based on {\em constant-degree graded encodings}. Still, our comprehension of such graded encodings remains limited, as known instantiations have exhibited different vulnerabilities.

Our main result is that, assuming LWE, {\em black-box constructions} of {\em sufficiently succinct} FE schemes from constant-degree graded encodings can be transformed to rely on a much better-understood object --- {\em bilinear groups}. In particular, under an {\em \"{u}ber assumption} on bilinear groups, the construction implies IO in the plain model. The result demonstrates that the exact level of ciphertext succinctness has a major impact on FE schemes based on constant-degree encodings. In particular, we draw a fine line between known FE constructions from constant-degree graded encodings, which just fall short of the required succinctness, and the holy grail of basing IO on better-understood assumptions.

In the heart of our result, are new techniques for removing ideal graded encoding oracles from FE constructions. Complementing the result, for weaker ideal models, namely the generic group model and the random oracle model, we show a transformation from {\em collusion-resistant} FE in either of the two models directly to FE (and IO) in the plain model, without assuming bilinear groups.
]]></description>
<guid>http://eprint.iacr.org/2016/962</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/961</link>
<title><![CDATA[A kilobit hidden SNFS discrete logarithm computation]]>, by Joshua Fried and Pierrick Gaudry and Nadia Heninger and Emmanuel Thom\'e</title>
<description><![CDATA[We perform a special number field sieve discrete logarithm
  computation in a 1024-bit prime field.  To our knowledge, this is
  the first kilobit-sized discrete logarithm computation ever reported
  for prime fields.  This computation took a little over two months of
  calendar time on an academic cluster using the open-source CADO-NFS
  software.

  Our chosen prime $p$ looks random, and $p-1$ has a 160-bit prime
  factor, in line with recommended parameters for the Digital
  Signature Algorithm.  However, our $p$ has been trapdoored in such a
  way that the special number field sieve can be used to compute discrete
    logarithms in $\mathbb{F}_p^*$, yet detecting that $p$ has this trapdoor
  seems out of reach.  Twenty-five years ago, there was considerable
  controversy around the possibility of backdoored parameters for DSA.
 Our computations show that trapdoored
  primes are entirely feasible with current computing technology.
  We also describe special number field sieve discrete log 
  computations carried out for multiple weak primes found in 
  use in the wild.
]]></description>
<guid>http://eprint.iacr.org/2016/961</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/960</link>
<title><![CDATA[Quantum-Secure Symmetric-Key Cryptography Based on Hidden Shifts]]>, by Gorjan Alagic and Alexander Russell</title>
<description><![CDATA[Recent results of Kaplan et al., building on previous work by Kuwakado and Morii, have shown that a wide variety of classically-secure symmetric-key cryptosystems are completely broken when exposed to quantum CPA attacks. In such an attack, the quantum adversary has the ability to query the cryptographic functionality in superposition. The vulnerable cryptosystems include the Even-Mansour block cipher, the three-round Feistel network, the Encrypted-CBC-MAC, and many others. In this work, we study simple algebraic adaptations of such schemes that replace $(\mathbb Z/2)^n$ addition with operations over alternate finite groups--such as $\mathbb Z/{2^n}$--and provide evidence that these adaptations are secure against quantum CPA attacks. These adaptations furthermore retain the classical security properties (and basic structural features) enjoyed by the original schemes. 

We establish security by treating the (quantum) hardness of the well-studied Hidden Shift problem as a basic cryptographic assumption. We observe that this problem has a number of attractive features in this cryptographic context, including random self-reducibility, hardness amplification, and--in many cases of interest--a reduction from the "search version" to the "decisional version." We then establish, under this assumption, the security of several such hidden-shift adaptations of symmetric-key constructions against quantum CPA attack. We show that a Hidden Shift version of the Even-Mansour block cipher yields a quantum-secure pseudorandom function, and that a Hidden Shift version of the Encrypted CBC-MAC yields a collision-resistant hash function. Finally, we observe that such adaptations frustrate the direct Simon's algorithm-based attacks in more general circumstances, e.g., Feistel networks and slide attacks.
]]></description>
<guid>http://eprint.iacr.org/2016/960</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/959</link>
<title><![CDATA[Impossibility of Simulation Secure Functional Encryption Even with Random Oracles]]>, by Shashank Agrawal and Venkata Koppula and Brent Waters</title>
<description><![CDATA[In this work we study the feasibility of achieving simulation security in functional encryption (FE) in the random oracle model.  Our main result is negative in that we give a functionality for which it is impossible to achieve simulation security even with the aid of random oracles.

We begin by giving a formal definition of simulation security that explicitly incorporates the random oracles. Next, we show a particular functionality for which it is impossible to achieve simulation security. Here messages are interpreted as seeds to a (weak) pseudorandom function family $F$ and private keys are ascribed to points in the domain of the function. On a message $s$ and private key $x$ one can learn $F(s,x)$. We show that there exists an attacker that makes a polynomial number of private key queries followed by a single ciphertext query for which there exists no simulator. 

Our functionality and attacker access pattern closely matches the standard model impossibility result of Agrawal, Gorbunov, Vaikuntanathan and Wee (CRYPTO 2013). The crux of their argument is that no simulator can succinctly program in the outputs of an unbounded number of evaluations of a pseudorandom function family into a fixed size ciphertext. However, their argument does not apply in the random oracle setting since the oracle acts as an additional conduit of information which the simulator can program. We overcome this barrier by proposing an attacker who decrypts the challenge ciphertext with the secret keys issued earlier without using the random oracle, even though the decryption algorithm may require it. This involves collecting most of the useful random oracle queries in advance, without giving the simulator too many opportunities to program. We note that our negative result contradicts a theorem of De Caro et al. (CRYPTO 2013) which claims that random oracles can transform any indistinguishability secure functional encryption system into one that is simulation secure.

On the flip side, we demonstrate the utility of the random oracle in simulation security. Given only public key encryption and low-depth PRGs we show how to build an FE system that is simulation secure for any poly-time attacker that makes an unbounded number of message queries, but an a-priori bounded number of key queries. This bests what is possible in the standard model where it is only feasible to achieve security for an attacker that is  bounded both in the number of key and message queries it makes. We achieve this by creating a system that leverages the random oracle to get one-key security and then adapt previously known techniques to boost the system to resist up to $q$ queries.  

Finally, we ask whether it is possible to achieve simulation security for an unbounded number of messages and keys, but where all key queries are made after the message queries. We show this too is impossible to achieve using a different twist on our first impossibility result.
]]></description>
<guid>http://eprint.iacr.org/2016/959</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/958</link>
<title><![CDATA[SafeDeflate: compression without leaking secrets]]>, by Micha&#322; Zieli&#324;ski</title>
<description><![CDATA[CRIME and BREACH attacks on TLS/SSL leverage the fact that compression ratio is not hidden by encryption to recover content of secrets. We introduce SafeDeflate---a modification of a standard Deflate algorithm which compression ratio does not leak information about secret tokens. The modification is compatible with existing Deflate and gzip decompressors. We introduce a model in which attacker can obtain ciphertexts of arbitrary compressed plaintext containing secret values. Then we prove that SafeDeflate is secure in this model.
]]></description>
<guid>http://eprint.iacr.org/2016/958</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/957</link>
<title><![CDATA[Computing generator in cyclotomic integer rings]]>, by Thomas Espitau and Pierre-Alain Fouque and Alexandre Gélin and Paul Kirchner</title>
<description><![CDATA[The Principal Ideal Problem (resp. Short Principal Ideal Problem), shorten as PIP (resp. SPIP), consists in finding a generator (resp. short generator) of a principal ideal in the ring of integers of a number field. Several lattice-based cryptosystems rely on the presumed hardness of these two problems. Yet, in practice, most of them do not use an arbitrary number field but a power-of-two cyclotomic field. The Smart and Vercauteren fully homomorphic encryption scheme and the multilinear map of Garg, Gentry and Halevi epitomize this common restriction. Recently, Cramer, Ducas, Peikert and Regev show that solving the SPIP in such cyclotomic rings boils down to solving the PIP. We complete their result by an algorithm that solves PIP in cyclotomic fields in subexponential time $L_{|\Delta_K|} (1/2) = 2^{N^{1/2+o(1)}}$, where $\Delta_K$ denotes the discriminant of the number field and N its degree. This asymptotic complexity could be compared with the one obtained by Biasse and Fieker method, that aims at finding a generator as we do, but runs in 
L_{|\Delta_K|} (2/3). Besides this theoretical improvement, our algorithm permits to recover in practice the secret key of the Smart and Vercauteren scheme, for the
smallest proposed parameters.
]]></description>
<guid>http://eprint.iacr.org/2016/957</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/956</link>
<title><![CDATA[Two Simple Composition Theorems with H-coefficients]]>, by Jacques Patarin</title>
<description><![CDATA[We will present here two simple theorems that show that when we compose permutation generators with
 independent keys, then the ``quality'' of CCA security increases. These theorems are written in terms of
 H-coefficients.
]]></description>
<guid>http://eprint.iacr.org/2016/956</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/955</link>
<title><![CDATA[Constant-deposit multiparty lotteries on Bitcoin]]>, by Massimo Bartoletti and Roberto Zunino</title>
<description><![CDATA[An active research trend is to exploit the consensus mechanism of cryptocurrencies to secure the execution of distributed applications. In particular, some recent works have proposed fair lotteries which work on Bitcoin. These protocols, however, require a deposit from each player which grows quadratically with the number of players. We propose a fair lottery on Bitcoin which only requires a constant deposit.
]]></description>
<guid>http://eprint.iacr.org/2016/955</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/954</link>
<title><![CDATA[Improving the lower bound on the maximum nonlinearity of 1-resilient Boolean functions and designing functions satisfying all cryptographic criteria]]>, by WeiGuo Zhang and Enes Pasalic</title>
<description><![CDATA[In this paper, we improve the lower bound on the maximum nonlinearity of 1-resilient Boolean functions, for $n$ even, by proposing a method of constructing this class of functions attaining the best nonlinearity currently known. Thus for the first time, at least for small values of $n$, the upper bound on nonlinearity can be reached in a deterministic manner in difference to some heuristic search methods proposed previously. The nonlinearity of these functions is extremely close to the maximum nonlinearity attained by bent functions and it might be the case that this is the highest possible nonlinearity of 1-resilient functions. Apart from this theoretical contribution, it turns out that the cryptographic properties of these functions are overall good apart from their moderate resistance to fast algebraic attacks (FAA). This weakness is repaired by a suitable modification of the original functions giving a class of balanced functions with almost optimal resistance to FAA whose nonlinearity is better than the nonlinearity of other methods.
]]></description>
<guid>http://eprint.iacr.org/2016/954</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/953</link>
<title><![CDATA[Collusion Resistant Broadcast Encryption with Tight Reductions and Beyond]]>, by Linfeng Zhou</title>
<description><![CDATA[The issue of tight security in identity-based encryption scheme (IBE) has been widely investigated. Recently, a tightly secure IBE scheme of bilinear groups  in the weak multi-challenge setting has been achieved by Chen (eprint 2016/891), and their scheme even achieves constant public parameters and is adaptively secure. However, we note that the issue of tight security in broadcast encryption scheme (BE) of bilinear groups has received less attention so far. Actually current broadcast encryption systems
of bilinear groups are either not tightly secure or based on non-static assumptions. In this work we mainly focus on the issue of tight security in the standard broadcast encryption scheme.

We present the first \textit{tightly secure} broadcast encryption scheme (BE) from the static assumptions (i.e., decisional subgroup assumptions) in the selective security model. Our construction and proof rely on the recently novel techniques from Chen (eprint 2016/891). The proof of our construction will lead to only \(O(\log n)\) or \(O(\log \lambda)\) security loss, where \(n\) is the number of users in the system and \(\lambda\) is the security parameter, rather than \(O(n)\) security loss as the construction given by Wee (TCC-A 16) and many other previous constructions.

Following this result, we present the first \textit{tightly secure} non-zero inner product encryption scheme (NIPE) from decisional subgroup assumptions in the selective security model. This NIPE scheme has the same parameter sizes as our BE scheme and there is only \(O(\log n)\) or \(O(\log \lambda)\) security loss as well, where \(n\) is the dimension of inner product space and \(\lambda\) is the security parameter. This result improves the most optimal NIPE scheme so far from static assumptions recently proposed by Chen et al.(SCN 16), which also suffers \(O(n)\) security loss during the reduction.

Finally, we further present the first functional commitment scheme (FC) for linear functions with \textit{tight reductions}, also from decisional subgroup assumptions. Recently Libert et al.(ICALP 16) introduced the notion of functional commitment scheme for linear functions. However, their scheme also suffers \(O(n)\) security loss during the reduction. In contrast, there is only \(O(\log n)\) or \(O(\log \lambda)\) security loss in our FC scheme, which significantly improves the result of Libert et al.
]]></description>
<guid>http://eprint.iacr.org/2016/953</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/952</link>
<title><![CDATA[ISAP -- Authenticated Encryption Inherently Secure Against Passive Side-Channel Attacks]]>, by Christoph Dobraunig and Maria Eichlseder and Stefan Mangard and Florian Mendel and Thomas Unterluggauer</title>
<description><![CDATA[Side-channel attacks and in particular differential power analysis (DPA) attacks pose a serious threat to cryptographic implementations. One approach to counteract such attacks are cryptographic schemes based on fresh re-keying. In settings of pre-shared secret keys, such schemes render DPA infeasible by deriving session keys and by ensuring that the attacker cannot collect side-channel leakage on the session key during cryptographic operations with different inputs. While these schemes can be applied to secure standard communication settings, current re-keying approaches are unable to provide protection in settings where the same input needs to be processed multiple times.

In this work, we therefore adapt the re-keying approach to present the first symmetric authenticated encryption scheme that is inherently secure against DPA attacks and that does not have such a usage restriction. This means that our scheme fully complies with the requirements given in the CAESAR call and hence, can be used like other standard authenticated encryption schemes without loss of side-channel protection. Its resistance against side-channel analysis is highly relevant for several applications in practice, like bulk storage settings in general and the protection of FPGA bitfiles and firmware images in particular.
]]></description>
<guid>http://eprint.iacr.org/2016/952</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/951</link>
<title><![CDATA[Revisiting Covert Multiparty Computation]]>, by Geoffroy Couteau</title>
<description><![CDATA[Is it feasible for parties to securely evaluate a function on their joint inputs, while hiding not only their private input, but even the very fact that they are taking part to the protocol? This intriguing question was given a positive answer in the two-party case at STOC'05, and in the general case at FOCS'07, under the name of covert multiparty computation (CMPC). A CMPC protocol allows n players with inputs (x1 ···xn) to compute a function f with the following guarantees:
- If all the parties are taking part to the protocol, and if the result of the computation is favorable to all the parties, then they get to learn f(x1,··· ,xn) (and nothing more)
- Else, when the result is not favorable to all the parties, or if some player does not participate to the computation, no one gets to learn anything (and in particular, no player can learn whether any of the other parties was indeed participating to the protocol)
While previous works proved the existence of CMPC under standard assumptions, their candidate CMPC protocols were exclusively of theoretical interest. In this work, we revisit the design of CMPC protocols and show that, perhaps surprisingly, this very strong security notion can be achieved essentially for free. More specifically, we show how to build a CMPC protocol out of a standard, state-of-the-art MPC protocol, where both the communication and the computation are the same than the original protocol, up to an additive factor independent of the size of the circuit. Along the way, we prove two variants of the UC theorem which greatly simplify the design and the security analysis of CMPC protocols.
]]></description>
<guid>http://eprint.iacr.org/2016/951</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/950</link>
<title><![CDATA[Orthogonalized Lattice Enumeration for Solving SVP]]>, by Zhongxiang Zheng and Xiaoyun Wang and  Yang Yu</title>
<description><![CDATA[In 2014, the orthogonalized integer representation is presented indepently by Dan Ding etc and Fukase etc to solve SVP respectively by genetic algorithm and  sampling technique, and both work have achieved very good results. In this paper, we first study the probability distribution  of the shortest vector with orthogonalized integer representations, and give a new enumeration method based on the representations, called orthognalized enumeration. Besides, we present a new BKZ method, called MBKZ, which alternately uses orthognalized enumeration and traditional enumeration. Our method has the obvious advantage in time complexity compared to the previous ones which achieved exponential speedups both in theory and in practice. Furthermore, a new technique of reducing enumeration space is described, we can't find a quantitative analysis about success probability but it is effective in practice.
]]></description>
<guid>http://eprint.iacr.org/2016/950</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/949</link>
<title><![CDATA[Functional Encryption for Computational Hiding in Prime Order Groups via Pair Encodings]]>, by Jongkil Kim and Willy Susilo and Fuchun Guo and Man Ho Au</title>
<description><![CDATA[Lewko and Waters introduced the computational hiding technique in Crypto'12. In their technique, two computational assumptions that achieve selective and co-selective security proofs lead to adaptive security of an encryption scheme. Later, pair encoding framework was introduced by Attrapadung in Eurocrypt'14. The pair encoding framework generalises the computational hiding technique for functional encryption (FE). It has been used to achieve a number of new FE schemes such as FE for regular languages and unbounded attribute based encryption allowing multi-use of attributes. 
Nevertheless, the generalised construction of Attrapadung's pair encoding for those schemes is adaptively secure only in composite order groups, which leads to efficiency loss. 
It remains a challenging task to explore constructions in prime order
groups for gaining efficiency improvement, which leaves the research gap in the existing literature. 
In this work, we aim to address this drawback by proposing a new generalised construction for pair encodings in prime order groups. Our construction will lead to a number of new FE schemes in prime order groups, which have been previously introduced only in composite order groups by Attrapadung.
]]></description>
<guid>http://eprint.iacr.org/2016/949</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/948</link>
<title><![CDATA[Secure Computation in Online Social Networks]]>, by Foteini Baldimtsi and Dimitrios Papadopoulos and Stavros Papadopoulos and Alessandra Scafuro and Nikos Triandopoulos</title>
<description><![CDATA[Apart from their numerous other benefits, online social networks (OSNs) allow users to jointly compute on each other's data (e.g., profiles, geo-locations, medical records, etc.). Privacy issues naturally arise in this setting due to the sensitive nature of the exchanged information. Ideally, nothing about a user's data should be revealed to the OSN provider or "non-friend" users, and even her "friends" should only learn the output of a joint computation. In this work we propose the first security framework to capture these strong privacy guarantees for general-purpose computation. We also achieve two additional attractive properties: users do not need to be online while their friends compute on their data, and any user value uploaded at the server can be repeatedly used in multiple computations. We formalize our framework in the setting of secure multi-party computation (MPC) and provide two instantiations: the first is a non-trivial adaptation of garbled circuits that converts inputs under different keys to ones under the same key, and the second is based on two-party mixed protocols and involves a novel two-party re-encryption module. We experimentally validate the efficiency of our instantiations using state-of-the-art tools for two concrete use-cases.
]]></description>
<guid>http://eprint.iacr.org/2016/948</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/947</link>
<title><![CDATA[Isogeny graphs of ordinary abelian varieties]]>, by Ernest Hunter Brooks and Dimitar Jetchev and Benjamin Wesolowski</title>
<description><![CDATA[Fix a prime number $\ell$. Graphs of isogenies of degree a power of $\ell$ are well-understood for elliptic curves, but not for higher-dimensional abelian varieties. We study the case of absolutely simple ordinary abelian varieties over a finite field.
We analyse graphs of so-called $\mathfrak l$-isogenies, resolving that they are (almost) volcanoes in any dimension. Specializing to the case of principally polarizable abelian surfaces, we then exploit this structure to describe graphs of a particular class of isogenies known as $(\ell, \ell)$-isogenies: those whose kernels are maximal isotropic subgroups of the $\ell$-torsion for the Weil pairing.
We use these two results to write an algorithm giving a path of computable isogenies from an arbitrary absolutely simple ordinary abelian surface towards one with maximal endomorphism ring, which has immediate consequences for the CM-method in genus 2, for computing explicit isogenies, and for the random self-reducibility of the discrete logarithm problem in genus 2 cryptography.
]]></description>
<guid>http://eprint.iacr.org/2016/947</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/946</link>
<title><![CDATA[Bitsliced Masking and ARM: Friends or Foes?]]>, by Wouter de Groot and Kostas Papagiannopoulos and Antonio de La Piedra and Erik Schneider and Lejla Batina</title>
<description><![CDATA[Software-based cryptographic implementations can be vulnerable to side-channel analysis. Masking countermeasures rank among the most prevalent techniques against it, ensuring formally the protection vs. value-based leakages. However, its applicability is halted by two factors. First, a masking countermeasure involves a computational overhead that can render implementations inefficient. Second, physical effects such as glitches and distance-based leakages can cause the reduction of the security order in practice, rendering the masking protection less effective.
This paper, attempts to address both factors. In order to reduce the computational cost, we implement a high-throughput, bitsliced, 2nd-order masked implementation of the PRESENT cipher, using assembly in ARM Cortex-M4. The implementation outperforms the current state of the art and is capable of encrypting a 64-bit block of plaintext in 6,532 cycles (excluding RNG), using 1,644 bytes of data RAM and 1,552 bytes of code memory. Second, we analyze experimentally the effectiveness of masking in ARM devices, i.e. we examine the effects of distance-based leakages on the security order of our implementation. We confirm the theoretical model behind distance leakages for the first time in ARM-based architectures.
]]></description>
<guid>http://eprint.iacr.org/2016/946</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/945</link>
<title><![CDATA[High throughput in slices: the case of PRESENT, PRINCE and KATAN64 ciphers]]>, by Kostas Papapagiannopoulos</title>
<description><![CDATA[This paper presents high-throughput assembly implementations of PRESENT,
PRINCE and KATAN64 ciphers for the ATtiny family of AVR microcontrollers. We report throughput records, achieving the speed of 2967 clock cycles per block encryption for PRESENT, 1803 cycles for PRINCE and 23671 cycles for KATAN64. In addition, we offer insight into the `slicing' techniques used for high throughput and their application to lightweight cryptographic implementations. We also demonstrate the speed-memory tradeoff by constructing high-throughput implementations with large memory requirements.
]]></description>
<guid>http://eprint.iacr.org/2016/945</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/944</link>
<title><![CDATA[High-Throughput Secure Three-Party Computation for Malicious Adversaries and an Honest Majority]]>, by Jun Furukawa and Yehuda Lindell and Ariel Nof and Or Weinstein</title>
<description><![CDATA[In this paper, we describe a new protocol for secure three-party computation of any functionality, with an honest majority and a \textit{malicious} adversary. Our protocol has both an information-theoretic and computational variant, and is distinguished by extremely low communication complexity and very simple computation. We start from the recent semi-honest protocol of Araki et al. (ACM CCS 2016) in which the parties communicate only a single bit per AND gate, and modify it to be secure in the presence of malicious adversaries. Our protocol follows the paradigm of first constructing Beaver multiplication triples and then using them to verify that circuit gates are correctly computed. As in previous work (e.g., the so-called TinyOT and SPDZ protocols), we rely on the cut-and-choose paradigm to verify that triples are correctly constructed. We are able to utilize the fact that at most one of three parties is corrupted in order to construct an extremely simple and efficient method of constructing such triples. We also present an improved combinatorial analysis for this cut-and-choose which can be used to achieve improvements in other protocols using this approach.
]]></description>
<guid>http://eprint.iacr.org/2016/944</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/943</link>
<title><![CDATA[Stadium: A Distributed Metadata-Private Messaging System]]>, by Nirvan Tyagi and Yossi Gilad and Matei Zaharia and Nickolai Zeldovich</title>
<description><![CDATA[Private communication over the Internet continues to be a challenging problem. Even if messages are encrypted, it is hard to deliver them without revealing metadata about which pairs of users are communicating. Scalable systems, such as Tor, are susceptible to traffic analysis. In contrast, the largest-scale systems with metadata privacy require passing all messages through each server, capping their throughput and scalability.

This paper presents Stadium, the first system to provide metadata and data privacy while being able to scale its work efficiently across many servers. Much like Vuvuzela, the current largest-scale system, Stadium is based on differential privacy. However, providing privacy in Stadium is more challenging because distributing users' traffic across servers creates opportunities for adversaries to observe it in fine granularity. To solve this challenge, Stadium uses a collaborative noise generation approach combined with a novel verifiable parallel mixnet design where servers collaboratively check that others follow the protocol. We show that Stadium can scale to use hundreds of servers, support over an order of magnitude more users than Vuvuzela, and cut the costs of operating each server.
]]></description>
<guid>http://eprint.iacr.org/2016/943</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/942</link>
<title><![CDATA[Optimizing Secure Computation Programs with Private Conditionals]]>, by Peeter Laud and Alisa Pankova</title>
<description><![CDATA[Secure multiparty computation platforms are often provided with a programming language that allows to write privacy-preserving applications without thinking of the underlying cryptography.
The control flow of these programs is expensive to hide, hence they typically disallow branching on private values. The application programmers have to specify their programs in terms of allowed constructions, either using ad-hoc methods to avoid such branchings, or the general methodology of executing all branches and obliviously selecting the effects of one at the end. There may be compiler support for the latter.

The execution of all branches introduces significant computational overhead. If the branches perform similar private operations, then it may make sense to compute repeating patterns only once, even though the necessary bookkeeping also has overheads. In this paper, we propose a program optimization doing exactly that, allowing the overhead of private conditionals to be reduced. The optimization is quite general, and can be applied to various privacy-preserving platforms.
]]></description>
<guid>http://eprint.iacr.org/2016/942</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/941</link>
<title><![CDATA[A New Class of Differentially 4-uniform Permutations from the Inverse Function]]>, by Jian Bai and Dingkang Wang</title>
<description><![CDATA[Differentially 4-uniform permutations on $\mathbb{F}_{2^{2k}}$ with high nonlinearity and algebraic degree are often used in block ciphers and some stream ciphers as Substitution boxes. Recently,Chen et al.(An equivalent
condition on the switching construction of differentially 4-uniform permutations on from the inverse function, International Journal of Computer Mathematics, DOI:10.1080/00207160.2016.1167884) presented a n equivalent condition on the switching construction. More precisely,they presented a sufficient and necessary condition on differentially 4-uniform permutations on $\mathbb{F}_{2^{2k}}$ of the form $G(x)=x^{-1}+f(x^{-1})$, where $f$ is a Boolean function. However, the number of the satisfied functions is so enormous that it is difficult to find all the functions. In this paper,a new class of such functions are constructed. These functions may provide more options for the design of Substitute boxes.
]]></description>
<guid>http://eprint.iacr.org/2016/941</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/940</link>
<title><![CDATA[Fast Actively Secure OT Extension for Short Secrets]]>, by Arpita Patra and Pratik Sarkar and  Ajith Suresh</title>
<description><![CDATA[Oblivious Transfer (OT) is one of the most fundamental cryptographic primitives  with wide-spread application in general secure multi-party computation (MPC) as well as in a number of tailored and special-purpose problems of interest such as private set intersection (PSI), private information retrieval (PIR), contract signing to name a few.  Often the instantiations of OT require prohibitive  communication and computation complexity. OT extension protocols are introduced to compute a very large number of OTs  referred as extended OTs at the cost of  a small number of OTs referred as seed OTs. 

We present a fast OT extension protocol for small secrets in active setting. Our protocol when used to produce $1$-out-of-$n$ OTs  outperforms all the known actively secure OT extensions. Our protocol is built on the semi-honest secure extension protocol of Kolesnikov and Kumaresan of CRYPTO'13 (referred as KK13 protocol henceforth) which is the best known OT extension for  short secrets. At the heart of our protocol lies an efficient consistency checking mechanism that relies on the linearity of Walsh-Hadamard (WH) codes. Asymptotically, our protocol adds a communication overhead of $O(\mu \log{\kappa})$ bits over KK13 protocol irrespective of the number of  extended OTs, where $\kappa$ and $\mu$ refer to computational and statistical security parameter respectively.  Concretely, our protocol when used to generate a large enough number of OTs adds only $0.011-0.028\%$  communication overhead and  $4-6\%$ runtime overhead both in LAN and WAN over KK13 extension. The runtime overheads drop below $2\%$ when  in addition the number of inputs of the sender in the extended OTs is large enough.

As an application of our proposed extension protocol, we show that it can be used to obtain the most efficient PSI protocol secure against a malicious receiver and a semi-honest sender.
]]></description>
<guid>http://eprint.iacr.org/2016/940</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/939</link>
<title><![CDATA[Key Reconciliation Protocols for Error Correction of Silicon PUF Responses]]>, by Brice Colombier and Lilian Bossuet, and David Hély and Viktor Fischer</title>
<description><![CDATA[Physical Unclonable Functions (PUFs) are promising primitives for lightweight integrated circuit authentication. Indeed, by extracting an identifier from random process variations, they allow each instance of a design to be uniquely identified. However, the extracted identifiers are not stable enough to be used as is, but need to be corrected first. This is currently achieved using error correcting codes, which generate helper data through a one-time process. As an alternative, we propose key reconciliation protocols. This interactive method, originating from quantum key distribution, allows two entities to correct errors in their respective correlated keys by discussing over a public channel. We believe this can also be used by a device and a remote server to agree on two different responses to the same challenge from the same PUF obtained at different times. This approach has the advantage of requiring few logic resources on the device side, at least three times fewer than existing error correcting codes. The leakage caused by the key reconciliation process is limited and easily computable. Results of implementation on various FPGA targets are presented.
]]></description>
<guid>http://eprint.iacr.org/2016/939</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/938</link>
<title><![CDATA[Kummer for Genus One over Prime Order Fields]]>, by Sabyasachi Karati and Palash Sarkar</title>
<description><![CDATA[This work considers the problem of fast and secure scalar multiplication using curves of genus one defined over a field of prime order. Previous work by Gaudry and Lubicz had suggested the use of the associated Kummer line to speed up scalar multiplication. In this work, we explore this idea in details. The first task is to obtain an elliptic curve in the Legendre form which satisfies necessary security conditions such that the associated Kummer line has small parameters and a base point with small coordinates. It turns out that the Kummer ladder supports parallelism and can be implemented very efficiently in constant time using the single-instruction multiple-data (SIMD) operations available in modern processors. We report implementation using Intel intrinsics and the code is publicly available. The timing results show that the performance of the Kummer line based approach compare favourably with previously proposed genus one curves over prime order fields.
]]></description>
<guid>http://eprint.iacr.org/2016/938</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/937</link>
<title><![CDATA[A Comparative S-Index in Factoring RSA Modulus via Lucas Sequences]]>, by Nur Azman Abu and Shekh Faisal Abdul-Latip and Muhammad Rezal Kamel Ariffin</title>
<description><![CDATA[General Lucas sequences are practically useful in cryptography. In the past quarter century, factoring large RSA modulo into its primes is one of the most important and most challenging problems in computational number theory. A factoring technique on RSA modulo is mainly hindered by the strong prime properties. The success of factoring few large RSA modulo within the last few decades has been due to computing prowess overcoming one strong prime of RSA modulo. In this paper, some useful properties of Lucas sequences shall be explored in factoring RSA modulo. This paper introduces the S-index formation in solving quadratic equation modulo N. The S-index pattern is very useful in designing an algorithm to factor RSA modulo. At any instance in the factoring algorithm, the accumulative result stands independently. In effect, there is no clear direction to maneuver whether to go left or right. The S-index will add another comparative tool to better maneuver in a factoring process. On one hand, it shall remain a theoretical challenge to overcome the strong prime properties. On the other hand, it shall remain a computational challenge to achieve a running time within polynomial time to factor RSA modulo. This paper will propose an avenue to do both using general Lucas sequences.
]]></description>
<guid>http://eprint.iacr.org/2016/937</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/936</link>
<title><![CDATA[Linear Complexity of Designs based on  Coordinate Sequences of LRS  and  on Digital Sequences of  Matrix/Skew LRS  Coordinate Sequences  over Galois Ring]]>, by Vadim N. Tsypyschev</title>
<description><![CDATA[This article continues investigation of ways to obtain pseudo-random sequences over Galois field via generating LRS over Galois ring and complication it.

Previous work is available at http://eprint.iacr.org/2016/212

In this work we provide low rank estimations for sequences generated by two different designs based on coordinate sequences of linear recurrent sequences (LRS)   of maximal period (MP)  over Galois ring 
 $R=GR(p^n,r)$, $p\ge 5$, $r\ge2$, with numbers   $s$ such that   $s=kr+2$,  $k\in \mathbb{N}_0$, and based on digital sequences of coordinate sequences of matrix/skew MP LRS over such Galois rings.
]]></description>
<guid>http://eprint.iacr.org/2016/936</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/935</link>
<title><![CDATA[Concealing Secrets in Embedded Processors Designs]]>, by Hannes Gross and Manuel Jelinek and Stefan Mangard and Thomas Unterluggauer and Mario Werner</title>
<description><![CDATA[Side-channel analysis (SCA) attacks pose a serious threat to embedded systems. So far, the research on masking as a countermeasure against SCA focuses merely on cryptographic algorithms, and has either been implemented for particular hardware or software implementations. However, the drawbacks of protecting specific implementations are the lack of flexibility in terms of used algorithms, the impossibility to update protected hardware implementations, and long development cycles for protecting new algorithms. Furthermore, cryptographic algorithms are usually just one part of an embedded system that operates on informational assets. Protecting only this part of a system is thus not sufficient for most security critical embedded applications.
 In this work, we introduce a flexible, SCA-protected processor design based on the open-source V-scale RISC-V processor. The introduced processor design can be synthesized to defeat SCA attacks of arbitrary attack order. Once synthesized, the processor protects the computation on security-sensitive data against side-channel leakage. The benefits of our approach are (1) flexibility and updatability, (2) faster development of SCA-protected systems, (3) transparency for software developers, (4) arbitrary SCA protection level, (5) protection not only for cryptographic algorithms, but against leakage in general caused by processing sensitive data.
]]></description>
<guid>http://eprint.iacr.org/2016/935</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/934</link>
<title><![CDATA[Cryptography with Updates]]>, by Prabhanjan Ananth and Aloni Cohen and Abhishek Jain</title>
<description><![CDATA[Starting with the work of Bellare, Goldreich and Goldwasser [CRYPTO'94], a rich line of work has studied the design of updatable cryptographic primitives. For example, in an updatable signature scheme, it is possible to efficiently transform a signature over a message into a signature over a related message without recomputing a fresh signature. 

In this work, we continue this line of research, and perform a systematic study of updatable cryptography. We take a unified approach towards adding updatability features to recently studied cryptographic objects such as attribute-based encryption, functional encryption, witness encryption, indistinguishability obfuscation, and many others that support non-interactive computation over inputs. We, in fact, go further and extend our approach to classical protocols such as zero-knowledge proofs and secure multiparty computation.

To accomplish this goal, we introduce a new notion of updatable randomized encodings that extends the standard notion of randomized encodings to incorporate updatability features. We show that updatable randomized encodings can be used to generically transform cryptographic primitives to their updatable counterparts.

We provide various definitions and constructions of updatable randomized encodings based on varying assumptions, ranging from one-way functions to compact functional encryption.
]]></description>
<guid>http://eprint.iacr.org/2016/934</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/932</link>
<title><![CDATA[Mistakes Are Proof That You Are Trying: On Verifying Software Encoding Schemes' Resistance to Fault Injection Attacks]]>, by Jakub Breier and Dirmanto Jap and Shivam Bhasin</title>
<description><![CDATA[Software encoding countermeasures are becoming increasingly popular among researchers proposing code-level prevention against data-dependent leakage allowing an attacker to mount a side-channel attack. Recent trends show that it is possible to design a solution that does not require excessive overhead and yet provides a reasonable security level. However, if the device leakage is hard to be observed, attacker can simply switch to a different class of physical attacks, such as fault injection attack. 
Instead of stacking several layers of countermeasures, it is always more convenient to choose one that provides decent protection against several attack methods. Therefore, in our paper we use our custom designed code analyzer to formally inspect a recently proposed software encoding countermeasure based on device-specific encoding function, and compare it with other solutions, either based on balanced look-up tables or balanced encoding. We also provide an experimental validation, using the laser fault injection setup. 
Our results show that the device-specific encoding scheme provides a good protection against fault injection attacks, being capable of preventing majority of faults using different fault models.
]]></description>
<guid>http://eprint.iacr.org/2016/932</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/928</link>
<title><![CDATA[The complexity of the connected graph access structure on seven participants]]>, by Massoud Hadian Dehkordi and Ali Safi</title>
<description><![CDATA[In this paper, we study an important problem in secret sharing that
determines the exact value or bound for the complexity. First, we used induced subgraph complexity of the graph G with access structure, Gama, to obtain a lower bound on the complexity of the graph G. Secondly, by applying decomposition techniques we obtain an upper bound on the complexity of the graph G. We determine the exact values of the complexity for each of the ten graph access structures on seven participants. Also, we improve the value bound of the complexity of the six graph access structures with seven participants.
]]></description>
<guid>http://eprint.iacr.org/2016/928</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/916</link>
<title><![CDATA[FruitChains: A Fair Blockchain]]>, by Rafael Pass and Elaine Shi</title>
<description><![CDATA[Nakamoto's famous blockchain protocol enables achieving consensus in a so-called permissionless setting---anyone can join (or leave) the protocol execution, and the protocol instructions do not depend on the identities of the players. His ingenious protocol prevents ``sybil attacks'' (where an adversary spawns any number of new players) by relying on computational puzzles (a.k.a. ``moderately hard functions') introduced by Dwork and Naor (Crypto'92). Recent work by Garay et al (EuroCrypt'15) and Pass et al (manuscript, 2016) demonstrate that this protocol provably achieves consistency and liveness assuming a) honest players control a majority of the computational power in the network, b) the puzzle-hardness is appropriately set as a function of the maximum network delay and the total computational power of the network, and c) the computational puzzle is modeled as a random oracle. 

Assuming honest participation, however, is a strong assumption, especially in a setting where honest players are expected to perform a lot of work (to solve the computational puzzles). In Nakamoto's Bitcoin application of the blockchain protocol, players are incentivized to solve these puzzles by receiving rewards for every ``blocks'' (of transactions) they contribute to the blockchain. An elegant work by Eyal and Sirer (FinancialCrypt'14), strengthening and formalizing an earlier attack discussed on the Bitcoin forum, demonstrates that a coalition controlling even a minority fraction of the computational power in the network can gain (close to) 2 times its ``fair share'' of the rewards (and transation fees) by deviating from the protocol instructions. In contrast, in a fair protocol, one would expect that players controlling a $\phi$ fraction of the computational resources to reap a $\phi$ fraction of the rewards. 

In this work, we present a new blockchain protocol---the FruitChain protocol---which satisfies the same consistency and liveness properties as Nakamoto's protocol (assuming an honest majority of the computing power), and additionally is $\delta$-approximately fair: with overwhelming probability, any honest set of players controlling a $\phi$ fraction of computational power is guaranteed to get at least a fraction $(1 - \delta) \phi$ of the blocks (and thus rewards) in any $Omega( \kappa/\delta )$ length segment of the chain (where $\kappa$ is the security parameter).

As a consequence, if this blockchain protocol is used as the ledger underlying a cryptocurrency system, where rewards and transaction fees are evenly distributed among the miners of blocks in a length kappa segment of the chain, no coalition controlling less than a majority of the computing power can gain more than a factor $(1 + 3\delta)$ by deviating from the protocol (i.e., honest participation is an $n/2$-coalition-safe $3\delta$-Nash equilibrium).

Finally, the fruit chain protocol enables decreasing the variance of mining rewards and as such significantly lessens (or even obliterates) the need for mining pools.
]]></description>
<guid>http://eprint.iacr.org/2016/916</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/913</link>
<title><![CDATA[Small Field Attack, and Revisiting RLWE-Based Authenticated Key Exchange from Eurocrypt'15]]>, by Boru Gong and Yunlei Zhao</title>
<description><![CDATA[Authenticated key-exchange (AKE) plays a fundamental role in modern cryptography. Up to now, the HMQV protocol family is among the most efficient provably secure AKE protocols, which has been widely standardized and in use. Given recent advances in quantum computing, it is highly desirable to develop lattice-based HMQV-analogue protocols for the upcoming post-quantum era. Towards this goal, an important step is recently made by Zhang et al. at Eurocrypt'15. Similar to HMQV, the HMQV-analogue protocols proposed there consists of two variants: a two-pass protocol $\Pi_2$, as well as a one-pass protocol $\Pi_1$ that implies, in turn, a signcryption scheme (named as ``deniable encryption"). All these protocols are claimed to be provably secure under the ring-LWE (RLWE) assumption. 


In this work, we propose a new type of attack, referred to as small field attack (SFA), against the one-pass protocol $\Pi_1$, as well as its resultant deniable encryption scheme. With SFA, a malicious user can efficiently recover the static private key of the honest victim user in $\Pi_1$ with overwhelming probability. Moreover, the SFA attack is realistic and powerful in practice, in the sense that it is almost impossible for the honest user to prevent, or even detect, the attack. Besides, some new property regarding the CRT basis of $R_q$ is also developed in this work, which is essential for our small field attack and may be of independent interest. 


The security proof of the two-pass protocol $\Pi_2$ is then revisited. We are stunk at Claim 16 in [ZZDS14], with a gap identified and discussed in the security proof. To us, we do not know how to fix the gap, which traces back to some critical differences between the security proof of HMQV and that of its RLWE-based analogue.
]]></description>
<guid>http://eprint.iacr.org/2016/913</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/907</link>
<title><![CDATA[Cut-and-Choose for Garbled RAM]]>, by Peihan Miao</title>
<description><![CDATA[Garbled RAM, introduced by Lu and Ostrovsky (Eurocrypt 2013), provides a novel method to garble RAM (Random Access Machine) programs directly. It can be seen as a RAM analogue of Yao's garbled circuits such that, the size of the garbled program and the time it takes to create and evaluate it, is proportional only to the running time of the RAM program, avoiding the inefficient process of first converting it into a circuit. Secure RAM computation for two parties is a key application of garbled RAM. However, this construction is secure only against semi-honest adversaries.

In this paper we provide a cut-and-choose technique for garbled RAM. This gives the first constant round two-party secure computation protocol for RAM programs secure against malicious adversaries that makes only black-box use of the underlying cryptographic primitives. Our protocol allows for garbling multiple RAM programs being executed on a persistent database. Security of our construction is argued in the random oracle model.
]]></description>
<guid>http://eprint.iacr.org/2016/907</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/906</link>
<title><![CDATA[On Basing Search SIVP on NP-Hardness]]>, by Tianren Liu</title>
<description><![CDATA[The possibility of basing cryptography on the minimal assumption $\textbf{NP}\nsubseteq \textbf{BPP}$ is at the very heart of textbflexity-theoretic cryptography. The closest we have gotten so far is lattice-based cryptography whose average-case security is based on the worst-case hardness of approximate shortest vector problems on integer lattices. The state-of-the-art is the construction of a one-way function (and collision-resistant hash function) based on the hardness of the $\tilde{O}(n)$-approximate shortest independent vector problem $\textsf{SIVP}_{\tilde O(n)}$.

Although $\textsf{SIVP}$ is \textbf{NP}-hard in its exact version, Guruswami et al (CCC 2004) showed that $\textsf{gapSIVP}_{\sqrt{n/\log n}}$ is in $\textbf{NP} \cap \textbf{coAM}$ and thus unlikely to be $\textbf{NP}$-hard. Indeed, any textsfuage that can be reduced to $\textsf{gapSIVP}_{\tilde O(\sqrt n)}$ (under general probabilistic polynomial-time adaptive reductions) is in $\textbf{AM} \cap \textbf{coAM}$ by the results of Peikert and Vaikuntanathan (CRYPTO 2008) and Mahmoody and Xiao (CCC 2010). However, none of these results apply to reductions to {\em search problems}, still leaving open a ray of hope: {\em can $\textbf{NP}$ be reduced to solving search SIVP with approximation factor $\tilde O(n)$?}

We show that any textsfuage that can be reduced to solving search $\textsf{SIVP}$ with approximation factor $\tilde O(n)$ lies in \textbf{AM} intersect \textbf{coAM}, eliminating the possibility of basing current constructions on \textbf{NP}-hardness.
]]></description>
<guid>http://eprint.iacr.org/2016/906</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/901</link>
<title><![CDATA[Distance Bounding based on PUF]]>, by Mathilde Igier and Serge Vaudenay</title>
<description><![CDATA[Distance Bounding (DB) is designed to mitigate relay attacks. This paper provides a complete study of the DB protocol of Kleber et al. based on Physical Unclonable Functions (PUFs). We contradict the claim that it resists to Terrorist Fraud (TF). We propose some slight modifications to increase the security of the protocol and formally prove TF-resistance, as well as resistance to Distance Fraud (DF), and Man-In-the-Middle attacks (MiM) which include relay attacks.
]]></description>
<guid>http://eprint.iacr.org/2016/901</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/884</link>
<title><![CDATA[Robust, low-cost, auditable random number generation for embedded system security]]>, by Ben Lampert and Riad S. Wahby and Shane Leonard and Philip Levis</title>
<description><![CDATA[This paper presents an architecture for a discrete, high-entropy hardware random number generator. Because it is constructed out of simple hardware components, its operation is transparent and auditable. Using avalanche noise, a nondeterministic physical phenomenon, the circuit is inherently probabilistic and resists adversarial control. Furthermore, because it compares the outputs from two matched noise sources, it rejects environmental disturbances like power supply ripple. The resulting hardware produces more than 0.98 bits of entropy per sample, is inexpensive, has a small footprint, and can be disabled to conserve power when not in use.
]]></description>
<guid>http://eprint.iacr.org/2016/884</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/848</link>
<title><![CDATA[From Weakly Selective to Selective Security in Compact Functional Encryption]]>, by Linfeng Zhou</title>
<description><![CDATA[Functional Encryption (\(\mathsf{FE}\)) generalizes the notion of traditional encryption system by providing fine-grained access control. In a functional encryption scheme, the owner of the secret key can generate restricted functional keys that allow users to obtain specific functions over the encrypted messages and nothing else.

In this work, we show a generic transformation from weakly selective secure functional encryption to selectively secure functional encryption and this transformation preserves the compactness of the \(\mathsf{FE}\) scheme. This result is given by reusing techniques in the work of Ananth et al.(CRYPTO 2015) through a modified approach. Furthermore, combining recent results, we remark that this result gives an alternative approach of recent work by Garg and Srinivasan (TCC-B 2016). Namely, a single-key weakly selective secure functional encryption scheme, whose ciphertext size is \textit{sublinear} in the size of the function for which the functional key is issued, implies all other notions of functional encryption generically.
]]></description>
<guid>http://eprint.iacr.org/2016/848</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/841</link>
<title><![CDATA[A Methodology for the Characterisation of Leakages in Combinatorial Logic]]>, by Guido Bertoni and Marco Martinoli</title>
<description><![CDATA[Glitches represent a great danger for hardware implementations of cryptographic schemes. Their intrinsic random nature makes them difficult to tackle and their occurrence threatens side-channel protections. Although countermeasures aiming at structurally solving the problem already exist, they usually require some effort to be applied or introduce non-negligible overhead in the design. Our work addresses the gap between such countermeasures and the na{\"i}ve implementation of schemes being vulnerable in the presence of glitches. Our contribution is twofold: (1) we expand the mathematical framework proposed by Brzozowski and {\'E}sik (FMSD 2003) by meaningfully adding the notion of information leakage, (2) thanks to which we define a formal methodology for the analysis of vulnerabilities in combinatorial circuits when glitches are taken into account.
]]></description>
<guid>http://eprint.iacr.org/2016/841</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/835</link>
<title><![CDATA[Lightweight Diffusion Layer: Importance of Toeplitz Matrices]]>, by Sumanta Sarkar and Habeeb Syed </title>
<description><![CDATA[MDS matrices are used as building blocks of diffusion layers in block ciphers, and XOR count is a metric that estimates the hardware implementation cost. In this paper we report the minimum value of XOR counts of $4 \times 4$ MDS matrices over $\mathbb{F}_{2^4}$ and $\mathbb{F}_{2^8}$, respectively. We give theoretical constructions of Toeplitz MDS matrices and show that they achieve the minimum XOR count. We also prove that Toeplitz matrices cannot be both MDS and involutory. Further we give theoretical constructions of $4 \times 4$ involutory MDS matrices over $\mathbb{F}_{2^4}$ and $\mathbb{F}_{2^8}$ that have the best known XOR counts so far: for $\mathbb{F}_{2^4}$ our construction gives an involutory MDS matrix that actually improves the existing lower bound of XOR count, whereas for $\mathbb{F}_{2^8}$, it meets the known lower bound.
]]></description>
<guid>http://eprint.iacr.org/2016/835</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/792</link>
<title><![CDATA[Key-Homomorphic Signatures and Applications to Multiparty Signatures]]>, by David Derler and Daniel Slamanig</title>
<description><![CDATA[Key-homomorphic properties of cryptographic objects have proven to be useful, both from a theoretical as well as a practical perspective. Important cryptographic objects such as pseudorandom functions or (public key) encryption have been studied previously with respect to key-homomorphisms. Interestingly, however, signature schemes have not been explicitly investigated in this context so far. 

We close this gap and initiate the study of key-homomorphic signatures, which turns out to be an interesting and versatile concept. In doing so, we firstly propose a definitional framework for key-homomorphic signatures distilling various natural flavours of key-homomorphic properties. Those properties aim to generalize larger classes of existing signature schemes, which makes it possible to infer general statements about signature schemes from those classes by simply making black-box use of the respective properties. We then employ our definitional framework to show elegant and simple compilers from classes of schemes admitting different types of key-homomorphisms to a number of other interesting primitives such as ring signature schemes, (universal) designated verifier signature schemes and multisignature schemes. Additionally, using the formalisms provided by our framework, we can prove a tight implication from single-user security to key-prefixed multi-user security for a class of schemes admitting a certain key-homomorphism.

Moreover, we introduce the notion of multikey-homomorphic signatures. Such schemes provide homomorphic properties on the message space of signatures under different keys. We discuss key-homomorphisms in this context and present some first constructive results from key-homomorphic schemes. Finally, we discuss some interesting open problems and an application of multikey-homomorphic schemes to verifiable delegation of computations.
]]></description>
<guid>http://eprint.iacr.org/2016/792</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/790</link>
<title><![CDATA[Conditional Cube Attack on Reduced-Round Keccak Sponge Function]]>, by Senyang Huang and Xiaoyun Wang and Guangwu Xu and Meiqin Wang  and Jingyuan Zhao</title>
<description><![CDATA[The security analysis of Keccak, the winner of SHA-3, has
attracted considerable interest. Recently, some attention has been paid
to the analysis of keyed modes of Keccak sponge function. As a notable
example, the most ecient key recovery attacks on Keccak-MAC and
Keyak were reported at EUROCRYPT'15 where cube attacks and cubeattack-
like cryptanalysis have been applied. In this paper, we develop
a new type of cube distinguisher, the conditional cube tester, for Keccak
sponge function. By imposing some bit conditions for certain cube
variables, we are able to construct cube testers with smaller dimensions.
Our conditional cube testers are used to analyse Keccak in keyed modes.
For reduced-round Keccak-MAC and Keyak, our attacks greatly improve
the best known attacks in key recovery in terms of the number of rounds
or the complexity. Moreover, our new model can also be applied to
keyless setting to distinguish Keccak sponge function from random permutation.We provide a searching algorithm to produce the most ecient
conditional cube tester by modeling it as an MILP (mixed integer linear
programming) problem. As a result, we improve the previous distinguishing
attacks on Keccak sponge function signicantly. Most of our attacks
have been implemented and veried by desktop computers. Finally we
remark that our attacks on the the reduced-round Keccak will not threat
the security margin of Keccak sponge function.
]]></description>
<guid>http://eprint.iacr.org/2016/790</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/746</link>
<title><![CDATA[Improved Private Set Intersection against Malicious Adversaries]]>, by Peter Rindal and Mike Rosulek</title>
<description><![CDATA[Private set intersection (PSI) refers to a special case of secure two-party computation in which the parties each have a set of items and compute the intersection of these sets without revealing any additional information. In this paper we present improvements to practical PSI providing security in the presence of {\em malicious} adversaries.

Our starting point is the protocol of Dong, Chen \& Wen (CCS 2013) that is based on Bloom filters. We identify a bug in their malicious-secure variant and show how to fix it using a cut-and-choose approach that has low overhead while simultaneously avoiding one the main computational bottleneck in their original protocol. We also point out some subtleties that arise when using Bloom filters in malicious-secure cryptographic protocols.

We have implemented our PSI protocols and report on its performance. Our improvements reduce the cost of Dong et al.'s protocol by a factor of $14-110\times$ on a single thread. When compared to the previous fastest protocol of De Cristofaro et al., we improve the running time by $8-24\times$. For instance, our protocol has an online time of 14 seconds and an overall time of 2.1 minutes to securely compute the intersection of two sets of 1 million items each.
]]></description>
<guid>http://eprint.iacr.org/2016/746</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/741</link>
<title><![CDATA[MARKOV MODELING OF MOVING TARGET DEFENSE GAMES]]>, by Hoda Maleki and Saeed Valizadeh and William Koch and Azer Bestavros and Marten van Dijk</title>
<description><![CDATA[We introduce a Markov-model-based framework for Moving Target Defense (MTD) analysis. The framework allows modeling of a broad range of MTD strategies, provides general theorems about how the probability of a successful adversary defeating an MTD strategy is related to the amount of time/cost spent by the adversary, and shows how a multi-level composition of MTD strategies can be analyzed by a straightforward combination of the analysis for each one of these strategies. Within the proposed framework we define the concept of security capacity which measures the strength or effectiveness of an MTD strategy: the security capacity depends on MTD specific parameters and more general system parameters. We apply our framework to two concrete MTD strategies.
]]></description>
<guid>http://eprint.iacr.org/2016/741</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/728</link>
<title><![CDATA[Sophos -  Forward Secure Searchable Encryption]]>, by Raphael Bost</title>
<description><![CDATA[Searchable Symmetric Encryption aims at making possible searching over an encrypted database stored on an untrusted server while keeping privacy of both the queries and the data, by allowing some small controlled leakage to the server. 

Recent work shows that dynamic schemes -- in which the data is efficiently updatable -- leaking some information on updated keywords are subject to devastating adaptative attacks breaking the privacy of the queries.
The only way to thwart this attack is to design \emph{forward private} schemes whose update procedure does not leak if a newly inserted element matches previous search queries.

This work proposes ${\Sigma o\phi o\varsigma}$ as a forward private SSE scheme with performance similar to existing less secure schemes, and that is conceptually simpler (and also more efficient) than previous forward private constructions.
In particular, it only relies on trapdoor permutations and does not use an ORAM-like construction.
We also explain why ${\Sigma o\phi o\varsigma}$ is an optimal point of the security/performance tradeoff for SSE.

Finally, an implementation and evaluation results demonstrate its practical efficiency.
]]></description>
<guid>http://eprint.iacr.org/2016/728</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/714</link>
<title><![CDATA[All the AES You Need on Cortex-M3 and M4]]>, by Peter Schwabe and Ko Stoffelen</title>
<description><![CDATA[This paper describes highly-optimized AES-{128, 192, 256}-CTR assembly implementations for the popular ARM Cortex-M3 and M4 embedded microprocessors. These implementations are about twice as fast as existing implementations. Additionally, we provide the fastest bitsliced constant-time and masked implementations of AES-128-CTR to protect against timing attacks, power analysis and other (first-order) side-channel attacks. All implementations, including an architecture-specific instruction scheduler and register allocator, which we use to minimize expensive loads, are released into the public domain.
]]></description>
<guid>http://eprint.iacr.org/2016/714</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/682</link>
<title><![CDATA[Finding Significant Fourier Coefficients: Clarifications, Simplifications, Applications and Limitations]]>, by Steven D. Galbraith, Joel Laity and Barak Shani</title>
<description><![CDATA[Ideas from Fourier analysis have been used in cryptography for the last three decades. Akavia, Goldwasser and Safra unified some of these ideas to give a complete algorithm that finds significant Fourier coefficients of functions on any finite abelian group. Their algorithm stimulated a lot of interest in the cryptography community, especially in the context of ``bit security''. This paper attempts to be a friendly and comprehensive guide to the tools and results in this field.
The intended readership is cryptographers who have heard about these tools and seek an understanding of their mechanics, and their usefulness and limitations.
A compact overview of the algorithm is presented with emphasis on the ideas behind it. We survey some applications of this algorithm, and explain that several results should be taken in the right context. We point out that some of the most important bit security problems are still open. Our original contributions include: an approach to the subject based on modulus switching; a discussion of the limitations on the usefulness of these tools; an answer to an open question about the modular inversion hidden number problem.
]]></description>
<guid>http://eprint.iacr.org/2016/682</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/660</link>
<title><![CDATA[The SKINNY Family of Block Ciphers and its Low-Latency Variant MANTIS]]>, by Christof Beierle and Jérémy Jean and Stefan Kölbl and Gregor Leander and Amir Moradi and Thomas Peyrin and Yu Sasaki and Pascal Sasdrich and Siang Meng Sim</title>
<description><![CDATA[We present a new tweakable block cipher family SKINNY , whose goal is to compete with NSA recent design SIMON in terms of hardware/software performances, while proving in addition much stronger security guarantees with regards to differential/linear attacks. In particular, unlike SIMON, we are able to provide strong bounds for all versions, and not only in the single-key model, but also in the related-key or related-tweak model. SKINNY  has flexible block/key/tweak sizes and can also benefit from very efficient threshold implementations for side-channel protection. Regarding performances, it outperforms all known ciphers for ASIC round-based implementations, while still reaching an extremely small area for serial implementations and a very good efficiency for software and micro-controllers implementations (SKINNY has the smallest total number of AND/OR/XOR gates used for encryption process).

Secondly, we present MANTIS, a dedicated variant of SKINNY for low-latency implementations, that constitutes a very efficient solution to the problem of designing a tweakable block cipher for memory encryption. MANTIS basically reuses well understood, previously studied, known components. Yet, by putting those components together in a new fashion, we obtain a competitive cipher to PRINCE in latency and area, while being enhanced with a tweak input.
]]></description>
<guid>http://eprint.iacr.org/2016/660</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/658</link>
<title><![CDATA[Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3]]>, by Kevin Atighehchi and Alexis Bonnecaze</title>
<description><![CDATA[Discussions are currently underway about the choice of a tree hash mode of operation for a standardization. It appears that a single tree mode cannot address the specificities of all possible uses and specifications of a system. In this paper, we review the tree modes which have been proposed, we discuss their problems and propose remedies. We make the reasonable assumption that communicating systems have different specifications and that software applications are of different types (securing stored content or live-streamed content). More particularly, we propose modes of operation that address the memory usage problem. When designing a parallel algorithm, one major question is how to improve the running time (using as many processors as we want) while minimizing the required memory of an implementation using as few as one processor. Conversely, an interesting question is how to obtain a near-optimal running time while containing the memory consumption.
]]></description>
<guid>http://eprint.iacr.org/2016/658</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/622</link>
<title><![CDATA[Function-Revealing Encryption]]>, by Marc Joye and Alain Passelègue</title>
<description><![CDATA[Multi-input functional encryption is a paradigm that allows an authorized user to compute a certain function ---and nothing more--- over multiple plaintexts given only their encryption. The particular case of two-input functional encryption has very exciting applications like comparing the relative order of two plaintexts from their encrypted form, making range queries over an encrypted database, testing if two encrypted databases share common entries, and more.

While being extensively studied, multi-input functional encryption is not ready for a practical deployment, mainly for two reasons.  First, known constructions rely on heavy cryptographic tools such as multilinear maps.  Second, their security is still very uncertain, as revealed by recent devastating attacks.

This paper investigates a simpler approach.  Rather than addressing multi-input functional encryption in its full generality, we target specific functions and relax the security notions.  As a result, we obtain several practical realizations of multi-input encryption for specialized applications, including an efficient construction of order-revealing encryption with limited leakage, under the standard DLin assumption.
]]></description>
<guid>http://eprint.iacr.org/2016/622</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/599</link>
<title><![CDATA[Obfuscation from Low Noise Multilinear Maps]]>, by Nico Döttling and Sanjam Garg and Divya Gupta and Peihan Miao and Pratyay Mukherjee</title>
<description><![CDATA[Multilinear maps enable homomorphic computation on encoded values and a public procedure to check if the computation on the encoded values results in a zero. Encodings in known candidate constructions of multilinear maps have a (growing) noise component, which is crucial for security. For example, noise in GGH13 multilinear maps grows with the number of levels that need to be supported and must remain below the maximal noise supported by the multilinear map for correctness. A smaller maximal noise, which must be supported, is desirable both for reasons of security and efficiency.

In this work, we put forward new candidate constructions of obfuscation for which the maximal supported noise is polynomial (in the security parameter).
Our constructions are obtained by instantiating a modification of the Lin's [EUROCRYPT 2016] obfuscation construction with composite order variants of the GGH13 multilinear maps. For these schemes, we show that the maximal supported noise only needs to grow polynomially in the security parameter. We prove the security of these constructions in the weak multilinear map model that captures \emph{all known} vulnerabilities of GGH13 maps. Finally, we investigate the security of the considered composite order variants of GGH13 multilinear maps from a cryptanalytic standpoint.
]]></description>
<guid>http://eprint.iacr.org/2016/599</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/592</link>
<title><![CDATA[Subspace Trail Cryptanalysis and its Applications to AES]]>, by Lorenzo Grassi and Christian Rechberger and and Sondre Rønjom</title>
<description><![CDATA[We introduce subspace trail cryptanalysis, a generalization of invariant subspace cryptanalysis. With this more generic treatment of subspaces we do no longer rely on specific choices of round constants or subkeys, and the resulting method is as such a
potentially more powerful attack vector. Interestingly, subspace trail cryptanalysis in fact includes techniques based on impossible or truncated differentials and integrals as special
cases. 

Choosing AES-128 as the perhaps most studied cipher, we describe distinguishers up to 5-round AES with a single unknown key. We report (and practically verify) competitive key-recovery
attacks with very low data-complexity on 2, 3 and 4 rounds of AES. Additionally, we consider AES with a secret S-Box and we present a (generic) technique that allows to directly recover the secret key without finding any information about the secret S-Box.
This approach allows to use e.g. truncated differential, impossible differential and integral attacks to find the secret key. Moreover, this technique works also for other AES-like
constructions, if some very common conditions on the S-Box and on the MixColumns matrix (or its inverse) hold.

Finally, we show that our impossible differential attack on 5 rounds of AES with secret S-Box can be turned into a distinguisher for AES in the same setting as the one recently proposed by Sun, Liu, Guo, Qu and Rijmen at CRYPTO 2016. Besides having a much
lower data complexity, our result also provides a counter-example to the conjecture that the security margin for round-reduced AES under the chosen plaintext attack is different from that under the chosen-ciphertexts attack.
]]></description>
<guid>http://eprint.iacr.org/2016/592</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/549</link>
<title><![CDATA[Short and Adjustable Signatures]]>, by Xiong Fan and Juan Garay and Payman Mohassel</title>
<description><![CDATA[Motivated by the problem of one-time password generation with security against server breaches, we introduce the notion of {\em adjustable signature schemes} that allow the length of a signature to be adjusted---at the setup, signing or verification stages, depending on the application. Defining security for such schemes poses several challenges, such as: (i) different signature lengths should provide different levels of security, and  (ii) the effort required for forging a very short  signature (e.g., 6 bytes) should not be reusable for
forging additional signatures. We provide security definitions that concretely
capture the trade-off between signature length, number of forgeries and level
of security provided by the scheme.

The above requirements rule out all existing solutions for short signatures.
In this paper, as a feasibility result, we provide the first instantiation of
all variants of adjustable signatures based on indistinguishability
obfuscation. Our starting point is the state-of-the-art construction by
Ramchen and Waters [ACM CCS 2014]. We observe that their scheme fails to meet our requirements for an adjustable signature scheme, and enhance it to obtain adjustable signatures with {\em shorter} signatures, {\em faster} signing and {\em strong} unforgeability. We also employ new proof techniques in order toobtain the above-mentioned notions of security.

For the simpler case where adversarial effort does not grow with the number of forgeries, we also provide a concrete construction based on the BLS signature scheme, by instantiating it using smaller group sizes that yield shorter signature lengths while providing reasonable security. We implement this scheme for various signature sizes an report on its efficiency.
]]></description>
<guid>http://eprint.iacr.org/2016/549</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/545</link>
<title><![CDATA[On Trees, Chains and Fast Transactions in the Blockchain]]>, by Aggelos Kiayias and Giorgos Panagiotakos</title>
<description><![CDATA[A fundamental  open problem in the area of
blockchain protocols is whether the Bitcoin protocol
is the optimal solution (in terms of efficiency, security)
for building a secure transaction ledger. 
A recently proposed and 
widely considered alternative is the 
\GHOST protocol which, notably, 
was proposed to be at the core of Ethereum
as well as  other recent proposals for improved Bitcoin-like
systems. 
%
The \GHOST variant is touted as offering superior performance compared to Bitcoin (potentially offering block production
speed up by a factor of more than 40) without a security loss. Motivated by this, in this work, we study 
from both a  provable security  and attack susceptibility  point of view the problem of transaction processing time
for both \GHOST and Bitcoin. 

We introduce a new formal framework for the analysis
of blockchain protocols that relies on trees (rather
than chains)  and we showcase the power of the framework
by providing a unified description of the \GHOST and Bitcoin protocols, 
the former of which we extract and formally describe. We then prove that  \GHOST implements a
``robust transaction ledger'' (i.e., possesses liveness and persistence) and hence it is
a provably secure alternative to Bitcoin. Our proof follows a novel methodology which may be of independent interest. 

Given this, we then ask whether \GHOST is a better alternative. 
We focus on the liveness property of both Bitcoin and \GHOST, i.e.,
the worst-case transaction confirmation time that can be expected when playing against
an adversary. We present a general 
attack methodology against liveness and we instantiate it with two attacks for  Bitcoin 
and \GHOST. We prove (i)  our attack for Bitcoin is optimal and (ii)  \GHOST, when  under our attack, performs, in expectation, {\em worse} than Bitcoin under the optimal attack,
for various parameter choices. 

With the above results, our work provides a first example of comparative study between different blockchain designs from a provable security perspective.
]]></description>
<guid>http://eprint.iacr.org/2016/545</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/544</link>
<title><![CDATA[Efficient Secure Comparison Protocols]]>, by Geoffroy Couteau</title>
<description><![CDATA[A secure comparison protocol allows players to evaluate the greater-than predicate on hidden values; it addresses a problem that belongs to the field of multiparty computation, in which players wish to jointly and privately evaluate a function on secret inputs. Introduced by Yao under the name millionaires' problem, secure comparisons have received a great deal of attention. They have proven to be one of the most fundamental building block in a large variety of multiparty computation protocols. However, due to their inherent non-arithmetic structure, they are in general less efficient than other fundamental primitives, and as such, are often a major bottleneck in multiparty computation protocols.

In this work, we design new two-party protocols for the greater-than functionality, secure against honest-but-curious adversaries (who follow the specifications of the protocol), improving over the state of the art. They can be readily used in a large variety of applications in which secure comparisons constitute the main efficiency bottleneck. Our protocols are defined in the preprocessing model, and are extremely efficient during the online phase. They are based solely on oblivious transfers, and can therefore use oblivious transfer extensions to get rid of all but a constant amount of expensive computations. Toward our goal of secure comparison, we also design protocols for testing equality between private inputs, which improve similarly over the state of the art. The latter contribution is of independent interest.
]]></description>
<guid>http://eprint.iacr.org/2016/544</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/526</link>
<title><![CDATA[Extended Tower Number Field Sieve with Application to Finite Fields of Arbitrary Composite Extension Degree]]>, by Taechan Kim and Jinhyuck Jeong</title>
<description><![CDATA[We propose a generalization of exTNFS algorithm recently introduced by Kim and Barbulescu (CRYPTO 2016). The algorithm, exTNFS, is a state-of-the-art algorithm for discrete logarithm in $\mathbb{F}_{p^n}$ in the medium prime case, but it only applies when $n=\eta\kappa$ is a composite with nontrivial factors $\eta$ and $\kappa$ such that $\gcd(\eta,\kappa)=1$. Our generalization, however, shows that exTNFS algorithm can be also adapted to the setting with an arbitrary composite $n$ maintaining its best asymptotic complexity. We show that one can solve discrete logarithm in medium case in the running time of $L_{p^n}(1/3, \sqrt[3]{48/9})$ (resp. $L_{p^n}(1/3, 1.71)$ if multiple number fields are used), where $n$ is an \textit{arbitrary composite}. This should be compared with a recent variant by Sarkar and Singh (Asiacrypt 2016) that has the fastest running time of $L_{p^n}(1/3, \sqrt[3]{64/9})$ (resp. $L_{p^n}(1/3, 1.88)$) when $n$ is a power of prime 2. When $p$ is of special form, the complexity is further reduced to $L_{p^n}(1/3, \sqrt[3]{32/9})$. On the practical side, we emphasize that the keysize of pairing-based cryptosystems should be updated following to our algorithm if the embedding degree $n$ remains composite.
]]></description>
<guid>http://eprint.iacr.org/2016/526</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/427</link>
<title><![CDATA[Privacy Preserving Network Analysis of Distributed Social Networks]]>, by Varsha Bhat Kukkala and Jaspal Singh Saini and S.R.S. Iyengar</title>
<description><![CDATA[Social network analysis as a technique has been applied to
a diverse set of fields, including, organizational behavior, 
sociology, economics and biology. However, for sensitive networks such as hate networks, 
trust networks and sexual networks, these techniques have been
sparsely used. This is majorly attributed to the unavailability of network
data. Anonymization is the most commonly used technique for performing 
privacy preserving network analysis. The process involves the presence of a 
trusted third party, who is aware of the complete network, and
releases a sanitized version of it. In this paper, we propose an 
alternative, in which, the desired analysis can be performed by the parties who
distributedly hold the network, such that : (a) no central third party is
required; (b) the topology of the underlying network is kept hidden. We
design multiparty protocols for securely performing few of the commonly
studied social network analysis algorithms. The current paper addresses
a secure implementation of the most commonly used network analysis
measures, which include degree distribution, closeness centrality, 
PageRank algorithm and K-shell decomposition algorithm. The designed
protocols are proven to be secure in the presence of an arithmetic black-box
extended with operations like comparison and modulo.
]]></description>
<guid>http://eprint.iacr.org/2016/427</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/421</link>
<title><![CDATA[Homomorphic Encryption for Arithmetic of Approximate Numbers]]>, by Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song</title>
<description><![CDATA[We suggest a method to construct a homomorphic encryption scheme for approximate
arithmetic. It supports approximate addition and multiplication of ciphertexts, together with the rescaling procedure for managing the magnitude of plaintext. This procedure rescales the ciphertext modulus to a smaller size, which results in rounding of plaintext after homomorphic operations.
The main idea is to place a noise after the signicant gures of message. This noise is added to the plaintext for security, but considered to be part of error occurred during approximate computations, which is reduced along with plaintext by rescaling. As a result, our decryption structure outputs an approximate value of plaintext with the predetermined precision.
We also propose a new batching algorithm for our Ring-LWE based construction. In the previous schemes a message has a noise multiplied by the characteristic of the plaintext space and so can be easily annihilated after reversing a packing algorithm. However, it does not work for our scheme with a plaintext of characteristic zero. Instead, we embed messages into Gaussian integers and adopt an isometric ring homomorphism for packing messages, which preserves the size of error and so enables us to remove the errors in the decryption procedure. Specically, a vector of plaintext values is converted into a complex polynomial via the canonical embedding and then discretized to a polynomial over the Gaussian integers.
Our construction has the bit size of ciphertext modulus linear in the circuit depth due to rescaling procedure while all the previous works either require exponentially large size of modulus or expensive computations such as bootstrapping or bit extraction. One important feature of our method is that the precision loss during evaluation is bounded by circuit depth and it is at most one more bit compared with unencrypted approximate arithmetic such as oating-point operations. In addition to the basic approximate circuits, we show that our scheme can be applied to eciently evaluate transcendental functions such as multiplicative inverse, exponentiation, and even fast Fourier transform.
]]></description>
<guid>http://eprint.iacr.org/2016/421</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/411</link>
<title><![CDATA[Polymorphic Encryption and Pseudonymisation   for Personalised Healthcare]]>, by Eric Verheul and Bart Jacobs and Carlo Meijer and Mireille Hildebrandt and Joeri de Ruiter</title>
<description><![CDATA[Polymorphic encryption and Pseudonymisation, abbreviated as PEP, form
a novel approach for the management of sensitive personal data,
especially in health care. Traditional encryption is rather rigid:
once encrypted, only one key can be used to decrypt the data. This
rigidity is becoming an every greater problem in the context of big
data analytics, where different parties who wish to investigate part
of an encrypted data set all need the one key for decryption.

Polymorphic encryption is a new cryptographic technique that solves
these problems. Together with the associated technique of polymorphic
pseudonymisation new security and privacy guarantees can be given
which are essential in areas such as (personalised) healthcare,
medical data collection via self-measurement apps, and more generally
in privacy-friendly identity management and data analytics.

The key ideas of polymorphic encryption are:
1. Directly after generation, data can be encrypted in a
  `polymorphic' manner and stored at a (cloud) storage facility in
  such a way that the storage provider cannot get access. Crucially,
  there is no need to a priori fix who gets to see the data, so that
  the data can immediately be protected.

For instance a PEP-enabled self-measurement device will store all its
measurement data in polymorphically encrypted form in a back-end data
base.

2. Later on it can be decided who can decrypt the data. This
  decision will be made on the basis of a policy, in which the data
  subject should play a key role.

The user of the PEP-enabled device can, for instance, decide that
doctors $X,Y,Z$ may at some stage decrypt to use the data in their
diagnosis, or medical researcher groups $A, B, C$ may use it for their
investigations, or third parties $U,V,W$ may use it for additional
services, etc.

3. This `tweaking' of the encrypted data to make it decryptable by
  a specific party can be done in a blind manner. It will have to be
  done by a trusted party who knows how to tweak the ciphertext for
  whom.

This PEP technology can provide the necessary security and privacy
infrastructure for big data analytics. People can entrust their data
in polymorphically encrypted form, and each time decide later to make
(parts of) it available (decryptable) for specific parties, for
specific analysis purposes. In this way users remain in control, and
can monitor which of their data is used where by whom for which
purposes.

The polymorphic encryption infrastructure can be supplemented with a
pseudonymisation infrastructure which is also polymorphic, and
guarantees that each individual will automatically have different
pseudonyms at different parties and can only be de-pseudonymised by
participants (like medical doctors) who know the original identity.

This white paper provides an introduction to Polymorphic Encryption
and Pseudonymisation (PEP), at different levels of abstraction,
focusing on health care as application area.  It contains a general
description of PEP, explaining the basic functionality for laymen,
supplemented by a clarification of the legal framework provided by the
upcoming General Data Protection Regulation (GDPR) of the European
Union.  The paper also contains a more advanced, mathematically
oriented description of PEP, including the underlying cryptographic
primitives, key and pseudonym managment, interaction protocols,
etc. This second part is aimed at readers with a background in
computer security and cryptography. The cryptographic basis for PEP is
ElGamal public key encryption, which is well-known since the mid
1980s. It is the way in which this encryption is used --- with
re-randomisation, re-keying and re-shuffling --- that is new.

The PEP framework is currently elaborated into an open design and open
source (prototype) implementation at Radboud University in Nijmegen,
The Netherlands. The technology will be used and tested in a real-life
medical research project at the Radboud University Medical Center.
]]></description>
<guid>http://eprint.iacr.org/2016/411</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/410</link>
<title><![CDATA[Efficient Quantum-Resistant Trust Infrastructure based on HIMMO]]>, by Oscar Garcia-Morchon and Sauvik Bhattacharya and Ronald Rietman and Ludo Tolhuizen and Jose-Luis Torre-Arce and Maarten Bodlaender</title>
<description><![CDATA[Secure Internet communications face conflicting demands: advances in (quantum) computers require stronger, quantum-resistant cryptographic algorithms, while at the same time the Internet of Things demands better-performing protocols. Finally, communication links usually depend on a single root-of-trust, e.g., a certification authority, a single point-of-failure that is too big of a risk for future systems.
This paper addresses these problems by proposing a hybrid infrastructure that combines the quantum-resistant HIMMO key pre-distribution scheme based on multiple Trusted Third Parties with public-key cryptography. 
During operation, any pair of devices can use private HIMMO key material and public keys to establish a secure and authenticated link, where their public keys are certified beforehand by multiple TTPs, acting as roots of trust. The solution is resilient to the capture of individual roots of trust without affecting performance, while public-key cryptography can provide features such as forward-secrecy. Combining HIMMO identities with public keys enables secure certification of public keys and distribution of HIMMO key material from multiple TTPs, without requiring an out-of-band channel. The infrastructure can be tuned to fit Internet of Things scenarios benefiting from an efficient, non-interactive and authenticated key exchange, or to fit use cases where the use of multiple TTPs provides privacy safe-guards when lawful interception is required.
Our TLS proof-of-concept shows the feasibility of our proposal by integrating the above security features with minimal changes. 
Our TLS implementation provides classic and post-quantum confidentiality and authentication, all while adding a computation overhead of only 2.8\% and communication overhead of approximately 50 bytes to a pre-quantum Elliptic Curve Diffie-Hellman ciphersuite.
]]></description>
<guid>http://eprint.iacr.org/2016/410</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/361</link>
<title><![CDATA[Functional Encryption for Bounded Collusions, Revisited]]>, by Shweta Agrawal</title>
<description><![CDATA[We provide a new construction of functional encryption (FE) for circuits in the bounded collusion model. In this model, security of the scheme is guaranteed as long as the number of colluding adversaries can be a-priori bounded by some polynomial q. Compared to the previous best construction in this model, by Gorbunov, Vaikuntanathan and Wee (CRYPTO'12), our scheme has the following advantages:

o The ciphertext of our scheme can be divided into a succinct data dependent component and a non-succinct data independent component. This makes it well suited for optimization in an online-offline model that allows a majority of the computation to be performed in an offline phase, before the data becomes available. This is followed by an efficient online phase, which is performed when the data becomes known. The online component of our scheme significantly outperforms the online component of [GVW12], and depends only on the message size, whereas that of [GVW12] additionally depends on the circuit size and the number of queries.

o The ciphertext of our scheme is decomposable - namely, the data dependent component of the ciphertext CTx may be decomposed as CT_1, . . . , CT_|x|, where CT_i depends only on the bit x_i. Since our ciphertext enjoys both decomposability as well as succinctness of the online component, our scheme is very suitable for distributed data applications where the data to be encrypted is held by multiple, separated parties that share only a PRF seed.

o The ciphertext of our scheme grows as O(q^2), whereas that in [GVW12] grows as O(q^4).
Security of our scheme is based on the Learning With Errors assumption (LWE) or its ring variant (Ring-LWE). We believe our techniques are of at least as much interest as our final result: our scheme makes progress towards building compact FE and hence indistinguishability obfuscation [LV16] from LWE. In more detail, it allows shifting the burden of computation from arbitrarily distributed data to well distributed noise via a new proof technique, which we call noisy functional encryption.
]]></description>
<guid>http://eprint.iacr.org/2016/361</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/350</link>
<title><![CDATA[Probabilistic Termination and Composability of Cryptographic Protocols]]>, by Ran Cohen and Sandro Coretti and Juan Garay and Vassilis Zikas</title>
<description><![CDATA[When analyzing the round complexity of multi-party protocols, one often overlooks the fact that underlying resources, such as a broadcast channel, can by themselves be expensive to implement. For example, it is well known that it is impossible to implement a broadcast channel by a (deterministic) protocol in a sub-linear (in the number of corrupted parties) number of rounds.

The seminal works of Rabin and Ben-Or from the early 80's demonstrated that limitations as the above can be overcome by using randomization and allowing parties to terminate at different rounds, igniting the study of protocols over point-to-point channels with probabilistic termination and expected constant round complexity. However, absent a rigorous simulation-based definition, the suggested protocols are proven secure in a property-based manner or via ad hoc simulation-based frameworks, therefore guaranteeing limited, if any, composability.

In this work, we put forth the first simulation-based treatment of multi-party cryptographic protocols with probabilistic termination. We define secure multi-party computation (MPC) with probabilistic termination in the UC framework and prove a universal composition theorem for probabilistic-termination protocols. Our theorem allows to compile a protocol using deterministic-termination hybrids into a protocol that uses expected-constant-round protocols for emulating these hybrids, preserving the expected round complexity of the calling protocol.

We showcase our definitions and compiler by providing the first composable protocols (with simulation-based security proofs) for the following primitives, relying on point-to-point channels: (1) expected-constant-round perfect Byzantine agreement, (2) expected-constant-round perfect parallel broadcast, and (3) perfectly secure MPC with round complexity independent of the number of parties.
]]></description>
<guid>http://eprint.iacr.org/2016/350</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/276</link>
<title><![CDATA[Arithmetic Coding and Blinding Countermeasures for Lattice Signatures: Engineering a Side-Channel Resistant Post-Quantum Signature Scheme with Compact Signatures]]>, by Markku-Juhani O. Saarinen</title>
<description><![CDATA[We describe new arithmetic coding techniques and side-channel blinding
countermeasures for lattice-based cryptography. Using these techniques,
we develop a  practical, compact, and more quantum-resistant variant of
the BLISS Ideal Lattice Signature Scheme. We first show how the BLISS 
parameters and hash-based random oracle can be modified to be more 
secure against quantum pre-image attacks while optimizing signature size.
Arithmetic Coding offers an information theoretically optimal compression
for stationary and memoryless sources, such as the discrete Gaussian
distributions often present in lattice-based cryptography. We show that
this technique gives better signature sizes than the previously proposed
advanced Huffman-based signature compressors. We further demonstrate
that arithmetic decoding from an uniform source to target distribution
is also an optimal non-uniform sampling method in the sense that a
minimal amount of true random bits is required. Performance of this new
Binary Arithmetic Coding (BAC) sampler is comparable to other practical
samplers. The same code, tables, or circuitry can be utilized for both
tasks, eliminating the need for separate sampling and compression
components. We then describe simple randomized blinding techniques that
can be applied to anti-cyclic polynomial multiplication to mask timing-
and power consumption side-channels in ring arithmetic. We further show
that the Gaussian sampling process can also be blinded by a
split-and-permute techniques as an effective countermeasure against
side-channel attacks.
]]></description>
<guid>http://eprint.iacr.org/2016/276</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/175</link>
<title><![CDATA[Online/Offline OR Composition of Sigma Protocols]]>, by Michele Ciampi and Giuseppe Persiano and Alessandra Scafuro and Luisa Siniscalchi and Ivan Visconti</title>
<description><![CDATA[Proofs of partial knowledge allow a prover to prove knowledge of witnesses for k out of n instances of NP languages. Cramer, Schoenmakers and Damg\aa rd [CDS94] provided an efficient construction of a 3-round public-coin witness-indistinguishable (k, n)-proof of partial knowledge for any NP language, by cleverly combining n executions of Sigma-protocols for that language. This transform assumes that all n instances are fully specified before the proof starts, and thus directly rules out the possibility of choosing some of the instances after the first round.
Very recently, Ciampi et al. [CPS+16] provided an improved transform where one of the instances can be specified in the last round. They focus on (1,2)-proofs of partial knowledge with the additional feature that one instance is defined in the last round, and could be adaptively chosen by the verifier. They left as an open question the existence of an efficient (1, 2)-proof of partial knowledge where no instance is known in the first round. More in general, they left open the question of constructing an efficient (k, n)-proof of partial knowledge where knowledge of all n instances can be postponed. Indeed, this property is achieved only by inefficient constructions requiring NP reductions [LS90].
In this paper we focus on the question of achieving adaptive-input proofs of partial knowledge. We provide through a transform the first efficient construction of a 3-round public-coin witness- indistinguishable (k, n)-proof of partial knowledge where all instances can be decided in the third round. Our construction enjoys adaptive-input witness indistinguishability. Additionally, the proof of knowledge property remains also if the adversarial prover selects instances adaptively at last round as long as our transform is applied to a proof of knowledge belonging to the widely used class of proofs of knowledge described in [Mau15, CD98]. Since knowledge of instances and witnesses is not needed before the last round, we have that the first round can be precomputed and in the online/offline setting our performance is similar to the one of [CDS94].
Our new transform relies on the DDH assumption (in contrast to the transforms of [CDS94, CPS+16] that are unconditional). We also show how to strengthen the transform of [CPS+16] so that it also achieves adaptive soundness, when the underlying combined protocols belong to the class of protocols described in [Mau15, CD98].
]]></description>
<guid>http://eprint.iacr.org/2016/175</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/154</link>
<title><![CDATA[Fully-Anonymous Short Dynamic Group Signatures Without Encryption]]>, by David Derler and Daniel Slamanig</title>
<description><![CDATA[Group signatures are a central tool in privacy-enhancing crypto, which allow members of a group to anonymously sign on behalf of the group. Ideally, group signatures are dynamic and thus allow to dynamically enroll new members to a group. For such schemes Bellare et al. (CT-RSA'05) proposed a strong security model (BSZ model) that preserves anonymity of a group signature even if an adversary can see arbitrary key exposures or arbitrary openings of other group signatures. All previous constructions achieving this strong anonymity notion follow the so called sign-encrypt-prove (SEP) paradigm. In contrast, all known constructions which avoid this paradigm and follow the alternative "without encryption" paradigm introduced by Bichsel et al. (SCN'10), only provide a weaker notion of anonymity (which can be problematic in practice). Until now, it was not clear if constructions following this paradigm, while providing strong anonymity in the sense of BSZ, even exist. 

In this paper we positively answer this question by providing a novel approach to dynamic group signature schemes following this paradigm, which is a composition of structure preserving signatures on equivalence classes (Asiacrypt'14) and other standard primitives. Our results are interesting for various reasons: We can prove our construction following this "without encryption" paradigm secure without requiring random oracles. Moreover, when opting for an instantiation in the ROM, the so obtained scheme is extremely efficient and outperforms existing fully anonymous constructions following the SEP paradigm regarding computational efficiency. Regarding constructions providing a weaker anonymity notion than BSZ, we surprisingly even outperform the popular short BBS group signature scheme (Crypto'04) and thereby obtain shorter signatures.
]]></description>
<guid>http://eprint.iacr.org/2016/154</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/137</link>
<title><![CDATA[Rate-1, Linear Time and Additively Homomorphic UC Commitments]]>, by Ignacio Cascudo and Ivan Damgård and Bernardo David and Nico Döttling and Jesper Buus Nielsen</title>
<description><![CDATA[We propose the first UC commitment scheme for binary strings with the optimal properties of  rate approaching 1 and linear time (in the amortised sense, using a small number of seed OTs). On top of this, the scheme is additively homomorphic, which allows for applications to maliciously secure 2-party computation.
As tools for obtaining this, we make three contributions of independent interest: we construct the first (binary) linear time encodable codes with non-trivial distance and rate approaching 1, we construct the first almost universal hash function with small seed that can be computed in linear time, and we introduce a new primitive called interactive proximity testing that can be used to verify whether a string is close to a given linear code.
]]></description>
<guid>http://eprint.iacr.org/2016/137</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/128</link>
<title><![CDATA[Removing the Strong RSA Assumption from Arguments over the Integers]]>, by Geoffroy Couteau and Thomas Peters and David Pointcheval</title>
<description><![CDATA[Committing integers and proving relations between them is an essential ingredient in many cryptographic protocols. Among them, range proofs have shown to be fundamental. They consist in proving that a committed integer lies in a public interval, which can be seen as a particular case of the more general Diophantine relations: for the committed vector of integers x, there exists a vector of integers w such that P (x,w) = 0, where P is a polynomial.
In this paper, we revisit the security strength of the statistically hiding commitment scheme over the integers due to Damgard-Fujisaki, and the zero-knowledge proofs of knowledge of openings. Our first main contribution shows how to remove the Strong RSA assumption and replace it by the standard RSA assumption in the security proofs. This improvement naturally extends to generalized commitments and more complex proofs without modifying the original protocols.
As a second contribution, we design an interactive technique turning commitment scheme over the integers into commitment scheme modulo a prime p. Still under the RSA assumption, this results in more efficient proofs of relations between committed values. Our methods thus improve upon existing proof systems for Diophantine relations both in terms of performance and security. We illustrate that with more efficient range proofs under the sole RSA assumption.
]]></description>
<guid>http://eprint.iacr.org/2016/128</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/125</link>
<title><![CDATA[Compact Identity Based Encryption from LWE]]>, by Daniel Apon and Xiong Fan and Feng-Hao Liu</title>
<description><![CDATA[We construct an  identity-based encryption (IBE) scheme from the standard Learning with Errors (LWE) assumption that has \emph{compact} public-key and achieves adaptive security in the standard model. In particular, our scheme only needs 2 public matrices to support $O(\log^2 \secparam)$-bit length identity, and $O(\secparam / \log^2 \secparam)$ public matrices to support $\secparam$-bit length identity. This improves over previous IBE schemes from lattices substantially.

Additionally, our techniques from IBE can be adapted to construct a compact digital signature scheme, which achieves existential unforgeability under the standard Short Integer Solution (SIS) assumption with small polynomial parameters.
]]></description>
<guid>http://eprint.iacr.org/2016/125</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/121</link>
<title><![CDATA[Tightly-Secure Pseudorandom Functions via Work Factor Partitioning]]>, by Tibor Jager</title>
<description><![CDATA[We introduce a new technique for tight security proofs called work factor partitioning. Using this technique in a modified version of the framework of Döttling and Schröder (CRYPTO 2015), we obtain the first generic construction of tightly-secure pseudorandom functions (PRFs) from PRFs with small domain.

By instantiating the small-domain PRFs with the Naor-Reingold function (FOCS 1997) or its generalization by Lewko and Waters (ACM CCS 2009), this yields the first fully-secure PRFs whose black-box security proof loses a factor of only O(log^2 \lambda), where \lambda is the security parameter.

Interestingly, our variant of the Naor-Reingold construction can be seen as a standard Naor-Reingold PRF (whose security proof has a loss of \Theta(\lambda)), where a special encoding is applied to the input before it is processed. The tightness gain comes almost for free: the encoding is very efficiently computable and increases the length of the input only by a constant factor smaller than 2.
]]></description>
<guid>http://eprint.iacr.org/2016/121</guid>
</item>
<item>
<link>http://eprint.iacr.org/2016/088</link>
<title><![CDATA[On Linear Hulls and Trails]]>, by Tomer Ashur and Vincent Rijmen</title>
<description><![CDATA[This paper improves the understanding of linear cryptanalysis by highlighting some previously overlooked aspects. It shows that linear hulls are sometimes formed already in a single round, and that overlooking such hulls may lead to a wrong estimation of the linear correlation, and thus of the data complexity. It shows how correlation matrices can be used to avoid this, and provides a tutorial on how to use them properly. By separating the input and output masks from the key mask it refines the formulas for computing the expected correlation and the expected linear potential. Finally, it shows that when the correlation of a hull is not properly estimated (e.g., by using the correlation of a single trail as the correlation of the hull), the success probability of Matsui's Algorithm 1 drops, sometimes drastically. It also shows that when the trails composing the hull are properly accounted for, more than a single key bit can be recovered using Algorithm 1. 

All the ideas presented in this paper are followed by examples comparing previous methods to the corrected ones, and verified experimentally with reduced-round versions of Simon32/64.
]]></description>
<guid>http://eprint.iacr.org/2016/088</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/1128</link>
<title><![CDATA[New directions in nearest neighbor searching with applications to lattice sieving]]>, by Anja Becker and Léo Ducas and Nicolas Gama and Thijs Laarhoven</title>
<description><![CDATA[To solve the approximate nearest neighbor search problem (NNS) on the sphere, we propose a method using locality-sensitive filters (LSF), with the property that nearby vectors have a higher probability of surviving the same filter than vectors which are far apart. We instantiate the filters using spherical caps of height 1 - A, where a vector survives a filter if it is contained in the corresponding spherical cap, and where ideally each filter has an independent, uniformly random direction. 

For small A, these filters are very similar to the spherical locality-sensitive hash (LSH) family previously studied by Andoni et al. For larger A bounded away from 0, these filters potentially achieve a superior performance, provided we have access to an efficient oracle for finding relevant filters. Whereas existing LSH schemes are limited by a performance parameter of P \geq 1/(2c^2 - 1) to solve approximate NNS with approximation factor c, with spherical LSF we potentially achieve smaller asymptotic values of P, depending on the density of the data set. For sparse data sets where the dimension is super-logarithmic in the size of the data set, we asymptotically obtain P = 1/(2c^2 - 1), while for a logarithmic dimensionality with density constant K we obtain asymptotics of P \sim 1/(4 K c^2).

To instantiate the filters and prove the existence of an efficient decoding oracle, we replace the independent filters by filters taken from certain structured random product codes. We show that the additional structure in these concatenation codes allows us to decode efficiently using techniques similar to lattice enumeration, and we can find the relevant filters with low overhead, while at the same time not significantly changing the collision probabilities of the filters.

We finally apply spherical LSF to sieving algorithms for solving the shortest vector problem (SVP) on lattices, and show that this leads to a heuristic time complexity for solving SVP in dimension n of (3/2)^{n/2 + o(n)} ~ 2^{0.292 n + o(n)}. This asymptotically improves upon the previous best algorithms for solving SVP which use spherical LSH and cross-polytope LSH and run in time 2^{0.298 n + o(n)}. Experiments with the GaussSieve validate the claimed speedup and show that this method may be practical as well, as the polynomial overhead is small. Our implementation is available under an open-source license.
]]></description>
<guid>http://eprint.iacr.org/2015/1128</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/1106</link>
<title><![CDATA[POPE: Partial Order Preserving Encoding]]>, by Daniel S. Roche and Daniel Apon and Seung Geol Choi and Arkady Yerukhimovich</title>
<description><![CDATA[Recently there has been much interest in performing search queries over encrypted data to enable functionality while protecting sensitive data.  One particularly efficient mechanism for executing such queries is order-preserving encryption/encoding (OPE) which results in ciphertexts that preserve the relative order of the underlying plaintexts thus allowing range and comparison queries to be performed directly on ciphertexts.  Recently, Popa et al. (S&P 2013) gave the first construction of an ideally-secure OPE scheme and Kerschbaum (CCS 2015) showed how to achieve the even stronger notion of frequency-hiding OPE.  However, as Naveed et al. (CCS 2015) have recently demonstrated, these constructions remain vulnerable to several attacks.  Additionally, all previous ideal OPE schemes (with or without frequency-hiding) either require a large round complexity of O(log n) rounds for each insertion, or a large persistent client storage of size O(n), where n is the number of items in the database. It is thus desirable to achieve a range query scheme addressing both issues gracefully.

In this paper, we propose an alternative approach to range queries over encrypted data that is optimized to support insert-heavy workloads as are common in "big data" applications while still maintaining search functionality and achieving stronger security.  Specifically, we propose a new primitive called partial order preserving encoding (POPE) that achieves ideal OPE security with frequency hiding and also leaves a sizable fraction of the data pairwise incomparable.  Using only O(1) persistent and $O(n^\epsilon)$ non-persistent client storage for $0 < \epsilon < 1$, our POPE scheme provides extremely fast batch insertion consisting of a single round, and efficient search with O(1) amortized cost for up to $O(n^{1-\epsilon})$ search queries.  This improved security and performance makes our scheme better suited for today's insert-heavy databases.
]]></description>
<guid>http://eprint.iacr.org/2015/1106</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/1093</link>
<title><![CDATA[C$\emptyset$C$\emptyset$: A Framework for Building Composable Zero-Knowledge Proofs]]>, by Ahmed Kosba and Zhichao Zhao and Andrew Miller and Yi Qian and Hubert Chan and Charalampos Papamanthou and Rafael Pass and abhi shelat and Elaine Shi</title>
<description><![CDATA[Non-interactive zero-knowledge proofs are a powerful cryptographic primitive used in privacy-preserving protocols. We design and build C$\emptyset$C$\emptyset$, the first system enabling developers to build efficient, composable, non-interactive zero-knowledge proofs for generic, user-defined statements. C$\emptyset$C$\emptyset$ extends state-of-the-art SNARK constructions by applying known  strengthening transformations to yield UC-composable zero-knowledge proofs suitable for modular use in  larger cryptographic protocols.

To attain fast practical performance, C$\emptyset$C$\emptyset$ includes a library of several ``SNARK-friendly'' cryptographic primitives. These primitives are used in the strengthening transformations in order to reduce the overhead of achieving composable security.  Our open-source library of optimized arithmetic circuits for these functions are up to 40$\times$ more efficient than standard implementations and are thus of independent interest for use in other NIZK projects.

Finally, we evaluate C$\emptyset$C$\emptyset$ on applications such as anonymous credentials, private smart contracts, and nonoutsourceable proof-of-work puzzles and demonstrate 5$\times$ to 8$\times$ speedup in these application settings compared to naive implementations.
]]></description>
<guid>http://eprint.iacr.org/2015/1093</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/1082</link>
<title><![CDATA[Delegating RAM Computations with Adaptive Soundness and Privacy]]>, by Prabhanjan Ananth and Yu-Chi Chen and Kai-Min Chung and Huijia Lin and Wei-Kai Lin</title>
<description><![CDATA[We consider the problem of delegating RAM computations over
  persistent databases. A user wishes to delegate a sequence of
  computations over a database to a server, where each computation may
  read and modify the database and the modifications persist between
  computations. Delegating RAM computations is important as it has the
  distinct feature that the run-time of computations maybe 
    sub-linear in the size of the database.

  We present the first RAM delegation scheme that provide both
  soundness and privacy guarantees in the adaptive setting,
  where the sequence of delegated RAM programs are chosen adaptively,
  depending potentially on the encodings of the database and
  previously chosen programs. Prior works either achieved only
  adaptive soundness without privacy [Kalai and Paneth, ePrint'15], or
  only security in the selective setting where all RAM programs are
  chosen statically [Chen et al. ITCS'16, Canetti and Holmgren
  ITCS'16].

  Our scheme assumes the existence of indistinguishability obfuscation
  ($\iO$) for circuits and the decisional Diffie-Hellman (DDH)
  assumption. However, our techniques are quite general and in particular, might be applicable even in settings where iO is not used. We provide a
    "security lifting technique" that "lifts" any proof of
  selective security satisfying certain special properties into a
  proof of adaptive security, for arbitrary cryptographic schemes. We
  then apply this technique to the delegation scheme of Chen et al.
  and its selective security proof, obtaining that their scheme is
  essentially already adaptively secure. Because of the general
  approach, we can also easily extend to delegating parallel RAM
  (PRAM) computations.  We believe that the security lifting technique
  can potentially find other applications and is of independent
  interest.
]]></description>
<guid>http://eprint.iacr.org/2015/1082</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/1019</link>
<title><![CDATA[Speed-Security Tradeoffs in Blockchain Protocols]]>, by Aggelos Kiayias and Giorgos Panagiotakos</title>
<description><![CDATA[Transaction processing speed is one of the major considerations 
in cryptocurrencies that are based on proof of work (POW) such as Bitcoin. At an intuitive level it is widely understood that processing speed is at odds with the security aspects of the underlying POW based consensus mechanism of such protocols, nevertheless the tradeoff between the two properties is still not well understood. 

In this work, motivated by recent work \cite{GKL15} 
in the formal analysis of the Bitcoin backbone protocol, 
we investigate the tradeoff between provable security and transaction processing speed viewing the latter as a function of the block generation rate. 
%
We introduce a new formal  property of blockchain protocols, 
called {\em chain growth}, and we show it is fundamental 
for arguing the security of a robust transaction ledger. 
%
We strengthen the results of \cite{GKL15} in the following ways: 
we show how the properties of persistence and liveness of the ledger reduce in a black-box
fashion in the underlying properties of the backbone protocol, namely common prefix, chain quality and chain growth, and we improve the security bounds showing that the robustness of the ledger holds for even  the faster (than Bitcoin's)  block 
generation rates which have been adopted by other ``alt-coins.''
%
We also present a theoretical attack  against bitcoin which we validate in simulation that works when blockchain rate is highly accelerated. 
This presents a natural upper bound in the context of the speed-security tradeoff. 
By combining our positive and negative results we map the speed/security domain for blockchain protocols and list open problems for future work.
]]></description>
<guid>http://eprint.iacr.org/2015/1019</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/843</link>
<title><![CDATA[Rethinking Privacy for Extended Sanitizable Signatures and a Black-Box Construction of Strongly Private Schemes]]>, by David Derler and Daniel Slamanig</title>
<description><![CDATA[Sanitizable signatures, introduced by Ateniese et al. at ESORICS'05, allow to issue a signature on a message where certain predefined message blocks may later be changed (sanitized) by some dedicated party (the sanitizer) without invalidating the original signature. With sanitizable signatures, replacements for modifiable (admissible) message blocks can be chosen arbitrarily by the sanitizer. However, in various scenarios this makes sanitizers too powerful. To reduce the sanitizers power, Klonowski and Lauks at ICISC'06 proposed (among others) an extension that enables the signer to limit the allowed modifications per admissible block to a well defined set each. At CT-RSA'10 Canard and Jambert then extended the formal model of Brzuska et al. from PKC'09 to additionally include the aforementioned and other extensions. We, however, observe that the privacy guarantees of their model do not capture privacy in the sense of the original definition of sanitizable signatures. That is, if a scheme is private in this model it is not guaranteed that the sets of allowed modifications remain concealed. To this end, we review a stronger notion of privacy, i.e., (strong) unlinkability (defined by Brzuska et al. at EuroPKI'13), in this context. While unlinkability fixes this problem, no efficient unlinkable scheme supporting the aforementioned extensions exists and it seems to be hard to construct such schemes. As a remedy, in this paper, we propose a notion stronger than privacy, but weaker than unlinkability, which captures privacy in the original sense. Moreover, it allows to easily construct efficient schemes satisfying our notion from secure existing schemes in a black-box fashion.
]]></description>
<guid>http://eprint.iacr.org/2015/843</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/755</link>
<title><![CDATA[TESLA: Tightly-Secure Efficient Signatures from Standard Lattices]]>, by Erdem Alkim and Nina Bindel and Johannes Buchmann and Özgür Dagdelen and Peter Schwabe</title>
<description><![CDATA[Generally, lattice-based cryptographic primitives offer good performance and allow for strong security reductions. However, the most efficient current lattice-based signature schemes sacrifice (part of their) security to achieve good performance: first, security is not based on the worst-case hardness of lattice problems. Secondly, the security reductions of the most efficient schemes are non-tight; hence, their choices of parameters offer security merely heuristically. Moreover, lattice-based signature schemes are instantiated for classical adversaries, although they are based on presumably quantum-hard problems. Yet, it is not known how such schemes perform in a post-quantum world. 
We bridge this gap by proving the lattice-based signature scheme TESLA to be tightly secure based on the learning with errors problem over lattices in the random-oracle model. As such, we improve the security of the original proposal by Bai and Galbraith (CT-RSA'14) twofold: we tighten the security reduction and we minimize the underlying security assumptions. Remarkably, by enhancing the security we can greatly improve TESLA's performance. Furthermore, we are first to propose parameters providing a security of 128 bits against both classical and quantum adversaries, for a lattice-based signature scheme. Our implementation of TESLA competes well with state-of-the-art lattice-based signatures and SPHINCS (EUROCRYPT'15), the only signature scheme instantiated with quantum-hard parameters so far.
]]></description>
<guid>http://eprint.iacr.org/2015/755</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/712</link>
<title><![CDATA[Adaptive Proofs have Straightline Extractors (in the Random Oracle Model)]]>, by David Bernhard and Bogdan Warinschi and Ngoc Khanh Nguyen</title>
<description><![CDATA[Abstract. The concept of adaptive security for proofs of knowledge was recently studied by Bernhard et al. They formalised adaptive security in the ROM and showed that the non-interactive version of the Schnorr protocol obtained using the Fiat-Shamir transformation is not adaptively secure unless the one-more discrete logarithm problem is easy. Their only construction for adaptively secure protocols used the Fischlin transformation [3] which yields protocols with straight-line extractors. In this paper we provide two further key insights. Our main result shows that any adaptively secure protocol must have a straight-line extractor: even the most clever rewinding strategies cannot oer any benets against adaptive provers.


Then, we show that any Fiat-Shamir transformed SIGMA-protocol is not adaptively secure unless a related problem which we call the SIGMA-one-wayness problem is easy. This assumption concerns not just Schnorr but applies to a whole class of SIGMA-protocols including e.g. Chaum-Pedersen and representation proofs. We also prove that SIGMA-one-wayness is hard in the generic group model. Taken together, these results suggest that Fiat-Shamir transformed SIGMA-protocols should not be used in settings where adaptive security is important.
]]></description>
<guid>http://eprint.iacr.org/2015/712</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/674</link>
<title><![CDATA[Preprocessing-Based Verification of Multiparty Protocols with Honest Majority]]>, by Roman Jagomägis and Peeter Laud and Alisa Pankova</title>
<description><![CDATA[This paper presents a generic "GMW-style" method for turning passively secure protocols into protocols secure against covert attacks, adding relatively cheap offline preprocessing and post-execution verification phases. The execution phase, after which the computed result is already available to the parties, has only negligible overhead.

Our method uses shared verification based on precomputed and -verified multiplication triples. The verification proceeds by the verifiers repeating the computations of the prover in secret-shared manner, checking that they obtain the same messages that the prover sent out during execution. The verification preserves the privacy guarantees of the original protocol. It is applicable to protocols doing computations over finite rings, even if the same protocol performs its computation over several distinct rings at once.
We apply our verification method to the Sharemind platform for secure multiparty computations (SMC), evaluate its performance and compare it to other existing SMC platforms offering security against stronger than passive attackers.
]]></description>
<guid>http://eprint.iacr.org/2015/674</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/629</link>
<title><![CDATA[BeleniosRF: A Non-interactive Receipt-Free Electronic Voting Scheme]]>, by Pyrros Chaidos and Véronique Cortier and Georg Fuchsbauer and David Galindo</title>
<description><![CDATA[We propose a new voting scheme, BeleniosRF, that offers both  receipt-freeness and end-to-end verifiability. It is receipt-free in a strong sense, meaning that even dishonest voters cannot prove how they voted. We provide a game-based definition of receipt-freeness for voting protocols with non-interactive ballot casting, which we name strong receipt-freeness (sRF). To our knowledge, sRF is the first game-based definition of receipt-freeness in the literature, and it has the merit of being particularly concise and simple. Built upon the Helios protocol, BeleniosRF inherits its simplicity and does not require any anti-coercion strategy from the voters. We implement BeleniosRF and show its feasibility on a number of platforms, including desktop computers and smartphones.
]]></description>
<guid>http://eprint.iacr.org/2015/629</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/525</link>
<title><![CDATA[Short Randomizable Signatures]]>, by David Pointcheval and Olivier Sanders</title>
<description><![CDATA[Digital signature is a fundamental primitive with numerous applications. Following the development of pairing-based cryptography, several taking advantage of this setting have been proposed. Among them, the Camenisch-Lysyanskaya (CL) signature scheme is one of the most flexible and has been used as a building block for many other protocols. Unfortunately, this scheme suffers from a linear size in the number of messages to be signed which limits its use in many situations.

In this paper, we propose a new signature scheme with the same features as CL-signatures but without the linear-size drawback: our signature consists of only two elements, whatever the message length, and our algorithms are more efficient. This construction takes advantage of using type 3 pairings, that are already widely used for security and efficiency reasons.

We prove the security of our scheme without random oracles but in the generic group model. Finally, we show that protocols using CL-signatures can easily be instantiated with ours, leading to much more efficient constructions.
]]></description>
<guid>http://eprint.iacr.org/2015/525</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/233</link>
<title><![CDATA[Election Verifiability: Cryptographic Definitions and an Analysis of Helios and JCJ]]>, by Ben Smyth and Steven Frink and Michael R. Clarkson</title>
<description><![CDATA[Election verifiability is defined in the computational 
model of cryptography.  The definition formalizes
notions of voters verifying their own votes, auditors
verifying the tally of votes, and auditors verifying that
only eligible voters vote.  
The Helios (Adida et al., 2009), Helios-C (Cortier et al., 2014) and
JCJ (Juels et al., 2010) election schemes are analyzed using the definition.
Neither Helios nor Helios-C satisfy the definition
because they fail to ensure that recorded ballots are 
tallied in certain cases when the adversary posts malicious material on the bulletin board.
A variant of Helios is proposed and shown to satisfy the definition.
JCJ does not satisfy the definition because of a trust assumption it makes,
but it does satisfy a weakened definition.
Two previous definitions of verifiability (Juels et al., 2010; Cortier et al., 2014)
are shown to permit election schemes vulnerable to attacks, whereas the new definition 
prohibits those schemes. 
And a relationship between the new definition and global verifiability (Kuesters et al., 2010) is shown.
]]></description>
<guid>http://eprint.iacr.org/2015/233</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/163</link>
<title><![CDATA[Indistinguishability Obfuscation from Functional Encryption]]>, by Nir Bitansky and Vinod Vaikuntanathan</title>
<description><![CDATA[Indistinguishability obfuscation (IO) is a tremendous notion, powerful enough to give rise to almost any known cryptographic object. Prior candidate IO constructions were based on specific assumptions on algebraic objects called multi-linear graded encodings.

We present a generic construction of indistinguishability obfuscation from public-key functional encryption with succinct encryption circuits and subexponential security. This shows the equivalence of indistinguishability obfuscation and public-key functional encryption, a primitive that has so far seemed to be much weaker, lacking the power and the staggering range of applications of indistinguishability obfuscation.

Our main construction can be based on functional encryption schemes that support a {\em single function key}, and where the encryption circuit grows sub-linearly in the circuit-size of the function. We further show that sublinear succinctness in circuit-size for single-key schemes can be traded with sublinear succinctness in the number of keys (also known as the {\em collusion-size}) for multi-key schemes. As a consequence, we obtain a new candidate IO construction based on the functional encryption scheme of Garg, Gentry, Halevi and Zhandry (TCC'16) under their assumptions on multi-linear graded encodings. We also show that, under the Learning with Errors assumption, our techniques imply that any indistinguishability obfuscator can be converted into one where the size of obfuscated circuits is twice that of the original circuit plus an additive overhead that is polynomial in its depth, input length, and the security parameter.

Our reduction highlights the importance of succinctness in functional encryption schemes, which we hope will serve as a pathway to new IO constructions based on solid cryptographic foundations.
]]></description>
<guid>http://eprint.iacr.org/2015/163</guid>
</item>
<item>
<link>http://eprint.iacr.org/2015/129</link>
<title><![CDATA[Block-wise Non-Malleable Codes]]>, by Nishanth Chandran and Vipul Goyal and Pratyay Mukherjee and Omkant Pandey and Jalaj Upadhyay</title>
<description><![CDATA[Non-malleable codes, introduced by Dziembowski, Pietrzak, and Wichs (ICS '10) provide the guarantee that if a codeword c of a message m, is modied by a tampering function f to c',
then c' either decodes to m or to "something unrelated" to m. It is known that non-malleable
codes cannot exist for the class of all tampering functions and hence a lot of work has focused
on explicitly constructing such codes against a large and natural class of tampering functions.
One such popular, but restricted, class is the so-called split-state model in which the tampering
function operates on different parts of the codeword independently.
In this work, we consider a stronger adversarial model called block-wise tampering model,
in which we allow tampering to depend on more than one block: if a codeword consists of
two blocks c = (c1; c2), then the first tampering function f1 could produce a tampered part
c'1 = f1(c1) and the second tampering function f2 could produce c'2 = f2(c1; c2) depending on
both c2 and c1. The notion similarly extends to multiple blocks where tampering of block ci
could happen with the knowledge of all cj for j<=i. We argue this is a natural notion where,
for example, the blocks are sent one by one and the adversary must send the tampered block
before it gets the next block.
A little thought reveals however that one cannot construct such codes that are non-malleable
(in the standard sense) against such a powerful adversary: indeed, upon receiving the last block,
an adversary could decode the entire codeword and then can tamper depending on the message.

In light of this impossibility, we consider a natural relaxation called non-malleable codes with
replacement which requires the adversary to produce not only related but also a valid codeword
in order to succeed. Unfortunately, we show that even this relaxed difintion is not achievable in
the information-theoretic setting (i.e., when the tampering functions can be unbounded) which
implies that we must turn our attention towards computationally bounded adversaries.

As our main result, we show how to construct block-wise non-malleable codes from 
sub-exponentially hard one-way permutations. Moreover, we provide an interesting connection between block-wise
non-malleable codes and non-malleable commitments. We show that any block-wise nonmalleable
code can be converted into a non-malleable (w.r.t. opening) commitment scheme.
Our techniques, quite surprisingly, give rise to a non-malleable commitment scheme (secure
against so-called synchronizing adversaries), in which only the committer sends messages. We
believe this result to be of independent interest. In the other direction, we show that any
non-interactive non-malleable (w.r.t. opening) commitment can be used to construct a block-wise
non-malleable code only with 2 blocks. Unfortunately, such commitment scheme exists
only under highly non-standard assumptions (adaptive one-way functions) and hence can not
substitute our main construction.
]]></description>
<guid>http://eprint.iacr.org/2015/129</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/1026</link>
<title><![CDATA[Lattices with Symmetry]]>, by H. W. Lenstra, Jr. and A. Silverberg</title>
<description><![CDATA[For large ranks, there is no good algorithm that decides whether a given lattice has an orthonormal basis. But when the lattice is given with enough symmetry, we can construct a provably deterministic polynomial-time algorithm to accomplish this, based on the work of Gentry and Szydlo. The techniques involve algorithmic algebraic number theory, analytic number theory, commutative algebra, and lattice basis reduction.
]]></description>
<guid>http://eprint.iacr.org/2014/1026</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/805</link>
<title><![CDATA[Dual-System Simulation-Soundness with Applications to UC-PAKE and More]]>, by Charanjit S. Jutla  and Arnab Roy</title>
<description><![CDATA[We introduce a novel concept of dual-system simulation-sound non-interactive zero-knowledge (NIZK) proofs. Dual-system NIZK proof system can be seen as a two-tier proof system. As opposed to the usual notion of zero-knowledge proofs, dual-system defines an intermediate partial-simulation world, where the proof simulator may have access to additional auxiliary information about the potential language member, for example a membership bit, and simulation of proofs is only guaranteed if the membership bit is correct. Further, dual-system NIZK proofs allow a quasi-adaptive setting where the CRS can be generated based on language parameters. This allows for the further possibility that the partial-world CRS simulator may have access to further trapdoors related to the language parameters. We show that for important hard languages like the Diffie-Hellman language, such dual-system proof systems can be given which allow unbounded partial simulation soundness, and which further allow transition between partial simulation world and single-theorem full simulation world even when proofs are sought on non-members. The construction is surprisingly simple, involving only two additional group elements in asymmetric bilinear pairing groups.

As a first application we show a first single-round universally-composable password authenticated key-exchange (UC-PAKE) protocol which is secure under dynamic corruption in the erasure model. The single message flow only requires four group elements under the SXDH assumption, which is at least two times shorter than earlier schemes.
Adaptive Corruption is proved for a relaxed ideal functionality using non-information oracles.

As another application we give a short keyed-homomorphic CCA-secure encryption scheme. The ciphertext in this scheme consist of only six group elements (under the SXDH assumption) and the security reduction is linear-preserving. An earlier scheme of Libert et al based on their efficient unbounded simulation-sound QA-NIZK proofs only provided a quadratic-preserving security reduction, and further had ciphertexts almost twice as long as ours.
]]></description>
<guid>http://eprint.iacr.org/2014/805</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/792</link>
<title><![CDATA[General Classification of the Authenticated Encryption Schemes for the  CAESAR Competition]]>, by Farzaneh abed and Christian Forler and Stefan Lucks</title>
<description><![CDATA[An Authenticated encryption scheme is a scheme which provides privacy and integrity by using a secret key. In 2013, CAESAR (the ``Competition for Authenticated Encryption: Security, Applicability, and Robustness'')  was co-founded by NIST and Dan Bernstein with the aim of finding authenticated encryption schemes
that offer advantages over AES-GCM and are suitable for widespread adoption.
The first round started with 57 candidates in March 2014; and nine of these 
first-round candidates where broken and withdrawn from the competition. The 
remaining 48 candidates went through an intense process of review, analysis 
and comparison. While the cryptographic community benefits greatly from the 
manifold different submission designs, their sheer number
implies a challenging amount of study. This paper provides
an easy-to-grasp overview over functional aspects, security parameters, and
robustness offerings by the CAESAR candidates, clustered by their underlying
designs (block-cipher-, stream-cipher-, permutation-/sponge-,
compression-function-based, dedicated). After intensive review and analysis of all 48 candidates by the community, the CAESAR committee selected only 30 candidates for the second round. The announcement for the third round candidates was made on 15th August 2016 and 15 candidates were chosen for the third round.
]]></description>
<guid>http://eprint.iacr.org/2014/792</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/731</link>
<title><![CDATA[Secure modular password authentication for the web using channel bindings]]>, by Mark Manulis and Douglas Stebila and Franziskus Kiefer and Nick Denham</title>
<description><![CDATA[Secure protocols for password-based user authentication are well-studied in the cryptographic literature but have failed to see wide-spread adoption on the Internet; most proposals to date require extensive modifications to the Transport Layer Security (TLS) protocol, making deployment challenging.  Recently, a few modular designs have been proposed in which a cryptographically secure password-based mutual authentication protocol is run inside a confidential (but not necessarily authenticated) channel such as TLS; the password protocol is bound to the established channel to prevent active attacks.  Such protocols are useful in practice for a variety of reasons: security no longer relies on users' ability to validate server certificates and can potentially be implemented with no modifications to the secure channel protocol library.

We provide a systematic study of such authentication protocols.  Building on recent advances in modelling TLS, we give a formal definition of the intended security goal, which we call password-authenticated and confidential channel establishment (PACCE).  We show generically that combining a secure channel protocol, such as TLS, with a password authentication or password authenticated key exchange protocol, where the two protocols are bound together using the transcript of the secure channel's handshake, the server's certificate, or the server's domain name, results in a secure PACCE protocol.  Our prototypes based on TLS are available as a cross-platform client-side Firefox browser extension as well as an Android application and a server-side web application that can easily be installed on servers.
]]></description>
<guid>http://eprint.iacr.org/2014/731</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/230</link>
<title><![CDATA[Isogeny graphs with maximal real multiplication]]>, by Sorina Ionica and Emmanuel Thomé</title>
<description><![CDATA[An isogeny graph is a graph whose vertices are principally polarizable abelian varieties and whose edges are isogenies between these varieties. In his thesis, Kohel describes
the structure of isogeny graphs for elliptic curves and shows that one may compute the endomorphism ring of an elliptic curve defined over a finite field by using a depth-first search (DFS) algorithm in the graph. In dimension 2, the structure of isogeny graphs is less understood and existing algorithms for computing endomorphism rings are very expensive. In this article, we show that, under certain circumstances, the problem of determining the endomorphism ring can also be solved in genus 2 with a DFS-based algorithm. We consider
the case of genus-2 Jacobians with complex multiplication, with the assumptions that the real multiplication subring is maximal and has class number one. We describe the isogeny graphs in that case, locally at prime numbers which split in the real multiplication subfield. The resulting algorithm is implemented over finite fields, and examples are provided. To the best of our knowledge, this is the first DFS-based algorithm in genus 2.
]]></description>
<guid>http://eprint.iacr.org/2014/230</guid>
</item>
<item>
<link>http://eprint.iacr.org/2014/132</link>
<title><![CDATA[Efficient Revocable Identity-Based Encryption via Subset Difference Methods]]>, by Kwangsu Lee and Dong Hoon Lee and Jong Hwan Park</title>
<description><![CDATA[Providing an efficient revocation mechanism for identity-based encryption (IBE) is very important since a user's credential (or private key) can be expired or revealed. Revocable IBE (RIBE) is an extension of IBE that provides an efficient revocation mechanism. Previous RIBE schemes essentially use the complete subtree (CS) scheme of Naor, Naor and Lotspiech (CRYPTO 2001) for key revocation. In this paper, we present a new technique for RIBE that uses the efficient subset difference (SD) scheme of Naor et al. instead of using the CS scheme to improve the size of update keys.

Following our new technique, we first propose an efficient RIBE scheme in prime-order bilinear groups by combining the IBE scheme of Boneh and Boyen and the SD scheme and prove its selective security under the standard assumption. Our RIBE scheme is the first RIBE scheme in bilinear groups that has $O(r)$ number of group elements in an update key where $r$ is the number of revoked users. Next, we also propose another RIBE scheme in composite-order bilinear groups and prove its full security under static assumptions. Our RIBE schemes also can be integrated with the layered subset difference (LSD) scheme of Halevy and Shamir (CRYPTO 2002) to reduce the size of a private key.
]]></description>
<guid>http://eprint.iacr.org/2014/132</guid>
</item>
<item>
<link>http://eprint.iacr.org/2013/269</link>
<title><![CDATA[CMCC: Misuse Resistant Authenticated Encryption with Minimal Ciphertext Expansion]]>, by Jonathan Trostle</title>
<description><![CDATA[In some wireless environments, minimizing the size of messages is paramount due to the resulting significant energy savings. We present CCS which is a new family of tweakable enciphering schemes (TES). The main focus for this work is minimizing ciphertext expansion, especially for short messages including plaintext lengths less than the underlying block cipher length (e.g., 16 bytes). CMCC is an instantiation of the scheme providing authenticated encryption with associated data (AEAD), that is also nonce misuse resistant, and it leverages existing modes such as CBC, Counter, and CMAC. Our work can be viewed as extending the line of work starting with [HR03] to plaintext sizes smaller than the block cipher block length which is a problem posed in [Hal04]. For many existing AEAD schemes, a successful forgery leads directly to a loss of confidentiality. For CMCC, changes to the ciphertext randomize the resulting plaintext, thus forgeries do not result in a loss of confidentiality which allows us to reduce the length of the authentication tag. For protocols that send short messages, our scheme is similar to Counter with CBC-MAC (CCM) for computational overhead but has much smaller
expansion. We prove CCA2 security and misuse resistant authenticated encryption (MRAE) security for different variants of CMCC. Our contributions include both stateless and stateful versions which enable minimal sized message numbers using different network related trade-offs.
]]></description>
<guid>http://eprint.iacr.org/2013/269</guid>
</item>
</channel></rss>
