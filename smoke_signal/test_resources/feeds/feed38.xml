<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Onionesque Reality</title>
	<atom:link href="https://onionesquereality.wordpress.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://onionesquereality.wordpress.com</link>
	<description>..::A Random Walk::..</description>
	<lastBuildDate>Sun, 24 Jul 2016 00:38:22 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='onionesquereality.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s2.wp.com/i/buttonw-com.png</url>
		<title>Onionesque Reality</title>
		<link>https://onionesquereality.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://onionesquereality.wordpress.com/osd.xml" title="Onionesque Reality" />
	<atom:link rel='hub' href='https://onionesquereality.wordpress.com/?pushpress=hub'/>
	<item>
		<title>Where does the Sigmoid in Logistic Regression come from?</title>
		<link>https://onionesquereality.wordpress.com/2016/05/18/where-does-the-sigmoid-in-logistic-regression-come-from/</link>
		<comments>https://onionesquereality.wordpress.com/2016/05/18/where-does-the-sigmoid-in-logistic-regression-come-from/#comments</comments>
		<pubDate>Wed, 18 May 2016 10:36:26 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Bayes Classifiers]]></category>
		<category><![CDATA[Bayes Risk]]></category>
		<category><![CDATA[Log Linear Models]]></category>
		<category><![CDATA[Logistic Regression]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=6025</guid>
		<description><![CDATA[Note: The title of this post is circular. But I use/abuse it because of the post linked below. I noticed on the Hacker News front page (and via multiple reshares on twitter), a discussion on why logistic regression uses a sigmoid. The article linked in the story talks about the log-odds ratio, and how it [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=6025&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;"><span style="color:#993300;"><strong>Note: The title of this post is circular. But I use/abuse it because of the post linked below.</strong></span></p>
<p style="text-align:justify;">I noticed on the <a href="https://news.ycombinator.com/item?id=11712573">Hacker News</a> front page (and via multiple reshares on twitter), a discussion on why logistic regression uses a sigmoid. The article linked in the story talks about the log-odds ratio, and how it <em>leads</em> to the sigmoid (and gives a good intuitive plug on it).</p>
<p style="text-align:justify;">However, I think that the more important question is &#8211; Why do you care about log-odds? <strong>Why do you use log-odds and not something else?</strong> The point of this quick post is to write out why using the log-odds is infact very well motivated in the first place, and once it is modeled by a linear function, what you get is the logistic function.</p>
<p style="text-align:justify;"><strong>Beginning with log-odds would infact be begging the question</strong>, so let us try to understand.</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;">To motivate and in order to define the loss etc, suppose we had a linear classifier: <img src="https://s0.wp.com/latex.php?latex=%5Chat%7By%7D+%3D+h%28%5Cmathbf%7Bx%7D%29+%3D+sign%28w_0+%2B+%5Cmathbf%7Bw%7D%5ET%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{y} = h(&#92;mathbf{x}) = sign(w_0 + &#92;mathbf{w}^T&#92;mathbf{x})" title="&#92;hat{y} = h(&#92;mathbf{x}) = sign(w_0 + &#92;mathbf{w}^T&#92;mathbf{x})" class="latex" />. This just means that for a vector input, we take the dot product with the parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{w}" title="&#92;mathbf{w}" class="latex" /> and take the sign.</p>
<p style="text-align:justify;">The learning problem would be to figure out a good direction <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{w}" title="&#92;mathbf{w}" class="latex" /> and a good location of the decision boundary <img src="https://s0.wp.com/latex.php?latex=w_0&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="w_0" title="w_0" class="latex" />.</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;">We want to figure these out so as to minimize the <em>expected</em> 0-1 loss (or expected number of mistakes) for the classifier <img src="https://s0.wp.com/latex.php?latex=h%3A+%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BY%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h: &#92;mathcal{X} &#92;rightarrow &#92;mathcal{Y} " title="h: &#92;mathcal{X} &#92;rightarrow &#92;mathcal{Y} " class="latex" />. The 0-1 loss for a datapoint-label pair <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D%2Cy&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x},y" title="&#92;mathbf{x},y" class="latex" /> is simply:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L%28h%28%5Cmathbf%7Bx%7D%29%2Cy%29+%3D+%5Cbegin%7Bcases%7D+0%2C+%26+%5Ctext%7Bif+%7D+h%28%5Cmathbf%7Bx%7D%29+%3D+y+%5C%5C+1%2C+%26+%5Ctext%7Bif+%7D+h%28%5Cmathbf%7Bx%7D%29+%5Cneq+y+%5Cend%7Bcases%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle L(h(&#92;mathbf{x}),y) = &#92;begin{cases} 0, &amp; &#92;text{if } h(&#92;mathbf{x}) = y &#92;&#92; 1, &amp; &#92;text{if } h(&#92;mathbf{x}) &#92;neq y &#92;end{cases}" title="&#92;displaystyle L(h(&#92;mathbf{x}),y) = &#92;begin{cases} 0, &amp; &#92;text{if } h(&#92;mathbf{x}) = y &#92;&#92; 1, &amp; &#92;text{if } h(&#92;mathbf{x}) &#92;neq y &#92;end{cases}" class="latex" /></p>
<p style="text-align:justify;">Now, the next question we would like to ask. What is the <em>risk</em> of this classifier that we want to minimize? The <em>risk</em> is the expected loss. That is, if we draw a random sample from the (unknown) distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bx%7D%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p(&#92;mathbf{x},y)" title="p(&#92;mathbf{x},y)" class="latex" />, what would be the expected error? More concretely:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=R%28h%29%C2%A0+%3D+%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D%2Cy%7D+%5BL%28h%28%5Cmathbf%7Bx%7D%29%2Cy%29%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="R(h)  = &#92;mathbb{E}_{&#92;mathbf{x},y} [L(h(&#92;mathbf{x}),y)]" title="R(h)  = &#92;mathbb{E}_{&#92;mathbf{x},y} [L(h(&#92;mathbf{x}),y)]" class="latex" /></p>
<p style="text-align:justify;">Writing out the expectation:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28h%29+%3D%5Cint_x+%5Csum_%7Bc%3D1%7D%5EC+L%28h%28%5Cmathbf%7Bx%7D%29%2Cc%29+p%28%5Cmathbf%7Bx%7D%2C+y%3Dc%29+d%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle R(h) =&#92;int_x &#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(&#92;mathbf{x}, y=c) d&#92;mathbf{x}" title="&#92;displaystyle R(h) =&#92;int_x &#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(&#92;mathbf{x}, y=c) d&#92;mathbf{x}" class="latex" /></p>
<p style="text-align:justify;">Using the chain rule this becomes:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28h%29+%3D%5Cint_x+%5Csum_%7Bc%3D1%7D%5EC+L%28h%28%5Cmathbf%7Bx%7D%29%2Cc%29+p%28y%3Dc%7C%5Cmathbf%7Bx%7D%29+p%28%5Cmathbf%7Bx%7D%29d%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle R(h) =&#92;int_x &#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(y=c|&#92;mathbf{x}) p(&#92;mathbf{x})d&#92;mathbf{x}" title="&#92;displaystyle R(h) =&#92;int_x &#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(y=c|&#92;mathbf{x}) p(&#92;mathbf{x})d&#92;mathbf{x}" class="latex" /></p>
<p style="text-align:justify;">It is important to understand this expression. This is not assuming anything about the data. However, it is this expression that we want to minimize if we want to get a good classifier. To minimize this expression, it suffices to simply minimize for the <em>conditional risk</em> for any point <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" /> (i.e. the middle part of the above expression):</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28h%7C%5Cmathbf%7Bx%7D%29+%3D%5Csum_%7Bc%3D1%7D%5EC+L%28h%28%5Cmathbf%7Bx%7D%29%2Cc%29+p%28y%3Dc%7C%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle R(h|&#92;mathbf{x}) =&#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(y=c|&#92;mathbf{x})" title="&#92;displaystyle R(h|&#92;mathbf{x}) =&#92;sum_{c=1}^C L(h(&#92;mathbf{x}),c) p(y=c|&#92;mathbf{x})" class="latex" /></p>
<p style="text-align:justify;">But this conditional risk can be written as:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28h%7C%5Cmathbf%7Bx%7D%29+%3D0+%5Ctimes+p%28y%3Dh%28%5Cmathbf%7Bx%7D%29%7C%5Cmathbf%7Bx%7D%29+%2B+1+%5Ctimes+%5Csum_%7Bc+%5Cneq+h%28%5Cmathbf%7Bx%7D%29%7D+p%28y%3Dc%7C%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle R(h|&#92;mathbf{x}) =0 &#92;times p(y=h(&#92;mathbf{x})|&#92;mathbf{x}) + 1 &#92;times &#92;sum_{c &#92;neq h(&#92;mathbf{x})} p(y=c|&#92;mathbf{x})" title="&#92;displaystyle R(h|&#92;mathbf{x}) =0 &#92;times p(y=h(&#92;mathbf{x})|&#92;mathbf{x}) + 1 &#92;times &#92;sum_{c &#92;neq h(&#92;mathbf{x})} p(y=c|&#92;mathbf{x})" class="latex" /></p>
<p style="text-align:justify;">Note that, <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%C2%A0%5Csum_%7Bc+%5Cneq+h%28%5Cmathbf%7Bx%7D%29%7D+p%28y%3Dc%7C%5Cmathbf%7Bx%7D%29+%3D+1+-+p%28y%3Dh%28%5Cmathbf%7Bx%7D%29%7C%5Cmathbf%7Bx%7D%29+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;sum_{c &#92;neq h(&#92;mathbf{x})} p(y=c|&#92;mathbf{x}) = 1 - p(y=h(&#92;mathbf{x})|&#92;mathbf{x}) " title="&#92;displaystyle &#92;sum_{c &#92;neq h(&#92;mathbf{x})} p(y=c|&#92;mathbf{x}) = 1 - p(y=h(&#92;mathbf{x})|&#92;mathbf{x}) " class="latex" /></p>
<p style="text-align:justify;">Therefore, the conditional risk is simply:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28h%7C%5Cmathbf%7Bx%7D%29%C2%A0+%3D+1+-+p%28y%3Dh%28%5Cmathbf%7Bx%7D%29%7C%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle R(h|&#92;mathbf{x})  = 1 - p(y=h(&#92;mathbf{x})|&#92;mathbf{x})" title="&#92;displaystyle R(h|&#92;mathbf{x})  = 1 - p(y=h(&#92;mathbf{x})|&#92;mathbf{x})" class="latex" /></p>
<p style="text-align:justify;">Now, it is this conditional risk that we want to minimize given a point <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" />. And in order to do so, looking at the expression above, the classifier must make the following decision:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+%5Carg%5Cmax_c+p%28y%3Dc%7C+%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle h(&#92;mathbf{x}) = &#92;arg&#92;max_c p(y=c| &#92;mathbf{x})" title="&#92;displaystyle h(&#92;mathbf{x}) = &#92;arg&#92;max_c p(y=c| &#92;mathbf{x})" class="latex" /></p>
<p style="text-align:justify;">It is again important to note that so far we have made absolutely no assumptions about the data. So the above classifier is the <em>best</em> classifier that we can have in terms of generalization, in the sense of what might be the expected loss on a new sample point. Such a classifier is called the <em>Bayes Classifier</em> or sometimes called the <em>Plug-in classifier.</em></p>
<p style="text-align:justify;">But the optimal decision rule mentioned above i.e. <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+%5Carg%5Cmax_c+p%28y%3Dc%7C+%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle h(&#92;mathbf{x}) = &#92;arg&#92;max_c p(y=c| &#92;mathbf{x})" title="&#92;displaystyle h(&#92;mathbf{x}) = &#92;arg&#92;max_c p(y=c| &#92;mathbf{x})" class="latex" /> is equivalent to saying that:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+c%5E%5Cast+%5Ciff+%5Cfrac%7Bp%28y+%3D+c%5E%5Cast%7C%5Cmathbf%7Bx%7D%29%7D%7Bp%28y%3Dc%7C%5Cmathbf%7Bx%7D%29%7D+%5Cgeq+1+%5Ctext%7B%C2%A0+%7D%5Cforall+c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle h(&#92;mathbf{x}) = c^&#92;ast &#92;iff &#92;frac{p(y = c^&#92;ast|&#92;mathbf{x})}{p(y=c|&#92;mathbf{x})} &#92;geq 1 &#92;text{  }&#92;forall c" title="&#92;displaystyle h(&#92;mathbf{x}) = c^&#92;ast &#92;iff &#92;frac{p(y = c^&#92;ast|&#92;mathbf{x})}{p(y=c|&#92;mathbf{x})} &#92;geq 1 &#92;text{  }&#92;forall c" class="latex" /></p>
<p style="text-align:justify;">by taking log, this would be:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+c%5E%5Cast+%5Ciff+%5Clog+%5Cfrac%7Bp%28y+%3D+c%5E%5Cast%7C%5Cmathbf%7Bx%7D%29%7D%7Bp%28y%3Dc%7C%5Cmathbf%7Bx%7D%29%7D+%5Cgeq+0+%5Ctext%7B%C2%A0+%7D%5Cforall+c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle h(&#92;mathbf{x}) = c^&#92;ast &#92;iff &#92;log &#92;frac{p(y = c^&#92;ast|&#92;mathbf{x})}{p(y=c|&#92;mathbf{x})} &#92;geq 0 &#92;text{  }&#92;forall c" title="&#92;displaystyle h(&#92;mathbf{x}) = c^&#92;ast &#92;iff &#92;log &#92;frac{p(y = c^&#92;ast|&#92;mathbf{x})}{p(y=c|&#92;mathbf{x})} &#92;geq 0 &#92;text{  }&#92;forall c" class="latex" /></p>
<p style="text-align:justify;">If, we were only dealing with binary classification, this would imply:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+1+%5Ciff+%5Clog+%5Cfrac%7Bp%28y+%3D+1%7C%5Cmathbf%7Bx%7D%29%7D%7Bp%28y%3D0%7C%5Cmathbf%7Bx%7D%29%7D+%5Cgeq+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle h(&#92;mathbf{x}) = 1 &#92;iff &#92;log &#92;frac{p(y = 1|&#92;mathbf{x})}{p(y=0|&#92;mathbf{x})} &#92;geq 0 " title="&#92;displaystyle h(&#92;mathbf{x}) = 1 &#92;iff &#92;log &#92;frac{p(y = 1|&#92;mathbf{x})}{p(y=0|&#92;mathbf{x})} &#92;geq 0 " class="latex" /></p>
<p style="text-align:justify;">Notice that by making no assumptions about the data, simply by writing out the <em>conditional</em> <em>risk, </em>the log-odds ratio has fallen out directly. This is not an accident, because the optimal bayes classifier has this form for binary classification. But the question still remains, how do we model this log-odds ratio? The simplest option is to consider a linear model (there is no reason to stick to a linear model, but due to some reasons, one being convexity, we stick to a linear model):</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+log+%5Cfrac%7Bp%28y+%3D+1%7C%5Cmathbf%7Bx%7D%29%7D%7Bp%28y%3D0%7C%5Cmathbf%7Bx%7D%29%7D+%3D+w_0+%2B+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle log &#92;frac{p(y = 1|&#92;mathbf{x})}{p(y=0|&#92;mathbf{x})} = w_0 + &#92;mathbf{w}^T &#92;mathbf{x} = 0 " title="&#92;displaystyle log &#92;frac{p(y = 1|&#92;mathbf{x})}{p(y=0|&#92;mathbf{x})} = w_0 + &#92;mathbf{w}^T &#92;mathbf{x} = 0 " class="latex" /></p>
<p style="text-align:justify;">Now, we know that <img src="https://s0.wp.com/latex.php?latex=p%28y%3D1%7C%5Cmathbf%7Bx%7D%29+%3D+1+-+p%28y%3D0%7C%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p(y=1|&#92;mathbf{x}) = 1 - p(y=0|&#92;mathbf{x})" title="p(y=1|&#92;mathbf{x}) = 1 - p(y=0|&#92;mathbf{x})" class="latex" />, plugging this in the above, and exponentiating, we have:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+log+%5Cfrac%7Bp%28y+%3D+1%7C%5Cmathbf%7Bx%7D%29%7D%7B1+-+p%28y%3D0%7C%5Cmathbf%7Bx%7D%29%7D+%3D+%5Cexp%28w_0+%2B+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D%29+%3D+1+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle log &#92;frac{p(y = 1|&#92;mathbf{x})}{1 - p(y=0|&#92;mathbf{x})} = &#92;exp(w_0 + &#92;mathbf{w}^T &#92;mathbf{x}) = 1 " title="&#92;displaystyle log &#92;frac{p(y = 1|&#92;mathbf{x})}{1 - p(y=0|&#92;mathbf{x})} = &#92;exp(w_0 + &#92;mathbf{w}^T &#92;mathbf{x}) = 1 " class="latex" /></p>
<p style="text-align:justify;">Rearranging, yields the familiar logistic model (and the sigmoid):</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28y+%3D+1%7C%5Cmathbf%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7B1%2B+%5Cexp%28-+w_0+-+%5Cmathbf%7Bw%7D%5ET%5Cmathbf%7Bx%7D%29%7D+%3D+%5Cfrac%7B1%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle p(y = 1|&#92;mathbf{x}) = &#92;frac{1}{1+ &#92;exp(- w_0 - &#92;mathbf{w}^T&#92;mathbf{x})} = &#92;frac{1}{2}" title="&#92;displaystyle p(y = 1|&#92;mathbf{x}) = &#92;frac{1}{1+ &#92;exp(- w_0 - &#92;mathbf{w}^T&#92;mathbf{x})} = &#92;frac{1}{2}" class="latex" />.</p>
<p style="text-align:justify;">As noted in the post linked in the beginning, the logistic model, <img src="https://s0.wp.com/latex.php?latex=%5Csigma%28x%29+%3D+%5Cfrac%7B1%7D%7B1%2B+e%5E%7B-x%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sigma(x) = &#92;frac{1}{1+ e^{-x}}" title="&#92;sigma(x) = &#92;frac{1}{1+ e^{-x}}" class="latex" />, which for any <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> is, <img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+%5Csigma%28x%29+%5Cleq+1&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="0 &#92;leq &#92;sigma(x) &#92;leq 1" title="0 &#92;leq &#92;sigma(x) &#92;leq 1" class="latex" />, and is monotonic <img src="https://s0.wp.com/latex.php?latex=%5Csigma%28-%5Cinf%29+%3D+0%2C+%5Csigma%28%2B%5Cinf%29+%3D+1&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sigma(-&#92;inf) = 0, &#92;sigma(+&#92;inf) = 1" title="&#92;sigma(-&#92;inf) = 0, &#92;sigma(+&#92;inf) = 1" class="latex" />.</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;"><strong>This derivation shows that the log-odds is not an arbitrary choice, infact a very natural choice. The sigmoid is simply a consequence of modeling the log-odds with a linear function</strong> (infact logistic regression is arguably the simplest example of a log-linear model, if we had structured outputs, a natural extension of such a model would be the Conditional Random Field. The choice of using a linear function is simply to make the optimization convex, amongst some other favourable properties).</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;">Note: This post was inspired by some very succinct notes by Gregory Shakhnarovich from his Machine Learning class, that I both took and served as a TA for.</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/6025/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/6025/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=6025&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2016/05/18/where-does-the-sigmoid-in-logistic-regression-come-from/feed/</wfw:commentRss>
		<slash:comments>3</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>A Note on the Graph Laplacian</title>
		<link>https://onionesquereality.wordpress.com/2015/03/29/a-note-on-the-graph-laplacian/</link>
		<comments>https://onionesquereality.wordpress.com/2015/03/29/a-note-on-the-graph-laplacian/#comments</comments>
		<pubDate>Sun, 29 Mar 2015 18:13:33 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Graph Laplacian]]></category>
		<category><![CDATA[Spectral Graph Theory]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5905</guid>
		<description><![CDATA[A short note on how the Graph Laplacian is a natural analogue of the Laplacian in vector analysis. For a twice differentiable function on the euclidean space, the Laplacian of , is the div(grad()) or the divergence of the gradient of . Let&#8217;s consider the laplacian in two dimensions: To get an intuition about what [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5905&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;"><span style="color:#800000;"><em><strong>A short note on how the Graph Laplacian is a natural analogue of the Laplacian in vector analysis.</strong></em></span></p>
<p style="text-align:justify;">For a twice differentiable function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> on the euclidean space, the Laplacian of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5CDelta+f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Delta f" title="&#92;Delta f" class="latex" /> is the div(grad(<img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />)) or the divergence of the gradient of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />. Let&#8217;s consider the laplacian in two dimensions:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CDelta+f%28x%2Cy%29+%3D+%5Cnabla+%5Ccdot+%5Cnabla+f%28x%2Cy%29+%3D+%5Cfrac%7Bd%5E2+f%28x%2Cy%29%7D%7Bdx%5E2%7D+%2B+%5Cfrac%7Bd%5E2+f%28x%2Cy%29%7D%7Bdy%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;Delta f(x,y) = &#92;nabla &#92;cdot &#92;nabla f(x,y) = &#92;frac{d^2 f(x,y)}{dx^2} + &#92;frac{d^2 f(x,y)}{dy^2}" title="&#92;displaystyle &#92;Delta f(x,y) = &#92;nabla &#92;cdot &#92;nabla f(x,y) = &#92;frac{d^2 f(x,y)}{dx^2} + &#92;frac{d^2 f(x,y)}{dy^2}" class="latex" /></p>
<p style="text-align:justify;">To get an intuition about what it does, it is useful to consider a finite difference approximation to the above:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CDelta+f%28x%2Cy%29+%5Capprox+%5Cfrac%7Bf%28x%2Bh%2Cy%29+%2B+f%28x-h%2Cy%29+%2B+f%28x%2Cy%2Bh%29+%2B+f%28x%2Cy-h%29+-+4f%28x%2Cy%29%7D%7Bh%5E2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;Delta f(x,y) &#92;approx &#92;frac{f(x+h,y) + f(x-h,y) + f(x,y+h) + f(x,y-h) - 4f(x,y)}{h^2}" title="&#92;displaystyle &#92;Delta f(x,y) &#92;approx &#92;frac{f(x+h,y) + f(x-h,y) + f(x,y+h) + f(x,y-h) - 4f(x,y)}{h^2}" class="latex" /></p>
<p style="text-align:justify;">The above makes it clear that the Laplacian behaves like a <strong><em>local averaging operator</em></strong>. Infact the above finite difference approximation is used in image processing (say, when h is set to one pixel) to detect sharp changes such as edges, as the above is close to zero in smooth image regions.</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;">The finite difference approximation intuitively suggests that a discrete laplace operator should be natural. At the same time, the Graph Theory literature defines the Graph Laplacian as the matrix <img src="https://s0.wp.com/latex.php?latex=L+%3D+D+-+A&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L = D - A" title="L = D - A" class="latex" /> (where <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D" title="D" class="latex" /> is the degree matrix and <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="A" title="A" class="latex" /> the adjacency matrix of the graph <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G" title="G" class="latex" />, both are defined below). It is not obvious that this matrix <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L" title="L" class="latex" /> follows from the div(grad(f)) definition mentioned above. The purpose of this note is to explicate in short on how this is natural, once the notion of gradient and divergence operators have been defined over a graph.</p>
<p style="text-align:justify;">First of all the two most natural matrices that can be associated with a graph are defined below:</p>
<p style="text-align:center;">____________________________</p>
<p style="text-align:justify;">There are two natural matrices that are used to specify a simple graph <img src="https://s0.wp.com/latex.php?latex=G+%3D+%28V%2CE%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G = (V,E)" title="G = (V,E)" class="latex" /> with vertex set <img src="https://s0.wp.com/latex.php?latex=V+%3D%5C%7B1%2C+%5Cdots%2C+n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="V =&#92;{1, &#92;dots, n &#92;}" title="V =&#92;{1, &#92;dots, n &#92;}" class="latex" /> and edge set <img src="https://s0.wp.com/latex.php?latex=E+%3D+%5C%7B1%2C+%5Cdots%2C+m+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="E = &#92;{1, &#92;dots, m &#92;}" title="E = &#92;{1, &#92;dots, m &#92;}" class="latex" />: The Adjacency matrix <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="A" title="A" class="latex" /> and the Incidence matrix <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;nabla" title="&#92;nabla" class="latex" />.</p>
<p style="text-align:justify;">The Adjacency matrix is defined as a <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="n &#92;times n" title="n &#92;times n" class="latex" /> matrix such that:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A_%7Bu%2Cv%7D+%3D+%5Cbegin%7Bcases%7D+1+%26+%5Cmbox%7Bif+%7D+uv+%5Cin+E+%5C%5C+0+%26+%5Cmbox%7Botherwise%7D+%5Cend%7Bcases%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle A_{u,v} = &#92;begin{cases} 1 &amp; &#92;mbox{if } uv &#92;in E &#92;&#92; 0 &amp; &#92;mbox{otherwise} &#92;end{cases}" title="&#92;displaystyle A_{u,v} = &#92;begin{cases} 1 &amp; &#92;mbox{if } uv &#92;in E &#92;&#92; 0 &amp; &#92;mbox{otherwise} &#92;end{cases}" class="latex" /></p>
<p style="text-align:justify;">To define the <img src="https://s0.wp.com/latex.php?latex=m+%5Ctimes+n&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="m &#92;times n" title="m &#92;times n" class="latex" /> incidence matrix <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;nabla" title="&#92;nabla" class="latex" />, we need to fix an orientation of the edges (for our purpose an arbitrary orientation suffices), then:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cnabla_%7Be%2Cv%7D+%3D+%5Cbegin%7Bcases%7D+-1+%26+%5Ctext%7Bif+%7D+v+%5Ctext%7B+is+initial+vertex+of+edge+%7D+e+%5C%5C+%2B1+%26+%5Ctext%7Bif+%7D+v+%5Ctext%7B+is+terminal+vertex+of+edge+%7D+e+%5C%5C+0+%26+%5Ctext%7Bif+%7D+e+%5Ctext%7B+is+not+incident+on+vertex+%7D+v+%5Cend%7Bcases%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;nabla_{e,v} = &#92;begin{cases} -1 &amp; &#92;text{if } v &#92;text{ is initial vertex of edge } e &#92;&#92; +1 &amp; &#92;text{if } v &#92;text{ is terminal vertex of edge } e &#92;&#92; 0 &amp; &#92;text{if } e &#92;text{ is not incident on vertex } v &#92;end{cases}" title="&#92;displaystyle &#92;nabla_{e,v} = &#92;begin{cases} -1 &amp; &#92;text{if } v &#92;text{ is initial vertex of edge } e &#92;&#92; +1 &amp; &#92;text{if } v &#92;text{ is terminal vertex of edge } e &#92;&#92; 0 &amp; &#92;text{if } e &#92;text{ is not incident on vertex } v &#92;end{cases}" class="latex" /></p>
<p style="text-align:justify;">Now suppose we have a real valued function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="V" title="V" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=f%3A+V+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: V &#92;to &#92;mathbb{R}" title="f: V &#92;to &#92;mathbb{R}" class="latex" />. For <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G" title="G" class="latex" />, we view this as a column vector indexed by the vertices.</p>
<p style="text-align:justify;">Similarly let <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="g" title="g" class="latex" /> be a real valued function over the edges <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="E" title="E" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=g%3A+E+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="g: E &#92;to &#92;mathbb{R}" title="g: E &#92;to &#92;mathbb{R}" class="latex" /> (again, we view this as a row vector indexed by the edges).</p>
<p style="text-align:justify;">Now, consider the following map: <img src="https://s0.wp.com/latex.php?latex=f+%5Cmapsto+%5Cnabla+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f &#92;mapsto &#92;nabla f " title="f &#92;mapsto &#92;nabla f " class="latex" /> (which is now a vector indexed by <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="E" title="E" class="latex" />). The value of this map at any edge <img src="https://s0.wp.com/latex.php?latex=%28%5Cnabla+f%29+%28e%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(&#92;nabla f) (e)" title="(&#92;nabla f) (e)" class="latex" /> is simply the difference of the values of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> at the two end points of the edge <img src="https://s0.wp.com/latex.php?latex=e+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="e " title="e " class="latex" /> i.e. <img src="https://s0.wp.com/latex.php?latex=f_u+-+f_v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f_u - f_v" title="f_u - f_v" class="latex" />. This makes it somewhat intuitive that the matrix <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;nabla" title="&#92;nabla" class="latex" /> is some sort of a difference operator on <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G" title="G" class="latex" />. Given this &#8220;difference operator&#8221; or &#8220;discrete differential&#8221; view, <img src="https://s0.wp.com/latex.php?latex=%28%5Cnabla+f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(&#92;nabla f)" title="(&#92;nabla f)" class="latex" /> can then be viewed as the <em><strong>gradient </strong></em>that measures the change of the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> along the edges of <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G" title="G" class="latex" />. Thus the map <img src="https://s0.wp.com/latex.php?latex=f+%5Cmapsto+%5Cnabla+f+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f &#92;mapsto &#92;nabla f " title="f &#92;mapsto &#92;nabla f " class="latex" /> is just the <em><strong>gradient operator.</strong></em></p>
<p style="text-align:justify;">Like the above, now we consider the following map: <img src="https://s0.wp.com/latex.php?latex=g+%5Cmapsto+%5Cnabla%5E%7BT%7D+g%5ET+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="g &#92;mapsto &#92;nabla^{T} g^T " title="g &#92;mapsto &#92;nabla^{T} g^T " class="latex" /> (which is now a vector indexed by <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="V" title="V" class="latex" />). Recall that <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="g" title="g" class="latex" /> was defined on the edges. The value of this map at any vertex <img src="https://s0.wp.com/latex.php?latex=%28g+%5Cnabla%29%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(g &#92;nabla)(v)" title="(g &#92;nabla)(v)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Be+%5Ctext%7B+exits+%7D+v%7D+g_e+-+%5Csum_%7Be+%5Ctext%7B+enters+%7D+v%7D+g_e&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;sum_{e &#92;text{ exits } v} g_e - &#92;sum_{e &#92;text{ enters } v} g_e" title="&#92;displaystyle &#92;sum_{e &#92;text{ exits } v} g_e - &#92;sum_{e &#92;text{ enters } v} g_e" class="latex" />. If we were to view <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="g" title="g" class="latex" /> as describing some kind of a flow on the edges, then <img src="https://s0.wp.com/latex.php?latex=%28g+%5Cnabla%29%28v%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(g &#92;nabla)(v)" title="(g &#92;nabla)(v)" class="latex" /> is the <em>net outbound flow</em> on the vertex <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="v" title="v" class="latex" />, which is just the <em><strong>divergence</strong></em>. Thus the map <img src="https://s0.wp.com/latex.php?latex=f+%5Cmapsto+g+%5Cnabla+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f &#92;mapsto g &#92;nabla " title="f &#92;mapsto g &#92;nabla " class="latex" /> is just the <em><strong>divergence operator</strong></em>.</p>
<p style="text-align:justify;">Now consider <img src="https://s0.wp.com/latex.php?latex=f+%5Cmapsto+%5Cnabla%5E%7BT%7D+%5Cnabla+f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f &#92;mapsto &#92;nabla^{T} &#92;nabla f" title="f &#92;mapsto &#92;nabla^{T} &#92;nabla f" class="latex" /> (note that this makes sense since <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;nabla f" title="&#92;nabla f" class="latex" /> is a column vector), going by the above, this should be the <em><strong>divergence of the gradient</strong></em>. Thus, the analogy with the &#8220;real&#8221; Laplacian makes sense and the matrix <img src="https://s0.wp.com/latex.php?latex=L_G+%3D+%5Cnabla%5E%7BT%7D+%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L_G = &#92;nabla^{T} &#92;nabla" title="L_G = &#92;nabla^{T} &#92;nabla" class="latex" /> is appropriately called the Graph Laplacian.</p>
<p style="text-align:justify;">Using the definition of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;nabla" title="&#92;nabla" class="latex" />, a simple computation yields the following for <img src="https://s0.wp.com/latex.php?latex=L_G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L_G" title="L_G" class="latex" />:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_%7BG%7Bu%2Cv%7D%7D+%3D+%5Cbegin%7Bcases%7D+-1+%26+%5Cmbox%7Bif+%7D+uv+%5Cin+E+%5C%5C+deg%28u%29+%26+%5Cmbox%7Bif+%7D+u+%3Dv+%5Cend%7Bcases%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle L_{G{u,v}} = &#92;begin{cases} -1 &amp; &#92;mbox{if } uv &#92;in E &#92;&#92; deg(u) &amp; &#92;mbox{if } u =v &#92;end{cases}" title="&#92;displaystyle L_{G{u,v}} = &#92;begin{cases} -1 &amp; &#92;mbox{if } uv &#92;in E &#92;&#92; deg(u) &amp; &#92;mbox{if } u =v &#92;end{cases}" class="latex" /></p>
<p style="text-align:justify;">Looking at the above and recalling the definition for the adjacency matrix <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="A" title="A" class="latex" />, it is easy to see that <img src="https://s0.wp.com/latex.php?latex=L_G+%3D+D+-+A&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L_G = D - A" title="L_G = D - A" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D" title="D" class="latex" /> is the diagonal matrix with <img src="https://s0.wp.com/latex.php?latex=D_%7Bu%2Cu%7D+%3D+deg%28u%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D_{u,u} = deg(u)" title="D_{u,u} = deg(u)" class="latex" />. This is just the familiar definition that I mentioned at the start.</p>
<p style="text-align:justify;">PS: From the above it is also clear that the Laplacian is positive semidefinite. Indeed, consider <img src="https://s0.wp.com/latex.php?latex=f+%5ETL+f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f ^TL f" title="f ^TL f" class="latex" />, which is written as <img src="https://s0.wp.com/latex.php?latex=f%5ET+%5Cnabla%5ET+%5Cnabla+f+%3D%5C%7C%5Cnabla+f%5C%7C%5E2+%3D+%5Csum_%7B%28u%2Cv%29+%5Cin+E%7D+%28f_u+-+f_v%29%5E2+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f^T &#92;nabla^T &#92;nabla f =&#92;|&#92;nabla f&#92;|^2 = &#92;sum_{(u,v) &#92;in E} (f_u - f_v)^2 " title="f^T &#92;nabla^T &#92;nabla f =&#92;|&#92;nabla f&#92;|^2 = &#92;sum_{(u,v) &#92;in E} (f_u - f_v)^2 " class="latex" /></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5905/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5905/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5905&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2015/03/29/a-note-on-the-graph-laplacian/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>From Eros to Gaia</title>
		<link>https://onionesquereality.wordpress.com/2014/12/22/from-eros-to-gaia/</link>
		<comments>https://onionesquereality.wordpress.com/2014/12/22/from-eros-to-gaia/#respond</comments>
		<pubDate>Mon, 22 Dec 2014 10:31:37 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Books]]></category>
		<category><![CDATA[Quotes]]></category>
		<category><![CDATA[Freeman Dyson]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5898</guid>
		<description><![CDATA[I had been re-reading &#8220;From Eros to Gaia&#8221; by Freeman Dyson after some years. I have a bad habit of never reading prefaces to books, however I am glad I read it this time around because of this sobering passage that appears in it: My mother used to say that life begins at forty. That [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5898&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">I had been re-reading &#8220;From Eros to Gaia&#8221; by Freeman Dyson after some years. I have a bad habit of never reading prefaces to books, however I am glad I read it this time around because of this sobering passage that appears in it:</p>
<blockquote>
<p style="text-align:justify;">My mother used to say that life begins at forty. That was her age when she had her first baby. I say, on the contrary, that life begins at fifty-five, the age when I published my first book. <em>So long as you have courage and a sense of humour, it is never too late to start life afresh</em>. A book is in many ways like a baby. While you are writing, it is curled up in your belly. You cannot get a clear view of it. As soon as it is born, it goes out into the world and develops a character of its own. Like a daughter coming home from school, it surprises you with unexpected flashes of wisdom. The same thing happens with scientific theories. You sit quietly gestating them, for nine months or whatever the required time may be, and then one day they are out on their own, not belonging to you anymore but to the whole community of scientists. Whatever it is that you produce&#8211; a baby, a book, or a theory&#8211; it is a piece of the magic of creation. You are producing something that you do not fully understand. As you watch it grow, it becomes part of a larger world, and fits itself into a larger design than you imagined. You belong to the company of those medieval craftsmen who added a carved stone here or a piece of scaffolding there, and together built Chartres Cathedral.</p>
</blockquote>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/12/dyson1.jpg"><img class="aligncenter wp-image-5900 size-full" src="https://onionesquereality.files.wordpress.com/2014/12/dyson1.jpg?w=500" alt="Dyson"   /></a></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5898/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5898/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5898&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/12/22/from-eros-to-gaia/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/12/dyson1.jpg" medium="image">
			<media:title type="html">Dyson</media:title>
		</media:content>
	</item>
		<item>
		<title>Neighbourhood Gerrymandering: An Approach to Discriminative Metric Learning via Latent Structured Prediction</title>
		<link>https://onionesquereality.wordpress.com/2014/10/24/neighbourhood-gerrymandering-an-approach-to-discriminative-metric-learning-via-latent-structured-prediction/</link>
		<comments>https://onionesquereality.wordpress.com/2014/10/24/neighbourhood-gerrymandering-an-approach-to-discriminative-metric-learning-via-latent-structured-prediction/#comments</comments>
		<pubDate>Fri, 24 Oct 2014 21:51:06 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[k Nearest Neighbors]]></category>
		<category><![CDATA[Metric Learning]]></category>
		<category><![CDATA[Similarity Learning]]></category>
		<category><![CDATA[Structured Prediction]]></category>
		<category><![CDATA[Structured Support Vector Machines]]></category>
		<category><![CDATA[Support Vector Machines]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5805</guid>
		<description><![CDATA[An informal summary of a recent project I had some involvement in. Motivation: Why care about Metric Learning? In many machine learning algorithms, such as k-means, Support Vector Machines, k-Nearest Neighbour based classification, kernel regression, methods based on Gaussian Processes etc etc &#8211; there is a fundamental reliance, that is to be able to measure [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5805&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;"><span style="color:#800000;"><em>An informal summary of a recent project I had some involvement in.</em> </span></p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Motivation: Why care about Metric Learning?</strong></span></p>
<p style="text-align:justify;">In many machine learning algorithms, such as k-means, Support Vector Machines, k-Nearest Neighbour based classification, kernel regression, methods based on Gaussian Processes etc etc &#8211; there is a fundamental reliance, that is to be able to measure <strong>dissimilarity</strong> between two examples. Usually this is done by using the Euclidean distance between points (i.e. points that are closer in this sense are considered more similar), which is usually suboptimal in the sense that will be explained below. Being able to compare examples and decide if they are similar or dissimilar or return a measure of similarity is one of the most fundamental problems in machine learning. Ofcourse a related question is: What does mean by &#8220;similar&#8221; afterall?</p>
<p style="text-align:justify;">To illustrate the above let us work with <em>k-Nearest Neighbour</em> classification. Before starting, let us just illustrate the really simple idea (of kNN classification) by an example: Consider the following points in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{R}^2" title="&#92;mathbb{R}^2" class="latex" />, with the classes marked by different colours.</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/2dpoints.jpg"><img class="aligncenter wp-image-5812 size-medium" src="https://onionesquereality.files.wordpress.com/2014/10/2dpoints.jpg?w=300&#038;h=146" alt="2DPoints" width="300" height="146" /><br />
</a></p>
<p style="text-align:justify;">Now suppose we have a new point &#8211; marked with black &#8211; whose class is unknown. We assign it a class by looking at the nearest neighbors and taking the majority vote:</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/knn.jpg"><img class="aligncenter size-medium wp-image-5818" src="https://onionesquereality.files.wordpress.com/2014/10/knn.jpg?w=249&#038;h=300" alt="kNN" width="249" height="300" /></a></p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Some notes on kNN: </strong></span></p>
<p style="text-align:justify;">A brief digression first before moving on the problem in the above (what is nearest?). kNN classifiers are very simple and yet in many cases they can give excellent performance. For example, consider the performance on the MNIST dataset, it is clear that kNN can give competitive performance as compared to other more complicated models.</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/mnist.jpg"><img class="aligncenter wp-image-5820 size-medium" src="https://onionesquereality.files.wordpress.com/2014/10/mnist.jpg?w=300&#038;h=214" alt="MNIST" width="300" height="214" /></a></p>
<p style="text-align:justify;">Moreover, they are simple to implement, use local information and hence are inherently nonlinear. The biggest advantage in my opinion is that it is easy to add new classes (since no retraining from scratch is required) and since we average across points, kNN is also relatively robust to label noise. It also has some attractive theoretical properties: for example kNN is universally consistent (as the number of points approaches infinity, with appropriate choice of k, the kNN error will approach the Bayes Risk).</p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Notion of &#8220;Nearest&#8221;:</strong></span></p>
<p style="text-align:justify;">At the same time, kNN classifiers also have their disadvantages. One is related to the notion of &#8220;nearest&#8221; (which falls back on what was talked about at the start) i.e. how does one decide what points are &#8220;nearest&#8221;. Usually such points are decided on the basis of the Euclidean distance on the native feature space which usually has shortfalls. Why? Because nearness in the Euclidean space may not correspond to nearness in the label space i.e. points that might be far off in the Euclidean space may have similar labels. In such cases, clearly the notion of &#8220;near&#8221; using the euclidean distance is suboptimal. This is illustrated by a set of figures below (adapted from slides by Kilian Weinberger):</p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>An Illustration:</strong></span></p>
<p style="text-align:justify;">Consider the image of this lady &#8211; now how do we decide what is more similar to it?</p>
<p style="text-align:justify;"> <a href="https://onionesquereality.files.wordpress.com/2014/10/lady-who.jpg"><img class="aligncenter size-medium wp-image-5829" src="https://onionesquereality.files.wordpress.com/2014/10/lady-who.jpg?w=300&#038;h=243" alt="Lady-Who" width="300" height="243" /></a></p>
<p style="text-align:justify;">Someone might mean similar on the basis of the gender:</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/lady-gender.jpg"><img class="aligncenter size-medium wp-image-5831" src="https://onionesquereality.files.wordpress.com/2014/10/lady-gender.jpg?w=300&#038;h=222" alt="Lady-Gender" width="300" height="222" /></a>Or on the basis of age:</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/lady-age.jpg"><img class="aligncenter size-medium wp-image-5832" src="https://onionesquereality.files.wordpress.com/2014/10/lady-age.jpg?w=300&#038;h=222" alt="Lady-Age" width="300" height="222" /></a></p>
<p style="text-align:justify;">Or on the basis of the hairstyle!</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/lady-hair.jpg"><img class="aligncenter size-medium wp-image-5833" src="https://onionesquereality.files.wordpress.com/2014/10/lady-hair.jpg?w=300&#038;h=221" alt="Lady-Hair" width="300" height="221" /></a></p>
<p style="text-align:justify;">Similarity depends on the context! Something that the euclidean distance in the <em>native feature space</em> would fail to capture. This context is provided by labels.</p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Distance Metric Learning:</strong></span></p>
<p style="text-align:justify;">The goal of Metric Learning is to <em>learn</em> a distance metric, so that the above label information is incorporated in the notion of distance i.e. points that are semantically similar are now closer in the new space. The idea is to take the original or native feature space, use the label information and then amplify directions that are more informative and squish directions that are not. This is illustrated in this figure &#8211; notice that the point marked in black would be incorrectly classified in the native feature space, however under the learnt metric it would be correctly classified.</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/metriclearningamp.jpg"><img class="aligncenter size-medium wp-image-5836" src="https://onionesquereality.files.wordpress.com/2014/10/metriclearningamp.jpg?w=300&#038;h=231" alt="MetricLearningAmp" width="300" height="231" /></a></p>
<p style="text-align:justify;">It is worthwhile to have a brief look at what this means. The Euclidean distance (with <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x_i &#92;in &#92;mathbb{R}^d" title="x_i &#92;in &#92;mathbb{R}^d" class="latex" />) is defined by</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%28x_i+-+x_j%29%5ET+%28x_i+-+x_j%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sqrt{(x_i - x_j)^T (x_i - x_j)}" title="&#92;sqrt{(x_i - x_j)^T (x_i - x_j)}" class="latex" /></p>
<p style="text-align:justify;">as also was evident in the above figure, this corresponds to the following euclidean ball in 2-D</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/eucball.jpg"><img class="aligncenter wp-image-5838" src="https://onionesquereality.files.wordpress.com/2014/10/eucball.jpg?w=150&#038;h=153" alt="EucBall" width="150" height="153" /></a></p>
<p style="text-align:justify;">A family of distance measure may be defined using an inner product matrix. These are called the Mahalanobis metrics.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%28x_i+-+x_j%29%5ET+%5Cmathbf%7BW%7D%28x_i+-+x_j%29%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sqrt{(x_i - x_j)^T &#92;mathbf{W}(x_i - x_j)} " title="&#92;sqrt{(x_i - x_j)^T &#92;mathbf{W}(x_i - x_j)} " class="latex" /></p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/mahal-ball.jpg"><img class="aligncenter wp-image-5841" src="https://onionesquereality.files.wordpress.com/2014/10/mahal-ball.jpg?w=160&#038;h=162" alt="Mahal-Ball" width="160" height="162" /></a></p>
<p style="text-align:justify;">The learnt metric affects a rescaling and rotation of the original space. The goal is now to <em>learn</em> this <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+%5Csucceq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W} &#92;succeq 0" title="&#92;mathbf{W} &#92;succeq 0" class="latex" /> using the label information so that the new distances correspond better to the semantic context. It is easy to see that when <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+%5Csucceq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W} &#92;succeq 0" title="&#92;mathbf{W} &#92;succeq 0" class="latex" />, the above is still a <em>distance metric.</em></p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Learning <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W}" title="&#92;mathbf{W}" class="latex" />:</strong></span></p>
<p style="text-align:justify;">Usually the real motivation for metric learning is to optimize for the kNN objective i.e. learn the matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+%5Csucceq+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W} &#92;succeq 0 " title="&#92;mathbf{W} &#92;succeq 0 " class="latex" /> so that the kNN error is reduced. But note that directly optimizing for the kNN loss is intractable because of the combinatorial nature of the optimization (we&#8217;ll see this in a bit), so instead, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W}" title="&#92;mathbf{W}" class="latex" /> is learnt as follows:</p>
<p style="text-align:justify;">1. Define a set of &#8220;good&#8221; neighbors for each point. The definition of &#8220;good&#8221; is usually some combination of proximity to the query point and label agreement between the points.</p>
<p style="text-align:justify;">2. Define a set of &#8220;bad&#8221; neighbours for each point. This might be a set of points that are &#8220;close&#8221; to the query point but disagree on the label (i.e. inspite of being close to the query point they might give a wrong classification if they were chosen to classify the query point).</p>
<p style="text-align:justify;">3. Set up the optimization problem for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W}" title="&#92;mathbf{W}" class="latex" /> such that for each query point, &#8220;good&#8221; neighbours are pulled closer to it while &#8220;bad&#8221; neighbours are pushed farther away, and thus learn <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W}" title="&#92;mathbf{W}" class="latex" /> so as to minimize the leave one out kNN error.</p>
<p style="text-align:justify;">The exact formulation of &#8220;good&#8221; and &#8220;bad&#8221; varies from method to method. Here are some examples:</p>
<p style="text-align:justify;">In one of the earliest papers on distance metric learning by <a href="http://www.cs.cmu.edu/~epxing/papers/Old_papers/xing_nips02_metric.pdf" target="_blank">Xing, Ng, Jordan and Russell</a> (2002) &#8211; good neighbors are similarly labeled <em>k</em> points. The optimization is done so that each class is mapped into a ball of fixed radius. However no separation is enforced between the classes. This is illustrated in the following figure (the query point is marked with an X, similarly labeled k points are moved into a ball of a fixed radius):</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/xingngjordan.jpg"><img class="aligncenter size-medium wp-image-5853" src="https://onionesquereality.files.wordpress.com/2014/10/xingngjordan.jpg?w=300&#038;h=240" alt="XingNgJordan" width="300" height="240" /></a></p>
<p style="text-align:justify;">One problem with the above is that the kNN objective does not really require that similarly labeled points are clustered together, hence in a way it optimizes for a harder objective. This is remedied by the LMNN described briefly below.</p>
<p style="text-align:justify;">One of the more famous Metric Learning papers is the <a href="http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf" target="_blank">Large Margin Nearest Neighbors</a> by Weinberger and Saul (2006). Here good neighbors are similarly labeled <em>k</em> points (and the circle around x is the distance of the farthest of the good neighbours) and &#8220;worst offenders&#8221; or &#8220;bad&#8221; neighbours are points that are of a different class but still in the nearest neighbors of the query point. The optimization is basically a semidefinite program that works to pull the good neighbours towards the query point and a margin is enforced by pushing the offending points out of this circle. Thus in a way, the goal in LMNN is to deform the metric in such a way that the neighbourhood for each point is &#8220;pure.</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/lmnn.jpg"><img class="aligncenter size-medium wp-image-5854" src="https://onionesquereality.files.wordpress.com/2014/10/lmnn.jpg?w=300&#038;h=241" alt="LMNN" width="300" height="241" /></a></p>
<p style="text-align:justify;">There are many approaches to the metric learning problem, however a few more notable ones are:</p>
<p style="text-align:justify;">1. <a href="http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf" target="_blank">Neighbourhood Components Analysis</a> (Goldberger, Roweis, Hinton and Salakhutdinov, 2004): Here the piecewise constant error of the kNN rule is replaced by a soft version. This leads to a non-convex objective that can be optimized by gradient descent. Basically, NCA tries to optimize for the choice  of neighbour at the price of losing convexity.</p>
<p style="text-align:justify;">2. <a href="https://cs.nyu.edu/~roweis/papers/mcml.pdf" target="_blank">Collapsing Classes</a> (Globerson and Roweis, 2006): This method attempts to remedy the non-convexity above by optimizing a similar stochastic rule while attempting to collapse each class to one point, making the problem convex.</p>
<p style="text-align:justify;">3. <a href="http://www.icml2010.org/papers/504.pdf" target="_blank">Metric Learning to Rank</a> (McFee and Lankriet, 2010): This paper takes a different take on metric learning, treating it as a ranking problem. Note that given a fixed p.s.d matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{W}" title="&#92;mathbf{W}" class="latex" /> a query point induces a permutation on the training set (in order of increasing distance). The idea thus is to optimize the metric for some ranking measure (such as precision@k). But note that this is not necessarily the same as requiring correct classification.</p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Neighbourhood Gerrymandering:</strong></span></p>
<p style="text-align:justify;">As a motivation we can look at the cartoon above for LMNN. Since we are looking to optimize for the kNN objective, the requirement to learn the metric should just be correct classification. Thus, we should need to push the points to ensure the same. Thus we can have the circle around x as simply the distance of the farthest point in the k nearest neighbours (irrespective of class). Now, we would like to deform the metric such that enough points are pulled in and pushed out of this circle so as to ensure correct classification. This is illustrated below.</p>
<p style="text-align:justify;"><a href="https://onionesquereality.files.wordpress.com/2014/10/mlng.jpg"><img class="aligncenter size-medium wp-image-5861" src="https://onionesquereality.files.wordpress.com/2014/10/mlng.jpg?w=300&#038;h=246" alt="MLNG" width="300" height="246" /></a></p>
<p style="text-align:justify;">This method is akin to the common practice of <em>Gerrymandering</em>, in drawing up borders of election districts so as to provide advantages to desired political parties. This is done by concentrating voters from a particular party and/or by spreading out voters from other parties. In the above, the &#8220;districts&#8221; are cells in the Voronoi diagram defined by the Mahalanobis metric and &#8220;parties&#8221; are class labels voted for by each neighbour.</p>
<p style="text-align:justify;"> <span style="text-decoration:underline;"><strong>Motivations and Intuition:</strong></span></p>
<p style="text-align:justify;">Now we can step back a little from the survey above, and think a bit about the kNN problem in somewhat more precise terms so that the above approach can be motivated better.</p>
<p style="text-align:justify;">For kNN, given a query point and a fixed metric, there is an implicit <strong><em>latent variable</em></strong>: The choice of the k &#8220;neighbours&#8221;.</p>
<p style="text-align:justify;">Given this latent variable &#8211; inference of the label for the query point is trivial &#8211; since it is just the majority vote. But notice that for any given query point, there can exist a very large number of  choices of k points that may correspond to correct classification (basically any set of points with majority of correct class will work). Now we basically want to learn a metric so that we prefer one of <em>these</em> sets over any set of k neighbours which would vote for a wrong class. In particular, from the sets that affects correct classification we would like to pick the set that is on average most similar to the query point.</p>
<p style="text-align:justify;">We can write kNN prediction as an <em><strong>inference</strong></em> problem with a <em><strong>structured latent variable</strong></em><strong> </strong>being the choice of k neighbours.</p>
<p style="text-align:justify;">The learning then corresponds to  minimizing a sum of structured latent hinge loss and a regularizer. Computing the latent hinge loss involves <em><strong>loss-augmented inference</strong> &#8211;</em> which is basically looking for the <em>worst offending k points</em> (points that have high average similarity with the query point, yet correspond to a high loss). Given the combinatorial nature of the problem, efficient inference and loss-augmented inference is key. Optimization can basically be just gradient descent on the surrograte loss. To make this a bit more clear, the setup is described below:</p>
<p style="text-align:justify;"><span style="text-decoration:underline;"><strong>Problem Setup:</strong></span></p>
<p style="text-align:justify;">Suppose we are given <img src="https://s0.wp.com/latex.php?latex=N&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="N" title="N" class="latex" /> training examples that are represented by a &#8220;native&#8221; feature map, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D+%5C%7Bx_1%2C+%5Cdots%2C+x_N%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{X} = &#92;{x_1, &#92;dots, x_N&#92;}" title="&#92;mathbf{X} = &#92;{x_1, &#92;dots, x_N&#92;}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x_i &#92;in &#92;mathbb{R}^d" title="x_i &#92;in &#92;mathbb{R}^d" class="latex" /> with class labels <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D+%3D+%5By_1%2C+%5Cdots%2C+y_N%5D%5ET&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{y} = [y_1, &#92;dots, y_N]^T" title="&#92;mathbf{y} = [y_1, &#92;dots, y_N]^T" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=y_i+%5Cin+%5B%5Cmathbf%7BR%7D%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="y_i &#92;in [&#92;mathbf{R}]" title="y_i &#92;in [&#92;mathbf{R}]" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5B%5Cmathbf%7BR%7D%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="[&#92;mathbf{R}]" title="[&#92;mathbf{R}]" class="latex" /> stands for the set <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+%5Cmathbf%7BR%7D%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;{1, &#92;dots, &#92;mathbf{R}&#92;}" title="&#92;{1, &#92;dots, &#92;mathbf{R}&#92;}" class="latex" />.</p>
<p style="text-align:justify;">Suppose are also provided with a loss matrix <img src="https://s0.wp.com/latex.php?latex=%5CLambda&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Lambda" title="&#92;Lambda" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5CLambda%28r%2Cr%27%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Lambda(r,r&#039;)" title="&#92;Lambda(r,r&#039;)" class="latex" /> being the loss incurred by predicting <img src="https://s0.wp.com/latex.php?latex=r%27&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="r&#039;" title="r&#039;" class="latex" /> when the correct class is <img src="https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="r" title="r" class="latex" />. We assume that <img src="https://s0.wp.com/latex.php?latex=%5CLambda%28r%2Cr%29+%3D+0+&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Lambda(r,r) = 0 " title="&#92;Lambda(r,r) = 0 " class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%28r%2Cr%27%29%2C+%5CLambda%28r%2Cr%27%29+%5Cgeq+0&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;forall (r,r&#039;), &#92;Lambda(r,r&#039;) &#92;geq 0" title="&#92;forall (r,r&#039;), &#92;Lambda(r,r&#039;) &#92;geq 0" class="latex" />.</p>
<p style="text-align:justify;">Now let <img src="https://s0.wp.com/latex.php?latex=h+%5Csubset+%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h &#92;subset &#92;mathbf{X}" title="h &#92;subset &#92;mathbf{X}" class="latex" /> be a set of examples in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{X}" title="&#92;mathbf{X}" class="latex" />.</p>
<p style="text-align:justify;">As stated earlier, we are interested in the <em>Mahalanobis metrics:</em></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=D_W%28x%2Cx_i%29+%3D+%28x-x_i%29%5ET+W+%28x-x_i%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D_W(x,x_i) = (x-x_i)^T W (x-x_i)" title="D_W(x,x_i) = (x-x_i)^T W (x-x_i)" class="latex" /></p>
<p style="text-align:justify;">For a fixed <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="W" title="W" class="latex" /> we may define the distance of <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h" title="h" class="latex" /> with respect to a point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> as:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_W%28x%2Ch%29+-+%5Csum_%7Bx_j+%5Cin+h%7D+D_W%28x%2C+x_j%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle S_W(x,h) - &#92;sum_{x_j &#92;in h} D_W(x, x_j)" title="&#92;displaystyle S_W(x,h) - &#92;sum_{x_j &#92;in h} D_W(x, x_j)" class="latex" /></p>
<p style="text-align:justify;">Therefore, the set of k-Nearest Neighbours of <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{X}" title="&#92;mathbf{X}" class="latex" /> is:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=h_W%28x+%29+%3D+%5Carg%5Cmax_%7B%7Ch%7C%3Dk%7D+S_W%28x%2Ch%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h_W(x ) = &#92;arg&#92;max_{|h|=k} S_W(x,h)" title="h_W(x ) = &#92;arg&#92;max_{|h|=k} S_W(x,h)" class="latex" /></p>
<p style="text-align:justify;">For any set <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h" title="h" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" /> examples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{X}" title="&#92;mathbf{X}" class="latex" /> we can predict the label of <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> by a simple majority vote.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Chat%7By%7D%28h%29+%3D+majority%5C%7By_j%3A+x_j+%5Cin+h%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{y}(h) = majority&#92;{y_j: x_j &#92;in h&#92;}" title="&#92;hat{y}(h) = majority&#92;{y_j: x_j &#92;in h&#92;}" class="latex" /></p>
<p style="text-align:justify;">The kNN classifier therefore predicts <img src="https://s0.wp.com/latex.php?latex=%5Chat%7By%7D%28h_W%28x%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{y}(h_W(x))" title="&#92;hat{y}(h_W(x))" class="latex" />.</p>
<p style="text-align:justify;">Thus, the classification loss incurred using the set <img src="https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h" title="h" class="latex" /> can be defined as:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5CDelta%28y%2Ch%29+%3D+%5CLambda%28y%2C%5Chat%7By%7D%28h%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Delta(y,h) = &#92;Lambda(y,&#92;hat{y}(h))" title="&#92;Delta(y,h) = &#92;Lambda(y,&#92;hat{y}(h))" class="latex" /></p>
<p style="text-align:justify;">Learning and Inference:</p>
<p style="text-align:justify;">One might want to learn <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="W" title="W" class="latex" /> so as to minimize the training loss:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_i+%5CDelta%28y_i%2C+h_W%28x_i%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;sum_i &#92;Delta(y_i, h_W(x_i))" title="&#92;displaystyle &#92;sum_i &#92;Delta(y_i, h_W(x_i))" class="latex" /></p>
<p style="text-align:justify;">However as mentioned in passing above, this fails because of the intractable nature of  the classification loss <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;Delta" title="&#92;Delta" class="latex" />. Thus we&#8217;d have to resort to the usual remedy: define a tractable surrograte loss.</p>
<p style="text-align:justify;">It must be stressed again that the output of prediction is a structured object <img src="https://s0.wp.com/latex.php?latex=h_W&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="h_W" title="h_W" class="latex" />. The loss in structured prediction penalizes the gap between score of the correct structured output and the score of the &#8220;worst offending&#8221; incorrect output. This leads to the following definition of the surrogate:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=L%28x%2Cy%2CW%29+%3D+%5Cmax_h+%5BS_W%28x%2Ch%29+%2B+%5CDelta%28y%2Ch%29%5D+-+%5Cmax_%7Bh%3A+%5CDelta%28y%2Ch%29+%3D+0%7D+S_W%28x%2Ch%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L(x,y,W) = &#92;max_h [S_W(x,h) + &#92;Delta(y,h)] - &#92;max_{h: &#92;Delta(y,h) = 0} S_W(x,h)" title="L(x,y,W) = &#92;max_h [S_W(x,h) + &#92;Delta(y,h)] - &#92;max_{h: &#92;Delta(y,h) = 0} S_W(x,h)" class="latex" /></p>
<p style="text-align:justify;">This corresponds to our earlier intuition on wanting to learn <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="W" title="W" class="latex" /> such that the gap between the &#8220;good neighbours&#8221; and &#8220;worst offenders&#8221; is increased.</p>
<p style="text-align:justify;">So, although the loss above was arrived at by intuitive arguments, it turns out that our problem is an instance of a familiar type of problem: Latent Structured Prediction and hence the machinery for optimization there can be used here as well. The objective for us corresponds to:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_W+%5C%7C+W%5C%7C%5E2_%7BF%7D+%2B+C+%5Csum_i+%28L%28x_i%2C+y_i%2CW%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;min_W &#92;| W&#92;|^2_{F} + C &#92;sum_i (L(x_i, y_i,W))" title="&#92;displaystyle &#92;min_W &#92;| W&#92;|^2_{F} + C &#92;sum_i (L(x_i, y_i,W))" class="latex" /></p>
<p style="text-align:justify;">Where <img src="https://s0.wp.com/latex.php?latex=%5C%7C+%5Ccdot+%5C%7C_F&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;| &#92;cdot &#92;|_F" title="&#92;| &#92;cdot &#92;|_F" class="latex" /> is the Frobenius norm.</p>
<p style="text-align:justify;">Note that the regularizer is convex, but the loss is not convex to the subtraction of the max term i.e. now it is a difference of convex functions which means the concave convex procedure may be used for optimization (although we just use stochastic gradient descent). Also note that the optimization at each step needs an efficient subroutine to determine the correct structured output (inference of the best set of neighbours) and the worst offending incorrect structured output (loss augmented inference i.e. finding the worst set of neighbors). Turns out that for this problem this is possible (although not presented here).</p>
<p style="text-align:justify;">It is interesting to think about how this approach extends to regression and to see how it works when the embeddings learnt are not linear.</p>
<p style="text-align:justify;"><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5805/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5805/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5805&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/10/24/neighbourhood-gerrymandering-an-approach-to-discriminative-metric-learning-via-latent-structured-prediction/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/2dpoints.jpg?w=300" medium="image">
			<media:title type="html">2DPoints</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/knn.jpg?w=249" medium="image">
			<media:title type="html">kNN</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/mnist.jpg?w=300" medium="image">
			<media:title type="html">MNIST</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/lady-who.jpg?w=300" medium="image">
			<media:title type="html">Lady-Who</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/lady-gender.jpg?w=300" medium="image">
			<media:title type="html">Lady-Gender</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/lady-age.jpg?w=300" medium="image">
			<media:title type="html">Lady-Age</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/lady-hair.jpg?w=300" medium="image">
			<media:title type="html">Lady-Hair</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/metriclearningamp.jpg?w=300" medium="image">
			<media:title type="html">MetricLearningAmp</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/eucball.jpg" medium="image">
			<media:title type="html">EucBall</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/mahal-ball.jpg" medium="image">
			<media:title type="html">Mahal-Ball</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/xingngjordan.jpg?w=300" medium="image">
			<media:title type="html">XingNgJordan</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/lmnn.jpg?w=300" medium="image">
			<media:title type="html">LMNN</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/10/mlng.jpg?w=300" medium="image">
			<media:title type="html">MLNG</media:title>
		</media:content>
	</item>
		<item>
		<title>The Jacobian Inner Product</title>
		<link>https://onionesquereality.wordpress.com/2014/08/20/the-jacobian-inner-product/</link>
		<comments>https://onionesquereality.wordpress.com/2014/08/20/the-jacobian-inner-product/#comments</comments>
		<pubDate>Wed, 20 Aug 2014 03:20:50 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Classification]]></category>
		<category><![CDATA[Dimensionality Reduction]]></category>
		<category><![CDATA[Jacobian]]></category>
		<category><![CDATA[Manifold Learning]]></category>
		<category><![CDATA[Regression]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[Supervised Learning]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5753</guid>
		<description><![CDATA[This post may be considered an extension of the previous post. The setup and notation is the same as in the previous post (linked above). But to summarize: Earlier we had an unknown smooth regression function . The idea was to estimate at each training point, the gradient of this unknown function , and then [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5753&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;"><em>This post may be considered an extension of the <a href="https://onionesquereality.wordpress.com/2014/08/16/the-gradient-outer-product/" target="_blank">previous post</a>.</em></p>
<p style="text-align:justify;">The setup and notation is the same as in the previous post (linked above). But to summarize: Earlier we had an unknown smooth regression function <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" title="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" class="latex" />. The idea was to estimate at each training point, the gradient of this unknown function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />, and then taking the sample expectation of the outerproduct of the gradient. This quantity has some interesting properties and applications.</p>
<p style="text-align:justify;">However it has its limitations, for one, the mapping <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" title="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" class="latex" /> restricts the Gradient Outer Product being helpful for only regression and binary classification (since for binary classification the problem can be thought of as regression). It is not clear if a similar operator can be constructed when one is dealing with classification, that is the unknown smooth function is a <em>vector valued function</em> <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D%5Ec&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}^c" title="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}^c" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="c" title="c" class="latex" /> is the number of classes (let us say for the purpose of this discussion, that for each data point we have a probability distribution over the classes, a <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="c" title="c" class="latex" /> dimensional vector).</p>
<p style="text-align:justify;">In the case of the gradient outer product since we were working with a real valued function, it was possible to define the gradient at each point, which is simply:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CBigg%5B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x_1%7D%2C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x_2%7D%2C+%5Cdots%2C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x_d%7D+%5CBigg%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;Bigg[ &#92;frac{&#92;partial f}{&#92;partial x_1}, &#92;frac{&#92;partial f}{&#92;partial x_2}, &#92;dots, &#92;frac{&#92;partial f}{&#92;partial x_d} &#92;Bigg]" title="&#92;displaystyle &#92;Bigg[ &#92;frac{&#92;partial f}{&#92;partial x_1}, &#92;frac{&#92;partial f}{&#92;partial x_2}, &#92;dots, &#92;frac{&#92;partial f}{&#92;partial x_d} &#92;Bigg]" class="latex" /></p>
<p style="text-align:justify;">For a vector valued function <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D%5Ec&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}^c" title="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}^c" class="latex" />, we can&#8217;t have the gradient, but instead can define the<em> Jacobian at each point</em>:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbf%7BJ%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_2%7D+%26+%5Cdots+%26+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_d%7D+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+%5Cfrac%7B%5Cpartial+f_c%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_c%7D%7B%5Cpartial+x_2%7D+%26+%5Cdots+%26+%5Cfrac%7B%5Cpartial+f_c%7D%7B%5Cpartial+x_d%7D%5Cend%7Bbmatrix%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle &#92;mathbf{J} = &#92;begin{bmatrix} &#92;frac{&#92;partial f_1}{&#92;partial x_1} &amp; &#92;frac{&#92;partial f_1}{&#92;partial x_2} &amp; &#92;dots &amp; &#92;frac{&#92;partial f_1}{&#92;partial x_d} &#92;&#92; &#92;vdots &amp; &#92;vdots &amp; &#92;ddots &amp; &#92;vdots &#92;&#92; &#92;frac{&#92;partial f_c}{&#92;partial x_1} &amp; &#92;frac{&#92;partial f_c}{&#92;partial x_2} &amp; &#92;dots &amp; &#92;frac{&#92;partial f_c}{&#92;partial x_d}&#92;end{bmatrix}" title="&#92;displaystyle &#92;mathbf{J} = &#92;begin{bmatrix} &#92;frac{&#92;partial f_1}{&#92;partial x_1} &amp; &#92;frac{&#92;partial f_1}{&#92;partial x_2} &amp; &#92;dots &amp; &#92;frac{&#92;partial f_1}{&#92;partial x_d} &#92;&#92; &#92;vdots &amp; &#92;vdots &amp; &#92;ddots &amp; &#92;vdots &#92;&#92; &#92;frac{&#92;partial f_c}{&#92;partial x_1} &amp; &#92;frac{&#92;partial f_c}{&#92;partial x_2} &amp; &#92;dots &amp; &#92;frac{&#92;partial f_c}{&#92;partial x_d}&#92;end{bmatrix}" class="latex" /></p>
<p style="text-align:justify;">Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BJ%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{J}" title="&#92;mathbf{J}" class="latex" /> may be estimated in a similar manner as estimating gradients as in the previous posts. Which leads us to define the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" class="latex" />.</p>
<p style="text-align:justify;">The first thing to note is that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cnabla+f%28X%29%5Cnabla+f%28X%29%5ET%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" class="latex" /> defined in the previous post is simply the quantity for the special case when <img src="https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" title="f: &#92;mathbb{R}^d &#92;to &#92;mathbb{R}" class="latex" />. Another note is also in order: The reason why we suffixed that quantity with &#8220;outer product&#8221; (as opposed to &#8220;inner product&#8221; here) is simply because we considered the gradient to be a column vector, otherwise they are similar in spirit.</p>
<p style="text-align:justify;">Another thing to note is that it is easy to see that the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" class="latex" /> is a positive semi-definite matrix and hence is a Reimannian Metric, which is defined below:</p>
<p style="text-align:justify;"><strong>Definition: </strong>A Reimannian Metric <img src="https://s0.wp.com/latex.php?latex=G&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G" title="G" class="latex" /> on a manifold <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathcal{M}" title="&#92;mathcal{M}" class="latex" /> is a symmetric and positive semi-definite matrix, which defines a smoothly varying inner product in the tangent space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BT%7D_x+%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{T}_x &#92;mathcal{M}" title="&#92;mathbf{T}_x &#92;mathcal{M}" class="latex" />, for each point <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x &#92;in &#92;mathcal{M}" title="x &#92;in &#92;mathcal{M}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=a%2C+b+%5Cin+%5Cmathbf%7BT%7D_x+%5Cmathcal%7BM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="a, b &#92;in &#92;mathbf{T}_x &#92;mathcal{M}" title="a, b &#92;in &#92;mathbf{T}_x &#92;mathcal{M}" class="latex" />. This associated p.s.d matrix is called the <strong>metric tensor.</strong> In the above case, since <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" class="latex" /> is p.s.d it defines a Reimannian metric:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clangle+a%2C+b+%5Crangle_x+%3D+a%5ET+%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29+b&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;langle a, b &#92;rangle_x = a^T &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J}) b" title="&#92;langle a, b &#92;rangle_x = a^T &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J}) b" class="latex" /></p>
<p style="text-align:justify;">Thus, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" title="&#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" class="latex" /> is a specific metric (more general metrics are dealt with in areas such as metric learning).</p>
<p style="text-align:justify;"><strong>Properties: </strong>We saw some properties of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cnabla+f%28X%29%5Cnabla+f%28X%29%5ET%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" class="latex" /> in the previous post. In the same vein, does <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cmathbf%7BJ%7D%5ET+%5Cmathbf%7BJ%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;mathbf{J}^T &#92;mathbf{J})" class="latex" /> have similar properties? i.e. does the first eigenvector also correspond to the direction of highest average variation? What about the <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" />-dimensional subspace? What difference does it make that we are looking at a vector valued function? Also what about the cases when <img src="https://s0.wp.com/latex.php?latex=d+%3E+c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="d &gt; c" title="d &gt; c" class="latex" /> and otherwise?</p>
<p style="text-align:justify;">These are questions that I need to think about and should be the topic for a future post to be made soon, hopefully.</p>
<p style="text-align:justify;"><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5753/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5753/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5753&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/08/20/the-jacobian-inner-product/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>The Gradient Outer Product</title>
		<link>https://onionesquereality.wordpress.com/2014/08/16/the-gradient-outer-product/</link>
		<comments>https://onionesquereality.wordpress.com/2014/08/16/the-gradient-outer-product/#comments</comments>
		<pubDate>Sat, 16 Aug 2014 17:47:59 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Classification]]></category>
		<category><![CDATA[Diffusion Maps]]></category>
		<category><![CDATA[Dimensionality Reduction]]></category>
		<category><![CDATA[Gradient Outer Product]]></category>
		<category><![CDATA[Manifold Learning]]></category>
		<category><![CDATA[Non-Linear Dimensionality Reduction]]></category>
		<category><![CDATA[Regression]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[Supervised Learning]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5678</guid>
		<description><![CDATA[Recently, in course of a project that I had some involvement in, I came across an interesting quadratic form. It is called in the literature as the Gradient Outer Product. This operator, which has applications in supervised dimensionality reduction, inverse regression and metric learning can be motivated in two (related) ways, but before doing so, [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5678&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">Recently, in course of a project that I had some involvement in, I came across an interesting quadratic form. It is called in the literature as the <strong>Gradient Outer Product</strong>. This operator, which has applications in supervised dimensionality reduction, inverse regression and metric learning can be motivated in two (related) ways, but before doing so, the following is the set up:</p>
<p style="text-align:justify;"><strong>Setup</strong>: Suppose we have the usual set up as for nonparametric regression and (binary) classification i.e. let <img src="https://s0.wp.com/latex.php?latex=Y+%5Capprox+f%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="Y &#92;approx f(X)" title="Y &#92;approx f(X)" class="latex" /> for some unknown smooth <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />, the input <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="X" title="X" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="d" title="d" class="latex" /> dimensional <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28X%5Ei%29_%7Bi%3D1%7D%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="X = (X^i)_{i=1}^d" title="X = (X^i)_{i=1}^d" class="latex" /></p>
<p style="text-align:justify;"><strong>1. Supervised Dimensionality Reduction</strong>: It is often the case that <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> varies most along only some relevant coordinates. This is the main motivation behind variable selection.</p>
<p style="text-align:justify;">The idea in variable selection is the following: That <img src="https://s0.wp.com/latex.php?latex=f%28X%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f(X)" title="f(X)" class="latex" /> may be written as <img src="https://s0.wp.com/latex.php?latex=f%28PX%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f(PX)" title="f(PX)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=P+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bk+%5Ctimes+d%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P &#92;in &#92;{0,1&#92;}^{k &#92;times d}" title="P &#92;in &#92;{0,1&#92;}^{k &#92;times d}" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> projects down the data to only <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" /> relevant coordinates (i.e. some features are selected by <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> while others are discarded).</p>
<p style="text-align:justify;">This idea is generalized in <em>Multi-Index Regression</em>, where the goal is to recover a subspace <em>most relevant</em> to prediction. That is, now suppose the data varies significantly along all coordinates but it still depends on some subspace of smaller dimensionality. This might be achieved by letting <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> from the above to be <img src="https://s0.wp.com/latex.php?latex=P+%5Cin+%5Cmathbb%7BR%7D%5E%7Bk+%5Ctimes+d%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P &#92;in &#92;mathbb{R}^{k &#92;times d}" title="P &#92;in &#92;mathbb{R}^{k &#92;times d}" class="latex" />. It is important to note that <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> is not <em>any</em> subspace, but rather the <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" />-dimensional subspace to which if the data is projected, the regression error would be the least. This idea might be further generalized by means of mapping <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="X" title="X" class="latex" /> to some <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> non-linearly, but for now we only stick to the <em>relevant subspace.</em></p>
<p style="text-align:justify;">How can we recover such a subspace?</p>
<p style="text-align:center;"><strong>________________</strong></p>
<p style="text-align:justify;"><strong>2. Average Variation of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />: </strong>Another way to motivate this quantity is the following: Suppose we want to find the direction in which <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> varies the most <em>on average</em>, or the direction in which <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> varies the second fastest on average and so on. Or more generally, given any direction, we want to find the variation of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> along it. How can we recover these?</p>
<p style="text-align:center;"><strong>________________</strong></p>
<p style="text-align:justify;"><strong>The Expected Gradient Outer Product:  </strong>The expected gradient outer product of the unknown classification or regression function is the quantity: <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+G%28X%29+%3D+%5Cmathbb%7BE%7D_X+%28+%5Cnabla+f%28X%29%5Cnabla+f%28X%29%5ET%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" title="&#92;mathbb{E}_X G(X) = &#92;mathbb{E}_X ( &#92;nabla f(X)&#92;nabla f(X)^T)" class="latex" /></p>
<p style="text-align:justify;">The expected gradient outer product recovers the average variation of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> in all directions. This can be seen as follows: The directional derivative at <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> along <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+%5Cmathbb%7BR%7D%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="v &#92;in &#92;mathbb{R}^d" title="v &#92;in &#92;mathbb{R}^d" class="latex" /> is given by <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7Bf%27%7D_v%28x%29+%3D+%5Cnabla+f%28x%29%5ET+v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle {f&#039;}_v(x) = &#92;nabla f(x)^T v" title="&#92;displaystyle {f&#039;}_v(x) = &#92;nabla f(x)^T v" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%7C%7Bf%27%7D_v%28X%29%7C%5E2+%3D+%5Cmathbb%7BE%7D_X+%28v%5ET+G%28X%29+v%29+%3D+v%5ET+%28%5Cmathbb%7BE%7D_X+G%28X%29%29v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X |{f&#039;}_v(X)|^2 = &#92;mathbb{E}_X (v^T G(X) v) = v^T (&#92;mathbb{E}_X G(X))v" title="&#92;mathbb{E}_X |{f&#039;}_v(X)|^2 = &#92;mathbb{E}_X (v^T G(X) v) = v^T (&#92;mathbb{E}_X G(X))v" class="latex" />.</p>
<p style="text-align:justify;">From the above it follows that if <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" /> does not vary along <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="v" title="v" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="v" title="v" class="latex" /> must be in the null space of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" />.</p>
<p style="text-align:justify;">Infact it is not hard to show that the relevant subspace <img src="https://s0.wp.com/latex.php?latex=P&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="P" title="P" class="latex" /> as defined earlier can also be recovered from <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" />. This fact is given in the following lemma.</p>
<p style="text-align:justify;"><strong>Lemma</strong><span style="text-decoration:underline;">:</span> Under the assumed model i.e. <img src="https://s0.wp.com/latex.php?latex=Y+%5Capprox+f%28PX%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="Y &#92;approx f(PX)" title="Y &#92;approx f(PX)" class="latex" />, the gradient outer product matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" /> is of rank at most <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=%5C%7Bv_1%2C+v_2%2C+%5Cdots%2C+v_k+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;{v_1, v_2, &#92;dots, v_k &#92;}" title="&#92;{v_1, v_2, &#92;dots, v_k &#92;}" class="latex" /> be the eigenvectors of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" /> corresponding to the top <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="k" title="k" class="latex" /> eigenvalues of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" />. Then the following is true:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=span%28P%29+%3D+span%28v_1%2C+v_2%2C+%5Cdots%2C+v_k%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="span(P) = span(v_1, v_2, &#92;dots, v_k)" title="span(P) = span(v_1, v_2, &#92;dots, v_k)" class="latex" /></p>
<p style="text-align:justify;">This means that a spectral decomposition of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" /> recovers the relevant subspace. Also note that the Gradient Outer Product corresponds to a kind of a <em><strong>supervised version of Principal Component Analysis</strong></em>.</p>
<p style="text-align:center;"><strong>________________</strong></p>
<p style="text-align:justify;"><strong>Estimation:</strong> Ofcourse in real settings the function is unknown and we are only given points sampled from it. There are various estimators for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_X+%28G%28X%29%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbb{E}_X (G(X))" title="&#92;mathbb{E}_X (G(X))" class="latex" />, which usually involve estimation of the derivatives. In one of them the idea is to estimate, at each point <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x" title="x" class="latex" /> a linear approximation to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="f" title="f" class="latex" />. The slope of this approximation approximates the gradient at that point. Repeating this at the <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="n" title="n" class="latex" /> sample points, gives a sample gradient outer product. There is some work that shows that some of these estimators are <a href="http://en.wikipedia.org/wiki/Consistent_estimator" target="_blank">statistically consistent</a>.</p>
<p style="text-align:center;"><strong>________________</strong></p>
<p style="text-align:justify;"><strong>Related: Gradient Based Diffusion Maps: </strong>The gradient outer product can not isolate local information or geometry and its spectral decomposition, as seen above, gives only a linear embedding. One way to obtain a non-linear dimensionality reduction would be to borrow from and extend the idea of <a href="http://en.wikipedia.org/wiki/Diffusion_map" target="_blank">diffusion maps</a>, which are well established tools in semi supervised learning. The central quantity of interest for diffusion maps is the graph laplacian <img src="https://s0.wp.com/latex.php?latex=L+%3D+I+-+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+W+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L = I - D^{-&#92;frac{1}{2}} W D^{-&#92;frac{1}{2}}" title="L = I - D^{-&#92;frac{1}{2}} W D^{-&#92;frac{1}{2}}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D" title="D" class="latex" /> is the degree matrix and <img src="https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="W" title="W" class="latex" /> the adjacency matrix of the nearest neighbor graph constructed on the data points. The non linear embedding is obtained by a spectral decomposition of the operator <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L" title="L" class="latex" /> or its powers <img src="https://s0.wp.com/latex.php?latex=L%5Et&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L^t" title="L^t" class="latex" />.</p>
<p style="text-align:justify;">As above, a similar diffusion operator may be constructed by using local gradient information. One such possible operator could be:</p>
<p style="text-align:justify;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+W_%7Bij%7D+%3D+W_%7Bf%7D%28x_i%2C+x_j%29+%3D+exp+%5CBig%28+-+%5Cfrac%7B+%5C%7C+x_i+-+x_j+%5C%7C+%5E2%7D%7B%5Csigma_1%7D+-+%5Cfrac%7B+%7C+%5Cfrac%7B1%7D%7B2%7D+%28%5Cnabla+f%28x_i%29+%2B+%5Cnabla+f%28x_j%29%29+%28x_i+-+x_j%29+%7C%5E2+%7D%7B%5Csigma_2%7D%5CBig%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;displaystyle W_{ij} = W_{f}(x_i, x_j) = exp &#92;Big( - &#92;frac{ &#92;| x_i - x_j &#92;| ^2}{&#92;sigma_1} - &#92;frac{ | &#92;frac{1}{2} (&#92;nabla f(x_i) + &#92;nabla f(x_j)) (x_i - x_j) |^2 }{&#92;sigma_2}&#92;Big)" title="&#92;displaystyle W_{ij} = W_{f}(x_i, x_j) = exp &#92;Big( - &#92;frac{ &#92;| x_i - x_j &#92;| ^2}{&#92;sigma_1} - &#92;frac{ | &#92;frac{1}{2} (&#92;nabla f(x_i) + &#92;nabla f(x_j)) (x_i - x_j) |^2 }{&#92;sigma_2}&#92;Big)" class="latex" /></p>
<p style="text-align:justify;">Note that the first term is the same that is used in unsupervised dimension reduction techniques such as laplacian eigenmaps and diffusion maps. The second term can be interpreted as a diffusion on function values. This operator gives a way for non linear supervised dimension reduction using gradient information.</p>
<p style="text-align:justify;">The above operator was defined <a href="http://jmlr.csail.mit.edu/papers/volume11/wu10a/wu10a.pdf" target="_blank">here</a>, however no consistency results for the same are provided.</p>
<p><strong>Also see:</strong> <a href="https://onionesquereality.wordpress.com/2014/08/20/the-jacobian-inner-product/">The Jacobian Inner Product</a>.</p>
<p style="text-align:center;"><strong>________________</strong></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5678/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5678/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5678&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/08/16/the-gradient-outer-product/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>Implementation and Abstraction in Mathematics</title>
		<link>https://onionesquereality.wordpress.com/2014/08/13/implementation-and-abstraction-in-mathematics/</link>
		<comments>https://onionesquereality.wordpress.com/2014/08/13/implementation-and-abstraction-in-mathematics/#respond</comments>
		<pubDate>Wed, 13 Aug 2014 15:37:54 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Books]]></category>
		<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Abstraction]]></category>
		<category><![CDATA[Foundations of Mathematics]]></category>
		<category><![CDATA[Type Theory]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5685</guid>
		<description><![CDATA[I recently noticed on arXiv that the following manuscript &#8220;Implementation and Abstraction in Mathematics&#8221; by David McAllester. A couple of years ago, I had taken a graduate course taught by David that had a similar flavour (the material in the manuscript is more advanced, in particular the main results, not to mention it is better [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5685&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">I recently noticed on arXiv that the following manuscript <a href="http://arxiv.org/abs/1407.7274" target="_blank">&#8220;Implementation and Abstraction in Mathematics&#8221;</a> by David McAllester. A couple of years ago, I had taken a graduate course taught by David that had a similar flavour (the material in the manuscript is more advanced, in particular the main results, not to mention it is better organized and the presentation more polished), presenting a type theoretic foundation of mathematics. Although I can&#8217;t say I did very well in the course, I certainly enjoyed the ideas in it very much, and thus the above manuscript might be worth a look. Perhaps it might be a good idea to audit that course again, just to make sure I understand the main ideas better this time. :)</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5685/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5685/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5685&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/08/13/implementation-and-abstraction-in-mathematics/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>The Evolution of Programming Languages</title>
		<link>https://onionesquereality.wordpress.com/2014/01/29/the-evolution-of-programming-languages/</link>
		<comments>https://onionesquereality.wordpress.com/2014/01/29/the-evolution-of-programming-languages/#comments</comments>
		<pubDate>Wed, 29 Jan 2014 01:46:13 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Humour]]></category>
		<category><![CDATA[Evolution]]></category>
		<category><![CDATA[Lisp]]></category>
		<category><![CDATA[Programming Languages]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5660</guid>
		<description><![CDATA[I found these images on twitter via (Paige Bailey) @DynamicWebPaige  Found them so hilarious that I thought they deserved a blog post (Needless to say, click on each image for a higher resolution version). PS: A quick search indicates that these are from &#8220;Land of Lisp: Learn to Program in List, One Game at a [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5660&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">I found these images on twitter via (Paige Bailey) <a href="https://twitter.com/DynamicWebPaige">@<b>DynamicWebPaige</b></a>  Found them so hilarious that I thought they deserved a blog post (Needless to say, click on each image for a higher resolution version).</p>
<p style="text-align:justify;">PS: A quick search indicates that these are from &#8220;<a href="http://www.amazon.com/gp/product/1593272812/ref=as_li_tf_il?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1593272812&amp;linkCode=as2&amp;tag=onionerealit-20" target="_blank">Land of Lisp: Learn to Program in List, One Game at a time</a>&#8221; by M. D. Conrad Barski.</p>
<p><strong>The 40s and 50s</strong></p>
<p><a href="https://onionesquereality.files.wordpress.com/2014/01/40s1.jpg"><img class="aligncenter size-full wp-image-5663" alt="40s" src="https://onionesquereality.files.wordpress.com/2014/01/40s1.jpg?w=500&#038;h=204" width="500" height="204" /></a><strong>60s and 70s</strong></p>
<p><a href="https://onionesquereality.files.wordpress.com/2014/01/60s.jpg"><img class="aligncenter size-full wp-image-5666" alt="60s" src="https://onionesquereality.files.wordpress.com/2014/01/60s.jpg?w=500&#038;h=204" width="500" height="204" /></a></p>
<p><strong>80s and 90s</strong></p>
<p><a href="https://onionesquereality.files.wordpress.com/2014/01/80s.jpg"><img class="aligncenter size-full wp-image-5667" alt="80s" src="https://onionesquereality.files.wordpress.com/2014/01/80s.jpg?w=500&#038;h=204" width="500" height="204" /></a></p>
<p><strong>2000</strong></p>
<p><a href="https://onionesquereality.files.wordpress.com/2014/01/90s.jpg"><img class="aligncenter size-full wp-image-5668" alt="90s" src="https://onionesquereality.files.wordpress.com/2014/01/90s.jpg?w=500&#038;h=204" width="500" height="204" /></a></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5660/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5660/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5660&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2014/01/29/the-evolution-of-programming-languages/feed/</wfw:commentRss>
		<slash:comments>3</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/01/40s1.jpg" medium="image">
			<media:title type="html">40s</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/01/60s.jpg" medium="image">
			<media:title type="html">60s</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/01/80s.jpg" medium="image">
			<media:title type="html">80s</media:title>
		</media:content>

		<media:content url="http://onionesquereality.files.wordpress.com/2014/01/90s.jpg" medium="image">
			<media:title type="html">90s</media:title>
		</media:content>
	</item>
		<item>
		<title>On the two notions of &#8220;Information&#8221;</title>
		<link>https://onionesquereality.wordpress.com/2013/11/23/on-the-two-notions-of-information/</link>
		<comments>https://onionesquereality.wordpress.com/2013/11/23/on-the-two-notions-of-information/#comments</comments>
		<pubDate>Sat, 23 Nov 2013 20:48:59 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Communications]]></category>
		<category><![CDATA[Computer Science]]></category>
		<category><![CDATA[Information Theory]]></category>
		<category><![CDATA[Algorithmic Information]]></category>
		<category><![CDATA[Andrey Kolmogorov]]></category>
		<category><![CDATA[Claude Shannon]]></category>
		<category><![CDATA[Information]]></category>
		<category><![CDATA[Kolmogorov Complexity]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5654</guid>
		<description><![CDATA[Recently I was going through Shannon&#8217;s original 1948 Information Theory paper and a paper by Kolmogorov from 1983 that places the differences between &#8220;Shannon Information&#8221; and &#8220;Algorithmic Information&#8221; in sharp relief. After much information diffusion over the past decades the difference is quite obvious and particularly interesting to contrast nonetheless. Nevertheless I found these two [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5654&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">Recently I was going through Shannon&#8217;s original 1948 Information Theory paper and a paper by Kolmogorov from 1983 that places the differences between &#8220;Shannon Information&#8221; and &#8220;Algorithmic Information&#8221; in sharp relief. After much information diffusion over the past decades the difference is quite obvious and particularly interesting to contrast nonetheless. Nevertheless I found these two paragraphs from these two papers interesting, if for nothing then for historical reasons.</p>
<blockquote>
<div dir="ltr" style="text-align:justify;">&#8220;The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one <em>selected from a set of possible messages</em>. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design.&#8221; (<em>C. E. Shannon, A Mathematical Theory of Communication, 1948</em>).</div>
<div dir="ltr" style="text-align:justify;"></div>
</blockquote>
<div dir="ltr" style="text-align:justify;">
<blockquote>
<div dir="ltr">&#8220;Our definition of the quantity of information has the advantage that it refers to individual objects and not to objects treated as members of a set of objects with a probability distribution given on it. The probabilistic definition can be convincingly applied to the information contained, for example, in a stream of congratulatory telegrams. But it would not be clear how to apply it, for example, to an estimate of the quantity of information contained in a novel or in the translation of a novel into another language relative to the original. I think that the new definition is capable of introducing in similar applications of the theory at least clarity of principle.&#8221; (<em>A. N. Kolmogorov, Combinatorial Foundations of Information Theory and the Calculus of Probabilities, 1983</em><span style="font-size:14px;line-height:1.5;">).</span><span style="font-size:14px;line-height:1.5;"><br />
</span></div>
</blockquote>
<div dir="ltr">Note the reference that Shannon makes is to a message selected from <em>a set of possible messages </em>(and hence the use of probability theory), his problem was mainly concerned with the engineering application of communication. While Kolmogorov talks of individual objects and is not directly concerned with communication and hence the usage of computability (the two notions are related ofcourse, for example the expected Kolmogorov Complexity is equal to Shannon entropy).</div>
</div><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5654/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5654/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5654&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2013/11/23/on-the-two-notions-of-information/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
		<item>
		<title>(Very) Informal thoughts on the No-Free-Lunch Theorem</title>
		<link>https://onionesquereality.wordpress.com/2013/10/30/very-informal-thoughts-on-the-no-free-lunch-theorem/</link>
		<comments>https://onionesquereality.wordpress.com/2013/10/30/very-informal-thoughts-on-the-no-free-lunch-theorem/#comments</comments>
		<pubDate>Wed, 30 Oct 2013 15:29:29 +0000</pubDate>
		<dc:creator><![CDATA[Shubhendu Trivedi]]></dc:creator>
				<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Incompleteness]]></category>
		<category><![CDATA[Kolmogorov Complexity]]></category>
		<category><![CDATA[No Free Lunch]]></category>

		<guid isPermaLink="false">http://onionesquereality.wordpress.com/?p=5626</guid>
		<description><![CDATA[The No-Free-Lunch theorems (Wolpert and Macready) are one of the mainstays in Machine Learning and optimization. Crudely, the NFL says that &#8220;for every learner there exists a distribution in which it fails&#8221;, giving a precise handle to reason about why a given learning algorithm might fail on certain tasks (or, &#8220;specialization&#8221; of a learning algorithm [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5626&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
				<content:encoded><![CDATA[<p style="text-align:justify;">The No-Free-Lunch theorems (Wolpert and Macready) are one of the mainstays in Machine Learning and optimization. Crudely, the NFL says that &#8220;for every learner there exists a distribution in which it fails&#8221;, giving a precise handle to reason about why a given learning algorithm might fail on certain tasks (or, &#8220;specialization&#8221; of a learning algorithm might just be accidental). It gives a good justification why it is useful to incorporate prior informaion about the problem at hand to perform better. Oftentimes the NFL is used to argue that a general purpose learning algorithm is theoretically impossible. This is incorrect. Indeed, universal learning algorithms <a href="http://www.eng.tau.ac.il/~danar/Public-pdf/universal.pdf">exist</a> and there is a (possibly very small) <a href="http://www.hutter1.net/publ/uivnfl.pdf">free lunch</a>, at least for the case of &#8220;interesting problems&#8221; (thanks to Vick for pointing out these papers).</p>
<p style="text-align:justify;">The view &#8220;for every learner there exists a distribution in which it fails&#8221; is very useful. However I had recently been thinking that is particularly instructive to think of No Free Lunch as a kind of diagnolization argument (this is just a different way of thinking about the prior statement). I am not completely sure if this <em>works</em> but it sounds plausible and makes me see NFL in a different light. It might also be that there is some subtle absrudity or flaw in this kind of thinking. However, I think the NFL theorem can be seen as a simple corollary of the following incompleteness theorem in <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" target="_blank">Kolmogorov Complexity</a>.</p>
<p style="text-align:justify;"><strong>Theorem (Chaitin):</strong> <em>There exists a constant L (which only depends on the particular axiomatic system and the choice of description language) such that there does not exist a string s for which the statement the &#8220;The Kolmogorov Complexity of s is greater than L&#8221; (as formalized in S) can be proven within the axiomatic system S. </em></p>
<p style="text-align:justify;">Stated <a href="http://www.math.ucr.edu/home/baez/surprises.html" target="_blank">informally</a>: &#8220;There&#8217;s a number L such that we can&#8217;t prove the Kolmogorov complexity of any specific string of bits is more than L&#8221;. In particular, you can think of any learning machine as a compression scheme (since compression implies learning, we can think of any learning algorithm as a way to compress the data) of some fixed Kolmogorov Complexity (consider the most optimal program for that machine, the programming language is irrelevant as long as it is Turing Complete. Consider the description length of it in bits. That is its Kolmogorov Complexity). Now for any such learning machine (or class of learning machines w.l.o.g) you can construct a data string which has Kolmogorov Complexity greater than what this learning machine has. i.e. this data string is random from the point of view of this learning machine as it is incompressible for it. In short IF the data string has some Kolmogorov Complexity K and your learning machine has Kolmogorov Complexity k and k &lt; K then you will never be able to find any structure in your data string i.e. can never show that it has a KC greater than k. Thus the data string given to our learner would appear to be structureless. Thus no free lunch is just a simple diagonalization argument in the spirit of incompleteness results. One can always construct a string that is Kolmogorov Random or structureless w.r.t our encoding scheme. Ofcourse the Kolmogorov Complexity is uncomputable. However I think the above argument works even when one is talking of upper bounds on the actual Kolmogorov Complexity, but I am not very sure about it or if there are any problems with it.</p>
<p style="text-align:justify;">Also, if one assumes that that there is no reason to believe that structureless functions exist (if we assume that the data is generated by a computational process then there is probably no reason to believe that it is structureless i.e. the problem is &#8220;interesting&#8221;). And if this is the case, then you would always have a machine in your library of learning algorithms that will be able to find some upper bound on the Kolmogorov Complexity of the data string (much less than the machine itself). This search might be done using <a href="http://www.scholarpedia.org/article/Universal_search">Levin&#8217;s universal search</a>, for example. I think this line of reasoning leads naturally, to thinking why NFL doesn&#8217;t say anything about whether universal learning algorithms can exist.</p>
<p style="text-align:justify;">PS: This thought train about NFL as diagonalization was inspired by a post by David McAllester on what he calls the &#8220;Free Lunch Theorem&#8221; over <a href="http://ai-thoughs.blogspot.com/2013/09/the-free-lunch-theorem.html" target="_blank">here</a>. Some of this appeared as a comment there. However, now I have been inspired enough to study some of the papers concerned including his. (Unrelated to all this. I particularly found likening Chomsky&#8217;s Universal Grammar as simply an invocation of NFL on information theoretic grounds in his post very enlightening).</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/onionesquereality.wordpress.com/5626/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/onionesquereality.wordpress.com/5626/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=onionesquereality.wordpress.com&#038;blog=2488525&#038;post=5626&#038;subd=onionesquereality&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
			<wfw:commentRss>https://onionesquereality.wordpress.com/2013/10/30/very-informal-thoughts-on-the-no-free-lunch-theorem/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
	
		<media:content url="http://1.gravatar.com/avatar/47c857d478235ab1307f501052d86975?s=96&#38;d=monsterid" medium="image">
			<media:title type="html">Shubhendu Trivedi</media:title>
		</media:content>
	</item>
	</channel>
</rss>
